 <!DOCTYPE html>
<html>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="UTF-8">
    <title>Simple Research Garden</title>
    <link rel="stylesheet" href="main.css">
    <link rel="stylesheet" href="garden.css">
    <style id="flower_css"></style>
</head>
<body>
    <!-- Research Garden -->
    <svg id="flower_template" xmlns="http://www.w3.org/2000/svg" viewBox="-10 -10 20 20">
        <ellipse rx="10" ry="20" transform="rotate(0)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
        <ellipse rx="10" ry="20" transform="rotate(45)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
        <ellipse rx="10" ry="20" transform="rotate(90)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
        <ellipse rx="10" ry="20" transform="rotate(135)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
        <ellipse rx="10" ry="20" transform="rotate(180)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
        <ellipse rx="10" ry="20" transform="rotate(225)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
        <ellipse rx="10" ry="20" transform="rotate(270)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
        <ellipse rx="10" ry="20" transform="rotate(315)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
        <circle r="6" fill="white"/>
    </svg>
    <div id="garden_title">Sanjoy's Research Garden</div>
    <div id="garden_container">
        <svg id="garden" preserveAspectRatio="xMinYMin meet"></svg>
     </div>
     <div id="coauthor_hall_of_fame">
        <div id="coauthor_title">Recurrent Coauthors ("Hall of Friends")</div>
        <div id="coauthor_list"></div>
    </div>
    <div id="paper_modal">
        <!-- a close icon at the top right -->
        <div id="paper_modal_close" onclick="$('#paper_modal').fadeOut(200);">X</div>
        <div id="paper_modal_title"></div>
        <div id="paper_modal_venue"></div>
        <div id="paper_modal_content"></div>
        <div id="paper_modal_links"></div>
    </div>

    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>

    <script src="garden.js?v=2"></script>
    <script>
        var papers = [
         {"id": "meerkat2", "title": "Meerkat2", "venue": "ECCV 2024", "url": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08071.pdf", "root_node": 1, "root_name": "Audio-Visual Foundational Models\n2024-now", "root_color": "#D1BDFF", "coauthors": ["Sanjoy Chowdhury"], "full_title": "Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time", "summary": "Leveraging Large Language Models' remarkable proficiency in text-based tasks, recent works on Multi-modal LLMs (MLLMs) extend them to other modalities like vision and audio. However, the progress in these directions has been mostly focused on tasks that only require a coarse-grained understanding of the audio-visual semantics. We present Meerkat, an audio-visual LLM equipped with a fine-grained understanding of image and audio both spatially and temporally. With a new modality alignment module based on optimal transport and a crossattention module that enforces audio-visual consistency, Meerkat can tackle challenging tasks such as audio referred image grounding, image guided audio temporal localization, and audio-visual fact-checking. Moreover, we carefully curate a large dataset AVFIT that comprises 3M instruction tuning samples collected from open-source datasets, and introduce MeerkatBench that unifies five challenging audio-visual tasks. We achieve state-of-the-art performance on all these downstream tasks with a relative improvement of up to 37.12%.", "additional_links": {"project page": "https://schowdhury671.github.io/meerkat_project/", "data": "https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVFIT%20dataset&ga=1"}},
        {"id": "aurelia2", "title": "Aurelia2", "venue": "arXiv", "url": "https://www.arxiv.org/pdf/2503.23219", "parent": "meerkat2", "coauthors": ["Sanjoy", "abcd"], "full_title": "AURELIA: Test-time Reasoning Distillation in Audio-Visual LLMs", "summary": "In this paper, we introduce AURELIA, a novel actor-critic based audio-visual reasoning framework that distils structured, step-by-step reasoning into AVLLMs at test time, improving their ability to process complex multi-modal inputs without additional training or fine-tuning. To further advance AVLLM reasoning skills, we present AVReasonBench, a challenging benchmark comprising 4500 audio-visual questions, each paired with detailed step-by-step reasoning. Our benchmark spans six distinct tasks, including AV-GeoIQ, which evaluates AV reasoning combined with geographical and cultural knowledge", "additional_links": {"code": "https://github.com/schowdhury671/aurelia", "data": "https://github.com/schowdhury671/aurelia"}},
        {"id": "avtrustbench2", "title": "AVTrustBench2", "venue": "arXiv", "url": "https://arxiv.org/abs/2501.02135", "parent": "meerkat2", "coauthors": ["Sanjoy", "efgh", "xyx"], "full_title": "AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs", "summary": "We introduce Audio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency. Using our benchmark we extensively evaluate 16 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human-like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model-agnostic calibrated audio-visual preference optimization based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks.", "additional_links": {"code": "https://github.com/schowdhury671/avtrustbench-", "data": "https://github.com/schowdhury671/avtrustbench-"}},

         {"id": "meerkat3", "title": "Meerkat2", "venue": "ECCV 2024", "url": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08071.pdf", "root_node": 1, "root_name": "Audio-Visual Foundational Models\n2024-now", "root_color": "#D1BDFF", "coauthors": ["Sanjoy Chowdhury"], "full_title": "Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time", "summary": "Leveraging Large Language Models' remarkable proficiency in text-based tasks, recent works on Multi-modal LLMs (MLLMs) extend them to other modalities like vision and audio. However, the progress in these directions has been mostly focused on tasks that only require a coarse-grained understanding of the audio-visual semantics. We present Meerkat, an audio-visual LLM equipped with a fine-grained understanding of image and audio both spatially and temporally. With a new modality alignment module based on optimal transport and a crossattention module that enforces audio-visual consistency, Meerkat can tackle challenging tasks such as audio referred image grounding, image guided audio temporal localization, and audio-visual fact-checking. Moreover, we carefully curate a large dataset AVFIT that comprises 3M instruction tuning samples collected from open-source datasets, and introduce MeerkatBench that unifies five challenging audio-visual tasks. We achieve state-of-the-art performance on all these downstream tasks with a relative improvement of up to 37.12%.", "additional_links": {"project page": "https://schowdhury671.github.io/meerkat_project/", "data": "https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVFIT%20dataset&ga=1"}},
        {"id": "aurelia3", "title": "Aurelia2", "venue": "arXiv", "url": "https://www.arxiv.org/pdf/2503.23219", "parent": "meerkat3", "coauthors": ["Sanjoy", "abcd"], "full_title": "AURELIA: Test-time Reasoning Distillation in Audio-Visual LLMs", "summary": "In this paper, we introduce AURELIA, a novel actor-critic based audio-visual reasoning framework that distils structured, step-by-step reasoning into AVLLMs at test time, improving their ability to process complex multi-modal inputs without additional training or fine-tuning. To further advance AVLLM reasoning skills, we present AVReasonBench, a challenging benchmark comprising 4500 audio-visual questions, each paired with detailed step-by-step reasoning. Our benchmark spans six distinct tasks, including AV-GeoIQ, which evaluates AV reasoning combined with geographical and cultural knowledge", "additional_links": {"code": "https://github.com/schowdhury671/aurelia", "data": "https://github.com/schowdhury671/aurelia"}},
        {"id": "avtrustbench3", "title": "AVTrustBench2", "venue": "arXiv", "url": "https://arxiv.org/abs/2501.02135", "parent": "meerkat3", "coauthors": ["Sanjoy", "efgh", "xyx"], "full_title": "AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs", "summary": "We introduce Audio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency. Using our benchmark we extensively evaluate 16 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human-like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model-agnostic calibrated audio-visual preference optimization based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks.", "additional_links": {"code": "https://github.com/schowdhury671/avtrustbench-", "data": "https://github.com/schowdhury671/avtrustbench-"}},

        {"id": "meerkat4", "title": "Meerkat2", "venue": "ECCV 2024", "url": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08071.pdf", "root_node": 1, "root_name": "Audio-Visual Foundational Models\n2024-now", "root_color": "#D1BDFF", "coauthors": ["Sanjoy Chowdhury"], "full_title": "Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time", "summary": "Leveraging Large Language Models' remarkable proficiency in text-based tasks, recent works on Multi-modal LLMs (MLLMs) extend them to other modalities like vision and audio. However, the progress in these directions has been mostly focused on tasks that only require a coarse-grained understanding of the audio-visual semantics. We present Meerkat, an audio-visual LLM equipped with a fine-grained understanding of image and audio both spatially and temporally. With a new modality alignment module based on optimal transport and a crossattention module that enforces audio-visual consistency, Meerkat can tackle challenging tasks such as audio referred image grounding, image guided audio temporal localization, and audio-visual fact-checking. Moreover, we carefully curate a large dataset AVFIT that comprises 3M instruction tuning samples collected from open-source datasets, and introduce MeerkatBench that unifies five challenging audio-visual tasks. We achieve state-of-the-art performance on all these downstream tasks with a relative improvement of up to 37.12%.", "additional_links": {"project page": "https://schowdhury671.github.io/meerkat_project/", "data": "https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVFIT%20dataset&ga=1"}},
        {"id": "aurelia4", "title": "Aurelia2", "venue": "arXiv", "url": "https://www.arxiv.org/pdf/2503.23219", "parent": "meerkat4", "coauthors": ["Sanjoy", "abcd"], "full_title": "AURELIA: Test-time Reasoning Distillation in Audio-Visual LLMs", "summary": "In this paper, we introduce AURELIA, a novel actor-critic based audio-visual reasoning framework that distils structured, step-by-step reasoning into AVLLMs at test time, improving their ability to process complex multi-modal inputs without additional training or fine-tuning. To further advance AVLLM reasoning skills, we present AVReasonBench, a challenging benchmark comprising 4500 audio-visual questions, each paired with detailed step-by-step reasoning. Our benchmark spans six distinct tasks, including AV-GeoIQ, which evaluates AV reasoning combined with geographical and cultural knowledge", "additional_links": {"code": "https://github.com/schowdhury671/aurelia", "data": "https://github.com/schowdhury671/aurelia"}},
        {"id": "avtrustbench4", "title": "AVTrustBench2", "venue": "arXiv", "url": "https://arxiv.org/abs/2501.02135", "parent": "meerkat4", "coauthors": ["Sanjoy", "efgh", "xyx"], "full_title": "AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs", "summary": "We introduce Audio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency. Using our benchmark we extensively evaluate 16 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human-like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model-agnostic calibrated audio-visual preference optimization based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks.", "additional_links": {"code": "https://github.com/schowdhury671/avtrustbench-", "data": "https://github.com/schowdhury671/avtrustbench-"}},
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           
        {"id": "meerkat5", "title": "Meerkat2", "venue": "ECCV 2024", "url": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08071.pdf", "root_node": 1, "root_name": "Audio-Visual Foundational Models\n2024-now", "root_color": "#D1BDFF", "coauthors": ["Sanjoy Chowdhury"], "full_title": "Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time", "summary": "Leveraging Large Language Models' remarkable proficiency in text-based tasks, recent works on Multi-modal LLMs (MLLMs) extend them to other modalities like vision and audio. However, the progress in these directions has been mostly focused on tasks that only require a coarse-grained understanding of the audio-visual semantics. We present Meerkat, an audio-visual LLM equipped with a fine-grained understanding of image and audio both spatially and temporally. With a new modality alignment module based on optimal transport and a crossattention module that enforces audio-visual consistency, Meerkat can tackle challenging tasks such as audio referred image grounding, image guided audio temporal localization, and audio-visual fact-checking. Moreover, we carefully curate a large dataset AVFIT that comprises 3M instruction tuning samples collected from open-source datasets, and introduce MeerkatBench that unifies five challenging audio-visual tasks. We achieve state-of-the-art performance on all these downstream tasks with a relative improvement of up to 37.12%.", "additional_links": {"project page": "https://schowdhury671.github.io/meerkat_project/", "data": "https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVFIT%20dataset&ga=1"}},
        {"id": "aurelia5", "title": "Aurelia2", "venue": "arXiv", "url": "https://www.arxiv.org/pdf/2503.23219", "parent": "meerkat5", "coauthors": ["Sanjoy", "abcd"], "full_title": "AURELIA: Test-time Reasoning Distillation in Audio-Visual LLMs", "summary": "In this paper, we introduce AURELIA, a novel actor-critic based audio-visual reasoning framework that distils structured, step-by-step reasoning into AVLLMs at test time, improving their ability to process complex multi-modal inputs without additional training or fine-tuning. To further advance AVLLM reasoning skills, we present AVReasonBench, a challenging benchmark comprising 4500 audio-visual questions, each paired with detailed step-by-step reasoning. Our benchmark spans six distinct tasks, including AV-GeoIQ, which evaluates AV reasoning combined with geographical and cultural knowledge", "additional_links": {"code": "https://github.com/schowdhury671/aurelia", "data": "https://github.com/schowdhury671/aurelia"}},
        {"id": "avtrustbench5", "title": "AVTrustBench2", "venue": "arXiv", "url": "https://arxiv.org/abs/2501.02135", "parent": "meerkat5", "coauthors": ["Sanjoy", "efgh", "xyx"], "full_title": "AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs", "summary": "We introduce Audio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency. Using our benchmark we extensively evaluate 16 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human-like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model-agnostic calibrated audio-visual preference optimization based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks.", "additional_links": {"code": "https://github.com/schowdhury671/avtrustbench-", "data": "https://github.com/schowdhury671/avtrustbench-"}}

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
        ]

        build_garden(papers);
    </script>

</body>
</html>
