<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Research Garden - Sanjoy Chowdhury</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <!-- Stylesheets -->
    <link rel="stylesheet" href="../main.css">
    <link rel="stylesheet" type="text/css" href="../style.css" />
    <link rel="icon" type="image/png" href="../images/IMG-20200202-WA0044_2.jpg">

    <style>
        body {
            font-family: 'Raleway', sans-serif;
            margin: 0;
            padding: 0;
            background: white;
            color: #111;
            font-weight: 400;
            line-height: 1.6;
        }
        
        .navbar {
            display: flex;
            justify-content: center;
            background-color: #f8f8f8;
            padding: 15px;
            border-bottom: 2px solid #e0e0e0;
        }

        .navbar a {
            margin: 0 30px;
            text-decoration: none;
            color: #333;
            font-weight: bold;
        }

        .navbar a:hover {
            color: #4CAF50;
        }

        .name {
            font-family: 'Dancing Script', cursive;
            font-size: 2.5em;
            color: black;
            margin: 0;
        }

        #garden_container {
            width: 80%;
            margin: 20px auto;
        }

        .research-section {
            margin-bottom: 40px;
        }

        .text h3 {
            color: #1E90FF; /* Blue color for section names */
        }
    </style>
    
    <link href="https://fonts.googleapis.com/css2?family=Dancing+Script:wght@700&display=swap" rel="stylesheet">
</head>

<body>
    <!-- Navigation Bar -->
    <div class="navbar">
        <a class="name" href="https://schowdhury671.github.io/">Sanjoy Chowdhury</a>
        <a href="https://schowdhury671.github.io/">Home</a>
        <a href="../others/index.html">Miscellaneous</a>
    </div>

    <!-- Research Summaries -->
    <div id="garden_container">
        
        <div class="research-section">
            <div class="text">
                <h3>Cross-modal Generation (2024-now)</h3>
                <p>
                    The exploration of cross-modal generation seeks to integrate and synthesize diverse data forms. **MAGNET** (arXiv) introduces methods for unifying sensory data, enhancing multimodal information processing. **MeLFusion** (CVPR 2024) builds on this by demonstrating techniques for effective multimedia synthesis. **Adverb** (ICCV 2023) focuses on visually guided audio dereverberation, blending visual cues with audio to ensure clarity and precision.
                </p>
            </div>
        </div>

        <div class="research-section">
            <div class="text">
                <h3>Audio-Visual Representation Learning (2021-now)</h3>
                <p>
                   This area addresses the need for machines to understand and summarize sensory data efficiently. **EgoAdapt** (arXiv) develops methods for adapting egocentric perspectives, utilizing contextual insights from first-person views. **AudViSum** (BMVC 2021) creates techniques to distill critical information from audio-visual data streams. **Listen to Pixels** (ICIP 2021) translates audio cues into visual stimuli, enhancing AI interpretation.
                </p>
            </div>
        </div>

        <div class="research-section">
            <div class="text">
                <h3>Audio-Visual LLMs (2024-now)</h3>
                <p>
                    The integration of auditory and visual data enhances contextual comprehension in LLMs. **AURELIA** (arXiv) lays the foundation with frameworks for nuanced interactions. **AVTrustBench** (ECCV 2024) assesses AI interpretation reliability across varied contexts, ensuring consistent performance. **Meerkat** (ECCV 2024) further bridges sensory inputs with language understanding.
                </p>
            </div>
        </div>

        <div class="research-section">
            <div class="text">
                <h3>Integrating Vision-Language (2022-now)</h3>
                <p>
                    Integration of vision and language enhances AI task performance. **ASPIRE** (ACL Findings 2024) refines tasks through sensory synchronization. **VLMNav** (NAACL Findings 2024) augments navigation by combining visual and verbal cues. **Intent-o-Meter** (Nature 2023) offers insights into understanding intent, while **Apollo** (EMNLP 2023) enhances word-image synergy.
                </p>
            </div>
        </div>

        <div class="research-section">
            <div class="text">
                <h3>Computational Photography (2021-2022)</h3>
                <p>
                    This area enhances image and video quality using advanced techniques. **MAW** (ICCP 2023) introduces methods to refine visual clarity. **VDESIRR** (ICCV 2021) focuses on video optimization with innovative algorithms.
                </p>
            </div>
        </div>

    </div>

</body>

</html>
