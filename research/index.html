<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Research Garden - Sanjoy Chowdhury</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <!-- Stylesheets -->
    <link rel="stylesheet" href="../main.css">
    <link rel="stylesheet" type="text/css" href="../style.css" />
    <link rel="icon" type="image/png" href="../images/IMG-20200202-WA0044_2.jpg">

<!--     for garden -->
      <!-- Stylesheets2 -->
    <link rel="stylesheet" href="../garden.css">
    <style id="flower_css"></style>
<!--     for garden -->

    <style>
        body {
            font-family: 'Raleway', sans-serif;
            margin: 0;
            padding: 0;
            background: white;
            color: #111;
            font-weight: 400;
            line-height: 1.6;
        }
        
        .navbar {
            display: flex;
            justify-content: center;
            background-color: #f8f8f8;
            padding: 15px;
            border-bottom: 2px solid #e0e0e0;
        }

        .navbar a {
            margin: 0 30px;
            text-decoration: none;
            color: #333;
            font-weight: bold;
        }

        .navbar a:hover {
            color: #4CAF50;
        }

        .name {
            font-family: 'Dancing Script', cursive;
            font-size: 2.5em;
            color: black;
            margin: 0;
        }

        #garden_container {
            width: 80%;
            margin: 20px auto;
        }

        .research-section {
            margin-bottom: 40px;
        }

        .text h2 {
            color: #1E90FF; /* Blue color for section names */
        }
    </style>
    
    <link href="https://fonts.googleapis.com/css2?family=Dancing+Script:wght@700&display=swap" rel="stylesheet">
</head>

<body>
    <!-- Navigation Bar -->
    <div class="navbar">
        <a class="name" href="https://schowdhury671.github.io/">Sanjoy Chowdhury</a>
        <a href="https://schowdhury671.github.io/">Home</a>
        <a href="../others/index.html">Miscellaneous</a>
    </div>


<!--     ################################################################################################ -->
<!--    adding garden here  -->


    	<!-- Research Garden -->
	<svg id="flower_template" xmlns="http://www.w3.org/2000/svg" viewBox="-10 -10 20 20">
	<ellipse rx="10" ry="20" transform="rotate(0)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
	<ellipse rx="10" ry="20" transform="rotate(45)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
	<ellipse rx="10" ry="20" transform="rotate(90)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
	<ellipse rx="10" ry="20" transform="rotate(135)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
	<ellipse rx="10" ry="20" transform="rotate(180)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
	<ellipse rx="10" ry="20" transform="rotate(225)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
	<ellipse rx="10" ry="20" transform="rotate(270)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
	<ellipse rx="10" ry="20" transform="rotate(315)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
	<circle r="6" fill="white"/>
    </svg>

		    
    




<div id="garden_title">Sanjoy's Research Garden</div> 
<div id="garden_container"> <svg id="garden" preserveAspectRatio="xMinYMin meet"></svg> </div> 
<!--      <div id="coauthor_hall_of_fame"> 
	<div id="coauthor_title">Recent Coauthors</div> 
	<div id="coauthor_list"></div> </div> --> 
<div id="paper_modal"> 
<!-- a close icon at the top right --> 
<div id="paper_modal_close" onclick="$('#paper_modal').fadeOut(200);">X</div> 
<div id="paper_modal_title"></div> <div id="paper_modal_venue"></div> 	
<div id="paper_modal_content"></div> <div id="paper_modal_links"></div> 
</div>
	


  <br>   <br>   <br>


    

    	<table style="width:20%;border:80px;border-spacing:0px;border-collapse:separate;margin-right:0;margin-left:auto;">
          <tr>
<!--             <td style="padding:2.5%;width:80%;vertical-align:middle"> -->
	      
<!-- 	 <table style="width:80%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:0;">   -->
<!-- 	 <tr>	  -->
	 <script style="width:30px;height:20px;align:right" type="text/javascript"  id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=pgVlF5ljpLWpcNcZ70hmBZf0I-uNxgD0lqx-sfyNPZQ&cl=ffffff&w=a"></script>      
	 </tr>
	 </table>	
		
        
  </table>


    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="garden.js?v=2"></script>
    
		
<script>
      var papers = [
	
	{"id": "adverb", "title": "Adverb", "venue": "ICCV 2023", "url": "https://openaccess.thecvf.com/content/ICCV2023/papers/Chowdhury_AdVerb_Visually_Guided_Audio_Dereverberation_ICCV_2023_paper.pdf", "root_node": 1, "root_name": "Cross-modal Generation\n2024-now", "root_color": "#FFE699", "coauthors": ["Sreyan Ghosh", "Subhrajyoti Dasgupta", "Anton Ratnarajah", "Utkarsh Tyagi", "Dinesh Manocha"], "full_title": "AdVerb: Visually Guided Audio Dereverberation", "summary": "We present a novel audio-visual dereverberation framework that uses visual cues in addition to the reverberant sound to estimate clean audio.", "additional_links": {"project page": "https://schowdhury671.github.io/adverb/", "code": "https://github.com/Sreyan88/AdVerb-dereverb", "video": "https://www.youtube.com/watch?v=dZuR-pZ9uM0"}},
        {"id": "melfusion", "title": "MeLFusion", "venue": "CVPR 2024", "url": "https://www.arxiv.org/pdf/2406.04673", "parent": "adverb", "coauthors": ["Sayan Nag", "Joseph KJ", "BV Srinivasan", "Dinesh Manocha"], "full_title": "MeLFusion: Synthesizing Music from Image and Language Cues using Diffusion Models", "summary": "We propose MeLFusion, a model that can effectively use cues from a textual description and the corresponding image to synthesize music. MeLFusion is a text-to-music diffusion model with a novel `visual synapse`, which effectively infuses the semantics from the visual modality into the generated music. To facilitate research in this area, we introduce a new dataset MeLBench, and propose a new evaluation metric IMSM.", "additional_links": {"code": "https://github.com/schowdhury671/melfusion/tree/main", "data": "https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FMeLFusion%20datasets&ga=1", "project page": "https://schowdhury671.github.io/melfusion_cvpr2024/"}},
        {"id": "magnet", "title": "MAGNET", "venue": "arXiv", "url": "https://schowdhury671.github.io/", "parent": "melfusion", "coauthors": ["Sayan Nag", "Mohamed Elmoghany", "Yohan Abeysinghe", "Yunjie Fei", "Salman Khan", "Dinesh Manocha", "Mohamed Elhoseiny"], "full_title": "MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks", "summary": "we introduce a novel task named AV-HaystacksQA, where the goal is to identify salient segments across different videos in response to a query and link them together to generate the most informative answer. To this end, we present AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA pairs designed to assess the capabilities of LMMs in multi-video retrieval and temporal grounding task. Additionally, we propose a model-agnostic, multi-agent framework MAGNET to address this challenge, achieving up to 89% and 65% relative improvements over baseline methods on BLEU@4 and GPT evaluation scores in QA task on our proposed AVHaystacks. To enable robust evaluation of multi-video retrieval and temporal grounding for optimal response generation, we introduce two new metrics, STEM, which captures alignment errors between a ground truth and a predicted step sequence and MTGS, to facilitate balanced and interpretable evaluation of segment-level grounding performance.", "additional_links": {"code": "https://schowdhury671.github.io/", "data": "https://schowdhury671.github.io/"}},

	
        {"id": "ltl", "title": "Listen to Pixels", "venue": "ICIP 2021", "url": "https://ieeexplore.ieee.org/document/9506019", "root_node": 1, "root_name": "Audio-Visual Representation Learning\n2021-now", "root_color": "#B3F5BC", "coauthors": ["Subhrajyoti Dasgupta", "Sudip Das", "Ujjwal Bhattacharya"], "full_title": "Listen to the Pixels", "summary": "In this study, we exploited the concurrency between audio and visual modalities in an attempt to solve the joint audio-visual segmentation problem in a self-supervised manner.", "additional_links": {"code": "https://github.com/schowdhury671/Audio-visual-joint-segmentation", "video": "https://www.youtube.com/watch?v=xUwzSQaQ9oQ"}},
        {"id": "audvisum", "title": "AudViSum", "venue": "BMVC 2021", "url": "https://www.bmvc2021-virtualconference.com/assets/papers/1430.pdf", "indirect_connections": ["magnet"], "parent": "ltl", "coauthors": ["Aditya Patra", "Subhrajyoti Dasgupta", "Ujjwal Bhattacharya"], "full_title": "AudViSum: Self-Supervised Deep Reinforcement Learning for Diverse Audio-Visual Summary Generation", "summary": "Introduced a novel deep reinforcement learning-based self-supervised audio-visual summarization model that leverages both audio and visual information to generate diverse yet semantically meaningful summaries.", "additional_links": {"code": "https://github.com/schowdhury671/AudViSum", "video": "https://www.youtube.com/watch?v=Hier-zMWcc0"}},
        {"id": "egoadapt", "title": "EgoAdapt", "venue": "ICCV 2025", "url": "https://schowdhury671.github.io/", "parent": "audvisum", "coauthors": ["Subrata Biswas", "Sayan Nag", "Tushar Nagarajan", "Calvin Murdock", "Yijun Qian", "Ishwarya Ananthabhotla", "Vamsi Ithapu", "Dinesh Manocha", "Ruohan Gao"], "full_title": "EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception", "summary": "Modern perception models, particularly those designed for multisensory egocentric tasks, have achieved remark- able performance but often come with substantial compu- tational costs. These high demands pose challenges for real-world deployment, especially in resource-constrained environments. In this paper, we introduce EGOADAPT, a framework that adaptively performs cross-modal distilla- tion and policy learning to enable efficient inference across different egocentric perception tasks, including egocentric action recognition, active speaker localization, and behav- ior anticipation. Our proposed policy module is adaptable to task-specific action spaces, making it broadly applicable. Experimental results on three challenging egocentric datasets EPIC-Kitchens, EasyCom, and Aria Everyday Activities—demonstrate that our method significantly enhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%, and energy up to 9.6×, while still on-par and in many cases outperforming, the performance of corresponding state-of-the-art models.", "additional_links": {"code": "https://schowdhury671.github.io/", "data": "https://schowdhury671.github.io/"}},

	{"id": "meerkat", "title": "Meerkat", "venue": "ECCV 2024", "url": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08071.pdf", "root_node": 1, "root_name": "Audio-Visual LLMs\n2024-now", "root_color": "#D1BDFF", "coauthors": ["Sayan Nag", "Subhrajyoti Dasgupta", "Jun Chen", "Mohamed Elhoseiny", "Ruohan Gao", "Dinesh Manocha"], "full_title": "Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time", "summary": "We present Meerkat, an audio-visual LLM equipped with a fine-grained understanding of image and audio both spatially and temporally. With a new modality alignment module based on optimal transport and a cross-attention module that enforces audio-visual consistency, Meerkat can tackle challenging tasks such as audio referred image grounding, image guided audio temporal localization, and audio-visual fact-checking. Moreover, we carefully curate a large dataset AVFIT that comprises 3M instruction tuning samples collected from open-source datasets, and introduce MeerkatBench that unifies five challenging audio-visual tasks.","additional_links": {"code": "https://github.com/schowdhury671/meerkat/tree/main", "data": "https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVFIT%20dataset&ga=1"}},
	{"id": "aurelia", "title": "AURELIA", "venue": "ICCV 2025", "url": "https://arxiv.org/pdf/2503.23219", "parent": "meerkat", "coauthors": ["Sayan Nag", "Hanan Gani", "Nishit Anand", "Mohamed Elhoseiny", "Ruohan Gao", "Salman Khan", "Dinesh Manocha"], "full_title": "AURELIA: Test-time Reasoning Distillation in Audio-Visual LLMs", "summary": "In this paper, we introduce AURELIA, a novel actor-critic based audio-visual reasoning framework that distils structured, step-by-step reasoning into AVLLMs at test time, improving their ability to process complex multi-modal inputs without additional training or fine-tuning. To further advance AVLLM reasoning skills, we present AVReasonBench, a challenging benchmark comprising 4500 audio-visual questions, each paired with detailed step-by-step reasoning. Our benchmark spans six distinct tasks, including AV-GeoIQ, which evaluates AV reasoning combined with geographical and cultural knowledge", "additional_links": {"code": "https://github.com/schowdhury671/aurelia", "data": "https://github.com/schowdhury671/aurelia"}},
	{"id": "avtrustbench", "title": "AVTrustBench", "venue": "ICCV 2025", "url": "https://arxiv.org/abs/2501.02135", "parent": "meerkat", "coauthors": ["Sayan Nag", "Subhrajyoti Dasgupta", "Yaoting Wang", "Mohamed Elhoseiny", "Ruohan Gao", "Dinesh Manocha"], "full_title": "AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs", "summary": "We introduce Audio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency. Using our benchmark we extensively evaluate 16 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human-like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model-agnostic calibrated audio-visual preference optimization based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks.", "additional_links": {"code": "https://github.com/schowdhury671/avtrustbench-", "data": "https://github.com/schowdhury671/avtrustbench-"}},
        
	{"id": "apollo", "title": "Apollo", "venue": "EMNLP 2023", "url": "https://aclanthology.org/2023.emnlp-main.629.pdf", "root_node": 1, "root_name": "Integrating Vision-Language\n2022-now", "root_color": "#FF409F", "coauthors": ["Sayan Nag", "Dinesh Manocha"], "full_title": "APoLLo: Unified Adapter and Prompt Learning for Vision Language Models", "summary": "Our method is designed to substantially improve the generalization capabilities of VLP models when they are fine-tuned in a few-shot setting. We introduce trainable cross-attention-based adapter layers in conjunction with vision and language encoders to strengthen the alignment between the two modalities.", "additional_links": {"code": "https://github.com/schowdhury671/APoLLo", "video": "https://www.youtube.com/watch?v=5OHwq3VCusA"}},
	{"id": "aspire", "title": "ASPIRE", "venue": "ACL Findings 2024", "url": "https://aclanthology.org/2024.findings-acl.22.pdf", "indirect_connections": ["avtrustbench"], "parent": "apollo", "coauthors": ["Sreyan Ghosh", "Chandra Kiran", "Sakshi S", "Sonal Kumar", "Utkarsh Tyagi", "Dinesh Manocha"], "full_title": "ASPIRE: Language-Guided Data Augmentation for Improving Robustness Against Spurious Correlations", "summary": "The paper proposes a simple yet effective solution for supplementing the training dataset with images without spurious features, for robust learning against spurious correlations via better generalization", "additional_links": {"code": "https://github.com/Sreyan88/ASPIRE"}},
	{"id": "vlmnav", "title": "VLMNav", "venue": "NAACL Findings 2024", "url": "https://arxiv.org/pdf/2403.11487v1", "parent": "aspire", "coauthors": ["Vishnu Dorbala", "Dinesh Manocha"], "full_title": "Can LLM’s Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis", "summary": "We present a novel approach to automatically synthesize `wayfinding instructions` for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references."},      
        {"id": "intent", "title": "Intent-o-Meter", "venue": "Nature Scientific Reports 2023", "url": "https://www.nature.com/articles/s41598-024-60299-w.pdf", "parent": "apollo", "coauthors": ["Trisha Mittal", "Pooja Guhan", "Dinesh Manocha", "Snikitha Chelluri"], "full_title": "Towards Determining Perceived Human Intent for Multimodal Social Media Posts using The Theory of Reasoned Action", "summary": "We propose Intent-o-meter, a perceived human intent prediction model for multimodal (image and text) social media posts. Intent-o-meter models ideas from psychology and cognitive modeling literature, in addition to using the visual and textual features for an improved perceived intent prediction.", "additional_links": {"data": "https://drive.google.com/drive/folders/1hQ1pdsuqBmKJkQ-bTQXdKuU2TkazCtbh"}},
        
	{"id": "vdesirr", "title": "VDESIRR", "venue": "ICCV 2021", "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Prasad_V-DESIRR_Very_Fast_Deep_Embedded_Single_Image_Reflection_Removal_ICCV_2021_paper.pdf", "root_node": 1, "root_name": "Computational Photography\n2021-2022", "root_color": "#FA9189", "coauthors": ["BH Pawan Prasad", "Lokesh RB", "Koushik Mitra", "Green Rosh"], "full_title": "V-DESIRR: Very Fast Deep Embedded Single Image Reflection Removal", "summary": "We have proposed a multi-scale end-to-end architecture for detecting and removing weak, medium, and strong reflections from naturally occurring images.", "additional_links": {"code": "https://github.com/ee19d005/vdesirr"}},
        {"id": "maw", "title": "MAW", "venue": "ICCP 2023", "url": "https://arxiv.org/pdf/2306.15662", "parent": "vdesirr", "coauthors": ["Jiaye Wu", "Roni Sengupta", "David Jacobs"], "full_title": "Measured Albedo in the Wild: Filling the Gap in Intrinsics Evaluation", "summary": "In order to comprehensively evaluate albedo, we collect a new dataset, Measured Albedo in the Wild (MAW), and propose three new metrics that complement WHDR", "additional_links": {"project page": "https://measuredalbedo.github.io/", "data": "https://umd.app.box.com/s/rzuzf12ooqnaxyojjgam09zctgp8fr2c"}},

	]

      build_garden(papers);
  </script>




<!--     ################################################################################################ -->
    

    <!-- Research Summaries -->
    <div id="garden_container">
        
        <div class="research-section">
            <div class="text">
                <h2>Cross-modal Generation (2024-now)</h2>
                <p>
                    The exploration of cross-modal generation is a burgeoning field that seeks to integrate and synthesize diverse data forms into cohesive outputs. <b>MAGNET</b> introduces a novel method for audio-visual RAG enhancing the fluidity with which machines process multimodal information. <b>MeLFusion (CVPR 2024)</b> demonstrates techniques for effective multimedia synthesis, offering new avenues for creative and applied uses in technology. Meanwhile, <b>Adverb (ICCV 2023)</b> focuses on visually guided audio dereverberation, setting a new standard in blending visual cues with audio refinement, ensuring clarity and precision in soundscapes. Together, these works pave the way for more seamless and intuitive interactions among various sensory modalities, underscoring the importance of integration in advancing digital experiences.
                </p>
            </div>
        </div>

        <div class="research-section">
            <div class="text">
                <h2>Audio-Visual Representation Learning (2021-now)</h2>
                <p>
                   Audio-visual representation learning addresses the need for machines to understand and summarize sensory data efficiently. In <b>EgoAdapt (ICCV 2025)</b>, new adaptation methods are developed for egocentric perspectives, enabling systems to extract and utilize contextual insights from first-person views. <b>AudViSum (BMVC 2021)</b> contributes by creating innovative summarization techniques that distill critical information from vast streams of audio-visual data, optimizing performance in real-time applications. <b>Listen to Pixels (ICIP 2021)</b> transforms how audio cues are visually represented, translating sound into visual stimuli that are easier for AI to interpret. Collectively, these studies provide a framework for advancing machine perception to interact with the world in human-like ways.
                </p>
            </div>
        </div>

        <div class="research-section">
            <div class="text">
                <h2>Audio-Visual LLMs (2024-now)</h2>
                <p>
                   In the realm of audio-visual LLMs, the integration of auditory and visual data enhances contextual comprehension and information processing. <b>AURELIA (ICCV 2025)</b> lays the foundation for audio visual reasoning facilitating nuanced interactions within complex environments. <b>AVTrustBench (ICCV 2025)</b> assesses the reliability of AI interpretations across varied audio-visual contexts, ensuring consistent and accurate model performance. <b>Meerkat (ECCV 2024)</b> introduces cutting-edge approaches to enable fine grained understanding in audio-visual LLMs by bridging the gap between sensory inputs and language understanding. These efforts collectively aim to elevate AI's capability to navigate and interpret multifaceted scenarios, akin to human cognitive processes.
                </p>
            </div>
        </div>

        <div class="research-section">
            <div class="text">
                <h2>Integrating Vision-Language (2022-now)</h2>
                <p>
                    The integration of vision and language is pivotal for enhancing AI's task performance in dynamic environments. <b>ASPIRE (ACL Findings 2024)</b> illustrates this by refining navigation and interaction tasks through improved sensory synchronization. <b>VLMNav (NAACL 2024)</b> focuses on augmenting navigation capabilities by seamlessly combining visual and verbal cues, thereby enabling more intuitive AI guidance systems. <b>Intent-o-Meter (Nature Scientific Reports 2023)</b> introduces revolutionary approaches for understanding intent, offering deeper insights into human-like AI interactions. Complementing these breakthroughs, <b>Apollo (EMNLP 2023)</b> enhances the synergy between words and images, contributing significantly to AI's communication proficiency. Together, these works chart a path towards more effective and context-aware AI systems.
                </p>
            </div>
        </div>

        <div class="research-section">
            <div class="text">
                <h2>Computational Photography (2021-2022)</h2>
                <p>
                    Computational photography is at the intersection of technology and art, striving to enhance image and video quality through advanced processing techniques. <b>MAW (ICCP 2023)</b> introduces state-of-the-art methods that refine visual clarity and detail, vital for applications demanding high-standard aesthetics. <b>VDESIRR (ICCV 2021)</b> complements this by focusing on video optimization, employing novel processing algorithms to elevate both the visual and functional quality of digital media. By advancing these techniques, these works make substantial contributions to fields where superior image quality is paramount, enriching the digital visual landscape with unparalleled precision.
                </p>
            </div>
        </div>

    </div>

</body>

</html>
