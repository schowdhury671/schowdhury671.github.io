<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Research Garden - Sanjoy Chowdhury</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <!-- Stylesheets -->
    <link rel="stylesheet" href="../main.css">
    <link rel="stylesheet" type="text/css" href="../style.css" />
    <link rel="icon" type="image/png" href="../images/IMG-20200202-WA0044_2.jpg">

    <style>
        body {
            font-family: 'Raleway', sans-serif;
            margin: 0;
            padding: 0;
            background: white;
            color: #111;
            font-weight: 400;
        }
        
        .navbar {
            display: flex;
            justify-content: center;
            background-color: #f8f8f8;
            padding: 15px;
            border-bottom: 2px solid #e0e0e0;
        }

        .navbar a {
            margin: 0 30px;
            text-decoration: none;
            color: #333;
            font-weight: bold;
        }

        .navbar a:hover {
            color: #4CAF50;
        }

        .name {
            font-family: 'Dancing Script', cursive;
            font-size: 2.5em;
            color: black;
            margin: 0;
        }

        #garden_title {
            text-align: center;
            font-size: 2.5em;
            margin-top: 20px;
        }

        #garden_container {
            text-align: center;
            margin: 20px auto;
        }

        .research-summary {
            width: 90%;
            border-collapse: collapse;
            margin: 20px auto;
        }

        .research-summary th, .research-summary td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: left;
        }

        .research-summary th {
            background-color: #f4f4f4;
        }
    </style>
    
    <link href="https://fonts.googleapis.com/css2?family=Dancing+Script:wght@700&display=swap" rel="stylesheet">
</head>

<body>
    <!-- Navigation Bar -->
    <div class="navbar">
        <a class="name" href="https://schowdhury671.github.io/">Sanjoy Chowdhury</a>
        <a href="https://schowdhury671.github.io/">Home</a>
        <a href="../others/index.html">Miscellaneous</a>
    </div>

    <!-- Research Summary -->
    <div id="garden_container">
        <h2 id="garden_title">Sanjoy's Research Summary</h2>
        <table class="research-summary">
            <tr>
                <th>Category</th>
                <th>Summary</th>
            </tr>
            <tr>
                <td>Cross-modal Generation (2024-now)</td>
                <td>
                    Recent work such as <strong>MAGNET (arXiv)</strong> and <strong>MeLFusion (CVPR 2024)</strong> explores innovative approaches to integrate and synthesize data across multiple modalities. The <strong>Adverb (ICCV 2023)</strong> paper focuses on visually guided audio dereverberation, showcasing advanced techniques in enhancing audio-visual synergy.
                </td>
            </tr>
            <tr>
                <td>Audio-Visual Representation Learning (2021-now)</td>
                <td>
                    The <strong>EgoAdapt (arXiv)</strong> and <strong>AudViSum (BMVC 2021)</strong> papers delve into adapting egocentric perspectives for efficient summarization of audio-visual data. <strong>Listen to Pixels (ICIP 2021)</strong> introduces methods to convert audio signals into visual representations, facilitating comprehensive understanding.
                </td>
            </tr>
            <tr>
                <td>Audio-Visual LLMs (2024-now)</td>
                <td>
                    In audio-visual large language models, <strong>AURELIA (arXiv)</strong>, <strong>AVTrustBench (ECCV 2024)</strong>, and <strong>Meerkat (ECCV 2024)</strong> investigate frameworks for integrating audio-visual interactions, enhancing contextual understanding and information processing in AI systems.
                </td>
            </tr>
            <tr>
                <td>Integrating Vision-Language (2022-now)</td>
                <td>
                    <strong>ASPIRE (ACL Findings 2024)</strong> and <strong>VLMNav (NAACL Findings 2024)</strong> highlight strides in integrating vision with language for AI tasks. <strong>Intent-o-Meter (Nature Scientific Reports 2023)</strong> introduces techniques for interpreting intent, while <strong>Apollo (EMNLP 2023)</strong> enhances word-image pair associations.
                </td>
            </tr>
            <tr>
                <td>Computational Photography (2021-2022)</td>
                <td>
                    In computational photography, <strong>MAW (ICCP 2023)</strong> and <strong>VDESIRR (ICCV 2021)</strong> focus on optimizing image and video quality through advanced processing, enhancing the aesthetic and functional aspects of visual media.
                </td>
            </tr>
        </table>
    </div>

</body>

</html>
