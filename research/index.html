<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Sanjoy Chowdhury</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Raleway:wght@300;600&display=swap" rel="stylesheet">

  <!-- Stylesheets -->
  <link rel="stylesheet" href="main.css">
  <link rel="stylesheet" href="garden.css">
  <link rel="stylesheet" type="text/css" href="{{ site.baseurl }}/style.css" />
  <style id="flower_css"></style>

  <!-- Canonical Link -->
  <link rel="canonical" href="{{ page.url | replace:'index.html','' | prepend: site.baseurl | prepend: site.url }}">

  <!-- Embedded Styles -->
  
  <style>
    body {
      font-family: 'Raleway', sans-serif; /* Use Raleway consistently */
      margin: 0;
      padding: 0;
      background: white;
      color: #111;
      font-weight: 400; /* Set default weight */
    }

.navbar {
  display: flex;
  justify-content: center; /* Center horizontally */
  align-items: center;     /* Center vertically */
  padding: 15px 10%;
  background-color: #fff;
  border-bottom: 1px solid #eee;
}

.navbar > * {  /* Add some space between the name and links */
    margin: 40 120px;
}

    .site-name {
      font-size: 38px;
      font-weight: 600;
      margin-right: 20px; /* Adjust as needed */
    }

    .nav-links {  /* Style the nav container */
      display: flex; /* Make nav links also flex */
    }

    .nav-links a {
      margin: 0 10px;
      text-decoration: none;
      color: #111;
      font-weight: 500; /* Or keep consistent 400 */
    }

    /* Consistent heading fonts */
    h1, h2, h3, h4, h5, h6 {
        font-family: 'Raleway', sans-serif;
        font-weight: 600; /* Or adjust as preferred */
    }

    .page-wrapper {
      width: 80%;
      margin: 0 auto;
      padding-top: 40px;
    }

    .code-box {
      border: 2px solid #4CAF50;
      padding: 15px 20px;
      border-radius: 8px;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
      font-family: 'Courier New', Courier, monospace;
      background-color: #f9f9f9;
      display: inline-block;
    }

    .center-content {
      text-align: center;
    }

    .red-text {
      color: red;
    }
  </style>
</head>

<body>

  <header class="navbar">
    <div class="site-name">Sanjoy Chowdhury</div>
    <nav class="nav-links">
      <a href="/about" style="font-size: large;">About</a>
      <a href="/research" style="font-size: large;">Research</a>
      <a href="/others" style="font-size: large;">Miscellaneous</a>
    </nav>
  </header>
<!-- add research -->

<table style="width:70%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <h2><font color="blue">Selected publications </font></h2>
              <p>
                I am interested in solving computer vision, computer audition, and machine learning problems and applying them to broad AI applications. My research focuses on applying multi-modal learning (Vision + X) for generative modeling and holistic cross-modal understanding with minimal supervision. Representative papers are <span style="background-color: #ffffe6">highlighted.</span>	      
	      </p>
            </td>
          </tr>
        </table>

	<table style="width:70%;border:00px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/aurelia.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><img src="../images/new.png" alt="project image" width="38" height="38" /> <font color="#0000FF">AURELIA: Test-time Reasoning Distillation in Audio-Visual LLMs </font> </h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Hanan Gani*, Nishit Anand, Sayan Nag, Ruohan Gao, Mohamed Elhoseiny, Salman Khan, Dinesh Manocha 

               <br>
<!--               <em>European Conference on Computer Vision (<font color="#ff0000">ECCV</font>)</em>, 2024 -->
              <br> 
              
               <a href="https://arxiv.org/pdf/2503.23219">ArXiv PrePrint</a> /
	       <a href="https://github.com/schowdhury671/aurelia">Project Page and Dataset (Coming soon!)</a> 
<!-- 	       <a href="https://drive.google.com/file/d/1P0thVIrkMjwRttn1oKBX_X6owiTQ3GR-/view?usp=sharing">Poster</a> /
	       <a href="https://youtu.be/gLFf6hxIj7k">Video</a> /
	       <a href="https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVFIT%20dataset&ga=1">Dataset</a> /
 	       <a href="https://github.com/schowdhury671/meerkat/tree/main">Code</a>  -->
              
              <p></p>
              <p> In this paper, we introduce AURELIA, a novel actor-critic based audio-visual reasoning framework that distils structured, step-by-step reasoning into 
		AVLLMs at test time, improving their ability to process
		complex multi-modal inputs without additional training or
		fine-tuning. To further advance AVLLM reasoning skills, we
		present AVReasonBench, a challenging benchmark comprising 4500 audio-visual questions, each paired with detailed
		step-by-step reasoning. Our benchmark spans six distinct
		tasks, including AV-GeoIQ, which evaluates AV reasoning
		combined with geographical and cultural knowledge
	      </p>

            </td>
          </tr>
		
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/avtrustbench_teaser.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><img src="../images/new.png" alt="project image" width="38" height="38" /> <font color="#0000FF">AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs </font> </h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sayan Nag*, Subhrajyoti Dasgupta, Yaoting Wang, Mohamed Elhoseiny, Ruohan Gao, Dinesh Manocha 

               <br>
<!--               <em>European Conference on Computer Vision (<font color="#ff0000">ECCV</font>)</em>, 2024 -->
              <br> 
              
               <a href="https://arxiv.org/abs/2501.02135">ArXiv PrePrint</a> /
	       <a href="https://github.com/schowdhury671/avtrustbench-">Project Page and Dataset (Coming soon!)</a> 
<!-- 	       <a href="https://drive.google.com/file/d/1P0thVIrkMjwRttn1oKBX_X6owiTQ3GR-/view?usp=sharing">Poster</a> /
	       <a href="https://youtu.be/gLFf6hxIj7k">Video</a> /
	       <a href="https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVFIT%20dataset&ga=1">Dataset</a> /
 	       <a href="https://github.com/schowdhury671/meerkat/tree/main">Code</a>  -->
              
              <p></p>
              <p> We introduce Audio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency. Using our benchmark we extensively evaluate 16 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human-like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model-agnostic calibrated audio-visual preference optimization based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks.  
	      </p>

            </td>
          </tr>
		
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/meerkat_overview.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><img src="../images/new.png" alt="project image" width="38" height="38" /> <img src="../images/meerkat_bg_removed.png" alt="project image" width="20" height="20" /><font color="#0000FF">Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time </font> </h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sayan Nag*, Subhrajyoti Dasgupta*, Jun Chen, Mohamed Elhoseiny, Ruohan Gao, Dinesh Manocha 

              <br>
              <em>European Conference on Computer Vision (<font color="#ff0000">ECCV</font>)</em>, 2024
              <br>
              
               <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08071.pdf">Paper</a>/ 
	       <a href="https://schowdhury671.github.io/meerkat_project/">Project Page</a> /
	       <a href="https://drive.google.com/file/d/1P0thVIrkMjwRttn1oKBX_X6owiTQ3GR-/view?usp=sharing">Poster</a> /
	       <a href="https://youtu.be/gLFf6hxIj7k">Video</a> /
	       <a href="https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVFIT%20dataset&ga=1">Dataset</a> /
 	       <a href="https://github.com/schowdhury671/meerkat/tree/main">Code</a> 
              
              <p></p>
              <p>We present Meerkat, an audio-visual LLM equipped with a
		fine-grained understanding of image and audio both spatially and temporally.
		With a new modality alignment module based on optimal transport and a
		cross-attention module that enforces audio-visual consistency, Meerkat can
		tackle challenging tasks such as audio referred image grounding, image guided
		audio temporal localization, and audio-visual fact-checking. Moreover, we
		carefully curate a large dataset AVFIT that comprises 3M instruction tuning
		samples collected from open-source datasets, and introduce MeerkatBench that
		unifies five challenging audio-visual tasks.  
	      </p>

            </td>
          </tr>

	<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/aspire.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">ASPIRE: Language-Guided Data Augmentation for Improving Robustness Against Spurious Correlations </font></h3>
              <br>
              Sreyan Ghosh*, Chandra Kiran Reddy Evuru*, Sonal Kumar, Utkarsh Tyagi, Sakshi Singh, <strong>Sanjoy Chowdhury</strong>, Dinesh Manocha

              <br>
              <em><font color="#ff0000">ACL Findings 2024</font></em>
              <br>
              
               <a href="https://aclanthology.org/2024.findings-acl.22.pdf">Paper</a> /
	       <a href="https://github.com/Sreyan88/ASPIRE">Code</a>    
              
              <p></p>
              <p>The paper proposes a simple yet effective solution for supplementing the training dataset with images without spurious features, for robust learning against spurious correlations via better generalization </p>

            </td>
          </tr>



	<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/intent-o-meter.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">Towards Determining Perceived Human Intent for Multimodal Social Media Posts using The Theory of Reasoned Action </font></h3>
              <br>
              Trisha Mittal, <strong>Sanjoy Chowdhury</strong>, Pooja Guhan, Snikhita Chelluri, Dinesh Manocha 

              <br>
              <em><font color="#ff0000">Nature Scientific Reports</font></em>
              <br>
              
               <a href="https://www.nature.com/articles/s41598-024-60299-w.pdf">Paper</a> /
	       <a href="https://drive.google.com/drive/folders/1hQ1pdsuqBmKJkQ-bTQXdKuU2TkazCtbh">Dataset</a>    
              
              <p></p>
              <p>We propose Intent-o-meter, a perceived human intent prediction model for multimodal (image and text) social media posts. Intent-o-meter models ideas from psychology and cognitive modeling literature, in addition to using the visual and textual features for an improved perceived intent prediction. </p>

            </td>
          </tr>

	<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/llmnav.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">Can LLM’s Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis </font></h3>
              <br>
              Vishnu Sashank Dorbala, <strong>Sanjoy Chowdhury</strong>, Dinesh Manocha 

              <br>
              <em>Annual Conference of the North American Chapter of the Association for Computational Linguistics (<font color="#ff0000">NAACL</font>)</em>, 2024
              <br>
              
               <a href="https://arxiv.org/pdf/2403.11487v1">Paper</a> 
              
              <p></p>
              <p>We present a novel approach to automatically synthesize “wayfinding instructions" for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references. </p>

            </td>
          </tr>
			
        <table style="width:70%;border:00px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/melfusion-diagram.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">MeLFusion: Synthesizing Music from Image and Language Cues using Diffusion Models </font> (<font color="#ff0000"> Highlight, Top 2.8% </font>) </h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sayan Nag*, Joseph KJ, Balaji Vasan Srinivasan, Dinesh Manocha 

              <br>
              <em>Conference on Computer Vision and Pattern Recognition (<font color="#ff0000">CVPR</font>)</em>, 2024
              <br>
              
               <a href="https://www.arxiv.org/pdf/2406.04673">Paper</a>/ 
	       <a href="https://schowdhury671.github.io/melfusion_cvpr2024/">Project Page</a> /
	       <a href="https://drive.google.com/file/d/1b_f-DFGWAQvL1ZbPX2gW_BJAweBm5xS-/view">Poster</a> /
	       <a href="https://youtu.be/_CNJeiYwLsE?si=1QnqvgwbC6lq_86e">Video</a> /
	       <a href="https://umd0-my.sharepoint.com/:f:/g/personal/sanjoyc_umd_edu/Eok6RG9QIZhNlGubG8-VsDIBhNMK6OOVAWuHpryEC3VnJw">Dataset</a> /
	       <a href="https://github.com/schowdhury671/melfusion/tree/main">Code</a> 
              
              <p></p>
              <p>We propose MeLFusion, a model that can effectively use cues from a textual description and the corresponding image to synthesize music. MeLFusion is a text-to-music diffusion model with a novel "visual synapse", which effectively infuses the semantics from the visual modality into the generated music. To facilitate research in this area, we introduce a new dataset MeLBench, and propose a new evaluation metric IMSM. </p>

            </td>
          </tr>	

	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/apollo.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">APoLLo <img src="../images/rocket1.png" alt="project image" width="20" height="20" />: Unified Adapter and Prompt Learning for Vision Language Models </font></h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sayan Nag*, Dinesh Manocha 

              <br>
              <em>Conference on Empirical Methods in Natural Language Processing (<font color="#ff0000">EMNLP</font>)</em>, 2023
              <br>
              
               <a href="https://aclanthology.org/2023.emnlp-main.629.pdf">Paper</a> / 
	       <a href="https://gamma.umd.edu/pro/vision_language/apollo/">Project Page</a> /
	       <a href="https://drive.google.com/file/d/1KiE_LZAvFEHREibZ4G9vTpMrCtk1UEt8/view?usp=sharing">Poster</a> /
	       <a href="https://youtu.be/5OHwq3VCusA">Video</a> /
<!-- 	       <a href="https://drive.google.com/file/d/1mudcCl0UR9tehffOk-PKxKWDzCcRChIO/view">Poster</a> / -->
	       <a href="https://github.com/schowdhury671/APoLLo">Code</a>
              
              <p></p>
              <p>Our method is designed to substantially improve the generalization capabilities of VLP models when they are fine-tuned in a few-shot setting. We introduce trainable cross-attention-based adapter layers in conjunction with vision and language encoders to strengthen the alignment between the two modalities. </p>

            </td>
          </tr>

	   <table style="width:70%;border:00px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/adverb.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">AdVerb: Visually Guided Audio Dereverberation </font></h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sreyan Ghosh*, Subhrajyoti Dasgupta, Anton Ratnarajah, Utkarsh Tyagi, Dinesh Manocha 

              <br>
              <em>International Conference on Computer Vision (<font color="#ff0000">ICCV</font>)</em>, 2023
              <br>
              
               <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chowdhury_AdVerb_Visually_Guided_Audio_Dereverberation_ICCV_2023_paper.pdf">Paper</a> / 
	       <a href="https://schowdhury671.github.io/adverb/">Project Page</a> /
	       <a href="https://www.youtube.com/watch?v=dZuR-pZ9uM0">Video</a> /
	       <a href="https://drive.google.com/file/d/1mudcCl0UR9tehffOk-PKxKWDzCcRChIO/view">Poster</a> /
	       <a href="https://github.com/Sreyan88/AdVerb-dereverb">Code</a>
              
              <p></p>
              <p>We present a novel audio-visual dereverberation framework that uses visual cues in addition to the reverberant sound to estimate clean audio. </p>

            </td>
          </tr>
		
	   <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/iccp.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">Measured Albedo in the Wild: Filling the Gap in Intrinsics Evaluation </font></h3>
              <br>
              Jiaye Wu, <strong>Sanjoy Chowdhury</strong>, Hariharmano Shanmugaraja, David Jacobs, Soumyadip Sengupta 

              <br>
              <em>International Conference on Computational Photography (<font color="#ff0000">ICCP</font>)</em>, 2023
              <br>
              
               <a href="https://arxiv.org/pdf/2306.15662.pdf">Paper</a> / 
	       <a href="https://measuredalbedo.github.io/">Project Page</a> /     
               <a href="https://umd.app.box.com/s/rzuzf12ooqnaxyojjgam09zctgp8fr2c">Dataset</a> 
              
              <p></p>
              <p> In order to comprehensively evaluate albedo, we collect a new dataset, Measured Albedo in the Wild (MAW), and propose three new metrics that complement WHDR</p>

            </td>
          </tr>
		
		
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/audViSum_bmvc.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">AudViSum: Self-Supervised Deep Reinforcement Learning for Diverse Audio-Visual Summary Generation </font></h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Aditya P. Patra*, Subhrajyoti Dasgupta, Ujjwal Bhattacharya

              <br>
              <em>British Machine Vision Conference (<font color="#ff0000">BMVC</font>)</em>, 2021
              <br>
              
              <a href="https://www.bmvc2021-virtualconference.com/assets/papers/1430.pdf">Paper</a> / 
              
              <a href="https://github.com/schowdhury671/AudViSum">Code</a> / 
		    
              <a href="https://www.youtube.com/watch?v=Hier-zMWcc0">Presentation</a> 
              
              <p></p>
              <p>Introduced a novel deep reinforcement learning-based self-supervised audio-visual summarization model that leverages both audio and visual information to generate diverse yet semantically meaningful summaries.</p>

            </td>
          </tr>
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/rr_iccv.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">V-DESIRR: Very Fast Deep Embedded Single Image Reflection Removal </font></h3>
              <br>
              B H Pawan Prasad, Green Rosh K S, Lokesh R B, Kaushik Mitra, <strong>Sanjoy Chowdhury</strong>

              <br>
              <em>International Conference on Computer Vision (<font color="#ff0000">ICCV</font>)</em>, 2021
              <br>
              
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Prasad_V-DESIRR_Very_Fast_Deep_Embedded_Single_Image_Reflection_Removal_ICCV_2021_paper.pdf">Paper</a> /
              
              
              
              <a href="https://github.com/ee19d005/vdesirr">Code</a> 
              
              
              
<!--               <a href="https://github.com/ee19d005/vdesirr">slides</a>  -->
              
              <p></p>
              <p>We have proposed a multi-scale end-to-end architecture for detecting and removing weak, medium, and strong reflections from naturally occurring images.</p>

            </td>
          </tr>
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/audVi_coseg_icip.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">Listen to the Pixels </font></h3>
              <br>
              <strong>Sanjoy Chowdhury</strong>, Subhrajyoti Dasgupta, Sudip Das, Ujjwal Bhattacharya

              <br>
              <em>International Conference on Image Processing (<font color="#ff0000">ICIP</font>)</em>, 2021
              <br>
              
              <a href="https://ieeexplore.ieee.org/document/9506019">Paper</a> /
              
              
              
              <a href="https://github.com/schowdhury671/Audio-visual-joint-segmentation">Code</a> /
              
              
              
              <a href="https://www.youtube.com/watch?v=xUwzSQaQ9oQ">Presentation</a> 
              
              <p></p>
              <p>In this study, we exploited the concurrency between audio and visual modalities in an attempt to solve the joint audio-visual segmentation problem in a self-supervised manner.</p>

            </td>
          </tr>
          
          <!--<tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/fuzzy.jpeg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>A Survey on Fuzzy Set Theoretic Approaches for Image Segmentation</h3>
              <br>
              Ajoy Mondal*, <strong>Sanjoy Chowdhury*</strong>

              <br>
              <em><font color="#ff0000">Soft Computing</font></em>, 2022 (Under review)
              <br>
              
               <a href="https://ieeexplore.ieee.org/document/9506019">Paper</a> / 
              
              
              
               <a href="https://github.com/schowdhury671/Audio-visual-joint-segmentation">code</a> / 
              
              
              
               <a href="https://sigport.org/documents/listen-pixels">slides</a> / 
              
              <p></p>
              <p>The survey paper performs an in-depth comparison and analysis on fuzzy set theory-based image segmentation techniques.</p>

            </td>
          </tr>-->
          
          <!--<tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/not_too_deep_cnn.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Not Too Deep CNN for Face Detection in Real-Life Scenario</h3>
              <br>
              <strong>Sanjoy Chowdhury</strong>, Parthasarathi Mukherjee, Ujjwal Bhattacharya

              <br>
              <em>International Conference on Next Generation Computing Technologies, Springer</em>, 2017 (<font color="#ff0000">Best paper award, Oral</font>)
              <br>
              
              <a href="https://link.springer.com/chapter/10.1007/978-981-10-8660-1_66">Paper</a> /
              
              
              
              <a href="https://github.com/schowdhury671/Not-too-deep-CNN">Code</a> 
              
              
              
               <a href="https://github.com/schowdhury671/Not-too-deep-CNN">Slides</a>  
              
              <p></p>
              <p>Proposed a multi-scale face detection framework that is capable of detecting faces of multiple sizes and different orientations in low-resolution images while achieving sufficiently low latency and modest detection rates in the wild.</p>

            </td>
          </tr>-->
          
         <!--<tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/citation.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Classification of Citation in Scientific Articles</h3>
              <br>
              <strong>Sanjoy Chowdhury</strong>, Harsh Vardhan, Pabitra Mitra, Dinabandhu Bhandari

              <br>
              <em>National Conference on Recent Advances in Science and Technology</em>, 2016 (<font color="#ff0000">Oral</font>)
              <br>
              
              <a href="https://github.com/schowdhury671/Citation-classification/blob/main/Citation_Classification_Abstract.docx">Abstract</a> /
              
              
              
              <a href="https://github.com/schowdhury671/Citation-classification">Code</a> 
              
              
              
              <p></p>
              <p>Designed a multi-class classification system to find out the type of citation i.e. a citation belongs to which facet. We aimed to
achieve this by extracting and analyzing citation information from the text.</p>

            </td>
          </tr>-->
          
          </table>
          
        <br><br><br>

          
</body>

</html>
