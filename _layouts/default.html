<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Sanjoy's webpage</title>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="Sanjoy" content="Sanjoy's webpage" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="{{ site.baseurl }}/style.css" />
  <link rel="canonical" href="{{ page.url | replace:'index.html','' | prepend: site.baseurl | prepend: site.url }}">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

</head>



<body>
  <table style="width:80%;max-width:640px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:70%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <h1>
                Sanjoy Chowdhury
              </h1>
              <p>I am a second year CS PhD student at <a href="https://www.cs.umd.edu/"> University of Maryland, College Park </a> advised by <a href="https://www.cs.umd.edu/people/dmanocha">Prof. Dinesh Manocha</a>. I am broadly interested in multi-modal learning and its different applications. My current line of research involves studying the interplay between vision and audio modality and learning their holistic understanding in a real-world setting.
	      </p>
	       <p>I am currently working as a research scientist intern at Meta Reality Labs. Before this, I was a student researcher at <a href="https://research.google/">Google Research </a> with <a href = "https://scholar.google.co.in/citations?user=4zgNd2UAAAAJ&hl=en">Avisek Lahiri </a> and <a href = "https://research.google/people/vivek-kwatra/">Vivek Kwatra </a> in the Talking heads team on speech driven facial synthesis. Previously, I spent a wonderful summer with <a href = "https://research.adobe.com/">Adobe Research </a> working with <a href = "https://josephkj.in/"> Joseph K J </a> in the Multi-modal AI team as a research PhD intern on multi-modal audio generation. I am also fortunate to have had the chance to work with <a href = "https://www.cs.utexas.edu/users/grauman/"> Prof. Kristen Grauman </a>, <a href = "https://www.mohamed-elhoseiny.com/"> Prof. Mohamed Elhoseiny </a> and <a href = "https://ruohangao.github.io/"> Ruohan Gao </a> among other wonderful collaborators. 
		</p>        
		<p>Before this, I was working as a Machine Learning Scientist with the Camera and Video AI team at <a href="https://sharechat.com/about">ShareChat</a>, India. I was also a visiting researcher at the Computer Vision and Pattern Recognition Unit at Indian Statistical Institute Kolkata under <a href="https://www.isical.ac.in/~ujjwal/"> Prof. Ujjwal Bhattacharya</a>.  Even before, I was a Senior Research Engineer with the Vision Intelligence Group at <a href="https://research.samsung.com/sri-b">Samsung R&D Institute Bangalore</a>.  I primarily worked on developing novel AI-powered solutions for different smart devices of Samsung. 
              </p>
              <p>
                I received my MTech in Computer Science & Engineering from <a href="https://www.iiit.ac.in/">IIIT Hyderabad</a> where I was fortunate to be advised by <a href="https://faculty.iiit.ac.in/~jawahar/">Prof. C V Jawahar</a>. During my undergrad, I worked as a research intern under <a href="http://cse.iitkgp.ac.in/~pabitra/">Prof. Pabitra Mitra </a> at IIT Kharagpur and the CVPR Unit at ISI Kolkata. 
	      </p>
              <p style="text-align:center">
                <a target="_blank" href="https://mailhide.io/e/PbuZVINh">Email</a> &nbsp;/&nbsp;
                <a href="https://github.com/schowdhury671">GitHub</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=CEdJKCIAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/sanjoy2528/"> LinkedIn</a> &nbsp;/&nbsp;
		<a href="https://twitter.com/schowdhury671"> Twitter </a>      
              </p>
            </td>
            <td style="padding:2.5%;width:80%;max-width:80%">
              <img style="width:100%;max-width:100%;border-radius: 50%" alt="profile photo" src="../images/IMG-20200202-WA0044_2.jpg">
		    <br> <br>
<!-- 		    &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp <a href="https://iribe.umd.edu/"><font color="#800080">Iribe #5116, 8125 Paint Branch Dr</font> <a href> <br>
		    &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  <font color="#800080">College Park, MD 20742 </font> <br> 
		    &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp <font color="#800080">sanjoyc[at]umd[dot]edu </font>     -->
            </td>
          </tr>
        </table>
        
	      
	  <table style="width:70%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              
        

	      
<h2><font color="blue">Updates </font></h2>
      <p>
            <div style="width:80%;overflow-y:scroll; height:250px;">
                <ul id="News">
        <p>
        </p>

	<li>
July 2024 - One paper got accepted to <a href="https://eccv.ecva.net/"> <font color="#ff0000"> ECCV 2024 </font></a> <img src="../images/new.png" alt="project image" width="20" height="20" />
        </li>
		<li>
June 2024 - Invited talk at the <a href="https://sightsound.org/"> <font color="#ff0000"> Sight and Sound workshop </font></a> at CVPR 2024
        </li>

        <li> 
May 2024 - Joined <font color="#ff0000"> Meta Reality Labs </font> as a Research Scientist intern. <img src="../images/new.png" alt="project image" width="20" height="20" />
	</li>
        
        <li>

May 2024 - Our <a href="https://www.nature.com/articles/s41598-024-60299-w.pdf"> paper </a> on determining perceived audience intent from multi-modal social media posts got accepted to <a href="https://www.nature.com/srep/"> <font color="#ff0000"> Nature Scientific Reports</font></a>
        </li>

        <li>
Mar 2024 - <a href="https://arxiv.org/pdf/2403.11487.pdf"> Paper </a> on LLM guided navigational instruction generation got accepted to <a href="https://2024.naacl.org/"> <font color="#ff0000"> NAACL 2024 </font></a> 

</li>


        <li>
Feb 2024 - MeLFusion (<font color="#ff0000"> <b> Highlight, Top 2.8% </b> </font>) got accepted to <a href="https://cvpr.thecvf.com/"> <font color="#ff0000"> CVPR 2024 </font></a>
        </li>

        <li>
Feb 2024 - Joined <font color="#ff0000"> Google Research </font> as a student researcher.

        </li>
        <li>
Oct 2023 - APoLLo gets accepted to <a href="https://2023.emnlp.org/"><font color="#ff0000">EMNLP 2023</font></a>

        </li>
        
        <li>
Oct 2023 - Invited talk on AdVerb at <a href="https://av4d.org/"> <font color="#ff0000">AV4D Workshop, ICCV 2023</font></a>

        </li>
        <li>
July 2023 - AdVerb got accepted to <a href="https://iccv2023.thecvf.com/"><font color="#ff0000">ICCV 2023</font></a>

        </li>
        <li>
May 2023 - Joined <a href="https://research.adobe.com/"><font color="#ff0000">Adobe Research</font></a> as a research intern.</a>
        </li>
        
        <li>
Aug 2022 - Joined as a CS PhD student at <a href="https://www.cs.umd.edu/"><font color="#ff0000">University of Maryland College Park</font> </a>. Awarded <font color="red"> Dean's fellowship. </font>

        </li>

        <li>
Oct 2021 - Paper on audio-visual summarization accepted in <font color="#ff0000">BMVC 2021</font>.
        </li>

        <li>
Sep 2021 - <a href=""><font color="#ff0000">Blog</font> </a> on Video Quality Enhancement released at Tech @ ShareChat.

        </li>


        <li>
July 2021 - Paper on reflection removal got accepted in <font color="#ff0000">ICCV 2021</font>.


        </li>

        <li>
June 2021 - Joined <font color="#ff0000">ShareChat</font> Data Science team.

        </li>

        <li>
May 2021 - Paper on audio-visual joint segmentation accepted in <font color="#ff0000">ICIP 2021</font>.

        </li>

        <li>
Dec 2018 - Accepted <font color="#ff0000">Samsung Research</font> offer. Will be joining in June'19.
        </li>

        <li>

Sep 2018 - Received <font color="#ff0000">Dean's Merit List Award </font> for academic excellence at IIIT Hyderabad.
        </li>


        <li>
Oct 2017 - Our work on a multi-scale, low-latency face detection framework received <font color="#ff0000">Best Paper Award</font> at NGCT-2017.

        </li>


            </ul>
            </div>
      </p>
    </td>
          </tr>
        </table>
	      
	<br><br>     
        <table style="width:70%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <h2><font color="blue">Selected publications </font></h2>
              <p>
                My research is at the intersection of Computer vision, deep learning with a focus on multi-modal learning (Vision + X), generative modeling, visual understanding, and their various applications. I'm broadly interested in studying the interplay between different modalities with minimal supervision. 	      
	      
	      </p>
            </td>
          </tr>
        </table>

	<table style="width:70%;border:00px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/meerkat_overview.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><img src="../images/new.png" alt="project image" width="38" height="38" />Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time </h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sayan Nag*, Subhrajyoti Dasgupta*, Jun Chen, Mohamed Elhoseiny, Ruohan Gao, Dinesh Manocha 

              <br>
              <em>European Conference on Computer Vision (<font color="#ff0000">ECCV</font>)</em>, 2024
              <br>
              
               <a href="">Paper</a>/ 
	       <a href="https://schowdhury671.github.io/meerkat/">Project Page</a> (coming soon) /
<!-- 	       <a href="https://drive.google.com/file/d/1b_f-DFGWAQvL1ZbPX2gW_BJAweBm5xS-/view">Poster</a> / -->
<!-- 	       <a href="https://youtu.be/_CNJeiYwLsE?si=1QnqvgwbC6lq_86e">Video</a> / -->
<!-- 	       <a href="https://umd0-my.sharepoint.com/:f:/g/personal/sanjoyc_umd_edu/Eok6RG9QIZhNlGubG8-VsDIBhNMK6OOVAWuHpryEC3VnJw">Dataset</a> / -->
<!-- 	       <a href="https://github.com/schowdhury671/melfusion/tree/main">Code</a>  -->
              
              <p></p>
              <p>Leveraging Large Language Models' remarkable proficiency in text-based tasks,
		recent works on Multi-modal LLMs (MLLMs) extend them to other modalities like
		vision and audio. However, the progress in these directions has been mostly
		focused on tasks that only require a coarse-grained understanding of the
		audio-visual semantics. We present Meerkat, an audio-visual LLM equipped with a
		fine-grained understanding of image and audio both spatially and temporally.
		With a new modality alignment module based on optimal transport and a
		cross-attention module that enforces audio-visual consistency, Meerkat can
		tackle challenging tasks such as audio referred image grounding, image guided
		audio temporal localization, and audio-visual fact-checking. Moreover, we
		carefully curate a large dataset AVFIT that comprises 3M instruction tuning
		samples collected from open-source datasets, and introduce MeerkatBench that
		unifies five challenging audio-visual tasks. We achieve state-of-the-art
		performance on all these downstream tasks with a relative improvement of up to 37.12%. 
	      </p>

            </td>
          </tr>

			
        <table style="width:70%;border:00px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/melfusion-diagram.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>MeLFusion: Synthesizing Music from Image and Language Cues using Diffusion Models (<font color="#ff0000"> Highlight, Top 2.8% </font>) </h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sayan Nag*, Joseph KJ, Balaji Vasan Srinivasan, Dinesh Manocha 

              <br>
              <em>Conference on Computer Vision and Pattern Recognition (<font color="#ff0000">CVPR</font>)</em>, 2024
              <br>
              
               <a href="https://www.arxiv.org/pdf/2406.04673">Paper</a>/ 
	       <a href="https://schowdhury671.github.io/melfusion_cvpr2024/">Project Page</a> /
	       <a href="https://drive.google.com/file/d/1b_f-DFGWAQvL1ZbPX2gW_BJAweBm5xS-/view">Poster</a> /
	       <a href="https://youtu.be/_CNJeiYwLsE?si=1QnqvgwbC6lq_86e">Video</a> /
	       <a href="https://umd0-my.sharepoint.com/:f:/g/personal/sanjoyc_umd_edu/Eok6RG9QIZhNlGubG8-VsDIBhNMK6OOVAWuHpryEC3VnJw">Dataset</a> /
	       <a href="https://github.com/schowdhury671/melfusion/tree/main">Code</a> 
              
              <p></p>
              <p>We propose MeLFusion, a model that can effectively use cues from a textual description and the corresponding image to synthesize music. MeLFusion is a text-to-music diffusion model with a novel "visual synapse", which effectively infuses the semantics from the visual modality into the generated music. To facilitate research in this area, we introduce a new dataset MeLBench, and propose a new evaluation metric IMSM. </p>

            </td>
          </tr>	

	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/apollo.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>APoLLo <img src="../images/rocket1.png" alt="project image" width="20" height="20" />: Unified Adapter and Prompt Learning for Vision Language Models</h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sayan Nag*, Dinesh Manocha 

              <br>
              <em>Conference on Empirical Methods in Natural Language Processing (<font color="#ff0000">EMNLP</font>)</em>, 2023
              <br>
              
               <a href="https://aclanthology.org/2023.emnlp-main.629.pdf">Paper</a> / 
	       <a href="https://gamma.umd.edu/pro/vision_language/apollo/">Project Page</a> /
	       <a href="https://drive.google.com/file/d/1KiE_LZAvFEHREibZ4G9vTpMrCtk1UEt8/view?usp=sharing">Poster</a> /
	       <a href="https://youtu.be/5OHwq3VCusA">Video</a> /
<!-- 	       <a href="https://drive.google.com/file/d/1mudcCl0UR9tehffOk-PKxKWDzCcRChIO/view">Poster</a> / -->
	       <a href="https://github.com/schowdhury671/APoLLo">Code</a>
              
              <p></p>
              <p>Our method is designed to substantially improve the generalization capabilities of VLP models when they are fine-tuned in a few-shot setting. We introduce trainable cross-attention-based adapter layers in conjunction with vision and language encoders to strengthen the alignment between the two modalities. </p>

            </td>
          </tr>

		
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/adverb.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>AdVerb: Visually Guided Audio Dereverberation</h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sreyan Ghosh*, Subhrajyoti Dasgupta, Anton Ratnarajah, Utkarsh Tyagi, Dinesh Manocha 

              <br>
              <em>International Conference on Computer Vision (<font color="#ff0000">ICCV</font>)</em>, 2023
              <br>
              
               <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chowdhury_AdVerb_Visually_Guided_Audio_Dereverberation_ICCV_2023_paper.pdf">Paper</a> / 
	       <a href="https://schowdhury671.github.io/adverb/">Project Page</a> /
	       <a href="https://www.youtube.com/watch?v=dZuR-pZ9uM0">Video</a> /
	       <a href="https://drive.google.com/file/d/1mudcCl0UR9tehffOk-PKxKWDzCcRChIO/view">Poster</a> /
	       <a href="https://github.com/Sreyan88/AdVerb-dereverb">Code</a>
              
              <p></p>
              <p>We present a novel audio-visual dereverberation framework that uses visual cues in addition to the reverberant sound to estimate clean audio. </p>

            </td>
          </tr>
		
	   <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/iccp.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Measured Albedo in the Wild: Filling the Gap in Intrinsics Evaluation</h3>
              <br>
              Jiaye Wu, <strong>Sanjoy Chowdhury</strong>, Hariharmano Shanmugaraja, David Jacobs, Soumyadip Sengupta 

              <br>
              <em>International Conference on Computational Photography (<font color="#ff0000">ICCP</font>)</em>, 2023
              <br>
              
               <a href="https://arxiv.org/pdf/2306.15662.pdf">Paper</a> / 
	       <a href="https://measuredalbedo.github.io/">Project Page</a> /     
               <a href="https://umd.app.box.com/s/rzuzf12ooqnaxyojjgam09zctgp8fr2c">Dataset</a> 
              
              <p></p>
              <p> In order to comprehensively evaluate albedo, we collect a new dataset, Measured Albedo in the Wild (MAW), and propose three new metrics that complement WHDR</p>

            </td>
          </tr>
		
		
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/audViSum_bmvc.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>AudViSum: Self-Supervised Deep Reinforcement Learning for Diverse Audio-Visual Summary Generation</h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Aditya P. Patra*, Subhrajyoti Dasgupta, Ujjwal Bhattacharya

              <br>
              <em>British Machine Vision Conference (<font color="#ff0000">BMVC</font>)</em>, 2021
              <br>
              
              <a href="https://www.bmvc2021-virtualconference.com/assets/papers/1430.pdf">Paper</a> / 
              
              <a href="https://github.com/schowdhury671/AudViSum">Code</a> / 
		    
              <a href="https://www.youtube.com/watch?v=Hier-zMWcc0">Presentation</a> 
              
              <p></p>
              <p>Introduced a novel deep reinforcement learning-based self-supervised audio-visual summarization model that leverages both audio and visual information to generate diverse yet semantically meaningful summaries.</p>

            </td>
          </tr>
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/rr_iccv.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>V-DESIRR: Very Fast Deep Embedded Single Image Reflection Removal</h3>
              <br>
              B H Pawan Prasad, Green Rosh K S, Lokesh R B, Kaushik Mitra, <strong>Sanjoy Chowdhury</strong>

              <br>
              <em>International Conference on Computer Vision (<font color="#ff0000">ICCV</font>)</em>, 2021
              <br>
              
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Prasad_V-DESIRR_Very_Fast_Deep_Embedded_Single_Image_Reflection_Removal_ICCV_2021_paper.pdf">Paper</a> /
              
              
              
              <a href="https://github.com/ee19d005/vdesirr">Code</a> 
              
              
              
<!--               <a href="https://github.com/ee19d005/vdesirr">slides</a>  -->
              
              <p></p>
              <p>We have proposed a multi-scale end-to-end architecture for detecting and removing weak, medium, and strong reflections from naturally occurring images.</p>

            </td>
          </tr>
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/audVi_coseg_icip.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Listen to the Pixels</h3>
              <br>
              <strong>Sanjoy Chowdhury</strong>, Subhrajyoti Dasgupta, Sudip Das, Ujjwal Bhattacharya

              <br>
              <em>International Conference on Image Processing (<font color="#ff0000">ICIP</font>)</em>, 2021
              <br>
              
              <a href="https://ieeexplore.ieee.org/document/9506019">Paper</a> /
              
              
              
              <a href="https://github.com/schowdhury671/Audio-visual-joint-segmentation">Code</a> /
              
              
              
              <a href="https://www.youtube.com/watch?v=xUwzSQaQ9oQ">Presentation</a> 
              
              <p></p>
              <p>In this study, we exploited the concurrency between audio and visual modalities in an attempt to solve the joint audio-visual segmentation problem in a self-supervised manner.</p>

            </td>
          </tr>
          
          <!--<tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/fuzzy.jpeg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>A Survey on Fuzzy Set Theoretic Approaches for Image Segmentation</h3>
              <br>
              Ajoy Mondal*, <strong>Sanjoy Chowdhury*</strong>

              <br>
              <em><font color="#ff0000">Soft Computing</font></em>, 2022 (Under review)
              <br>
              
               <a href="https://ieeexplore.ieee.org/document/9506019">Paper</a> / 
              
              
              
               <a href="https://github.com/schowdhury671/Audio-visual-joint-segmentation">code</a> / 
              
              
              
               <a href="https://sigport.org/documents/listen-pixels">slides</a> / 
              
              <p></p>
              <p>The survey paper performs an in-depth comparison and analysis on fuzzy set theory-based image segmentation techniques.</p>

            </td>
          </tr>-->
          
          <!--<tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/not_too_deep_cnn.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Not Too Deep CNN for Face Detection in Real-Life Scenario</h3>
              <br>
              <strong>Sanjoy Chowdhury</strong>, Parthasarathi Mukherjee, Ujjwal Bhattacharya

              <br>
              <em>International Conference on Next Generation Computing Technologies, Springer</em>, 2017 (<font color="#ff0000">Best paper award, Oral</font>)
              <br>
              
              <a href="https://link.springer.com/chapter/10.1007/978-981-10-8660-1_66">Paper</a> /
              
              
              
              <a href="https://github.com/schowdhury671/Not-too-deep-CNN">Code</a> 
              
              
              
               <a href="https://github.com/schowdhury671/Not-too-deep-CNN">Slides</a>  
              
              <p></p>
              <p>Proposed a multi-scale face detection framework that is capable of detecting faces of multiple sizes and different orientations in low-resolution images while achieving sufficiently low latency and modest detection rates in the wild.</p>

            </td>
          </tr>-->
          
         <!--<tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/citation.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Classification of Citation in Scientific Articles</h3>
              <br>
              <strong>Sanjoy Chowdhury</strong>, Harsh Vardhan, Pabitra Mitra, Dinabandhu Bhandari

              <br>
              <em>National Conference on Recent Advances in Science and Technology</em>, 2016 (<font color="#ff0000">Oral</font>)
              <br>
              
              <a href="https://github.com/schowdhury671/Citation-classification/blob/main/Citation_Classification_Abstract.docx">Abstract</a> /
              
              
              
              <a href="https://github.com/schowdhury671/Citation-classification">Code</a> 
              
              
              
              <p></p>
              <p>Designed a multi-class classification system to find out the type of citation i.e. a citation belongs to which facet. We aimed to
achieve this by extracting and analyzing citation information from the text.</p>

            </td>
          </tr>-->
          
          </table>
          
        <br><br><br>
	      
	      
<!-- 	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"> -->
	<table style="width:70%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <h2><font color="blue">Blog(s)</font></h2>
              <p>
                Have tried my hand at writing technical blogs.
              </p>
            </td>
          </tr>
        </table>
	            
        <table style="width:70%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/Screenshot 2021-12-01 at 1.12.53 PM.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>The devil is in the details: Video Quality Enhancement Approaches</h3>
              <br>
              
              
              <a href="https://medium.com/sharechat-techbyte/the-devil-is-in-the-details-video-quality-enhancement-approaches-c382e42bf7ed">Link</a> 
              <p></p>
              <p>The blog contextualizes the problem of video enhancement in present-day scenarios and talks about a couple of interesting approaches to handle this challenging task.</p>
              
              <p></p>
              <p></p>

            </td>
          </tr>
          </table>
	 

	 <table style="width:70%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <h2><font color="blue">Academic services</font></h2>
              <p>
                I have served as a reviewer for the following conferences:<br><br>
		<strong>CVPR:</strong> 2023, '24<br><br>
		<strong>ICCV:</strong> 2023<br><br>
		<strong>ECCV:</strong> 2024<br><br>
		<strong>NeurIPS:</strong> 2024<br><br>      
		<strong>WACV:</strong> 2022, '23, '24<br><br>
		<strong>ACMMM:</strong> 2023, '24
		
              </p>
            </td>
          </tr>
        </table>
	      
<!-- 	  <br><br><br><br><br><br> 
	      
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Selected projects</h2>
              <p>
                These include coursework, side projects, and unpublished research work.
              </p>
            </td>
          </tr>
        </table>
	            
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/doc_unwarping.jpeg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Document Image Unwarping</h3>
              <br>
              
              
              <a href="https://github.com/schowdhury671/Document-unwarping-">Code</a> 
              <p></p>
              <p>Worked towards proposing a novel end-to-end Deep Learning based method to unwarp arbitrarily curved and folded paper documents captured in the wild and extract text from it.</p>
              
              <p></p>
              <p></p>

            </td>
          </tr>
          </table>
	      
	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/Screenshot 2021-11-09 at 12.38.43 AM.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Semi-Supervised Multi-View Correlation Feature Learning with Application to Webpage Classification</h3>
              <br>
              
              
              <a href="https://github.com/schowdhury671/Semi-Supervised-MultiView-Feature-Learning">Code</a> 
              
              <p>Implemented a semi-supervised multi-view correlation feature learning (SMCFL) approach, for webpage classification. SMCFL seeks for a discriminant common space by learning a multi-view shared transformation in a semi-supervised manner. This was done as a part of course project and contains implementation of <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14582/13925">paper</a> </p>
              

            </td>
          </tr>
          </table>  
	      
	      
	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/Screenshot 2021-11-30 at 2.29.57 PM.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Bias-Free-Hatespeech-Detection</h3>
              <br>
              
              
              <a href="https://github.com/schowdhury671/Bias-Free-Hatespeech-Detection">Code</a> / 
	      <a href="https://arxiv.org/pdf/1707.00075.pdf"> Original paper </a>	    
              
              <p> Implemented a bias-free hate-speech detection system leveraging adversarial learning. </p>
              

            </td>
          </tr>
          </table>     -->
        
	      <br><br><br>

	<table style="width:50%;border:00px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle">
	
      <div class="affiliations-outer-container">
				<h2 class="section-heading"><font color="blue">Affiliations</font></h2>
	      <br><br>
	      			<table>
					<tr>
						<td style="padding-right: 3.4em;">
							<a href="http://www.iitkgp.ac.in/" target="_blank"><img style="width:80px"  src="../images/iit-kgp.jpeg"></a>
							<br>
							IIT Kharagpur<br> Apr-Sep 2016
						</td>
							
						<td style="padding-right: 3.4em;">
							<a href="https://www.isical.ac.in/" target="_blank"><img style="width:80 px"  src="../images/isi.jpeg"></a>
							<br>
							ISI Kolkata<br> Feb-July 2017
						</td>
						
						<td style="padding-right: 3.4em;">
							<a href="https://www.iiit.ac.in/" target="_blank"><img style="width:80px", "height:140px"  src="../images/iiith.jpeg"></a>
							<br>
							IIIT Hyderabad<br> Aug 2017 - May 2019
						</td>
						
						<td style="padding-right: 3.4em;">
							<a href="https://www.iiit.ac.in/" target="_blank"><img style="width:80px", "height:120px"  src="../images/mentorgraphics.jpeg"></a>
							<br>
							Mentor Graphics Hyderabad<br> May - July 2018
						</td>
						
						<td style="padding-right: 3.4em;">
							<a href="https://research.samsung.com/sri-b" target="_blank"><img style="width:80px"  src="../images/SamsungResearch Logo.jpeg"></a>
							<br>
							Samsung Research Bangalore<br> June 2019 - June 2021
						</td>
						
						<td style="padding-right: 2.4em;">
							<a href="https://sharechat.com/about" target="_blank"><img style="width:80px"  src="../images/ShareChat_logo.png"></a>
							<br>
							ShareChat Bangalore<br> June 2021 - May 2022
						</td>
						
						<td style="padding-right: 2.4em;">
							<a href="https://www.cs.umd.edu/" target="_blank"><img style="width:80px"  src="../images/umd-logo.jpg"></a>
							<br>
							UMD College Park<br> Aug 2022 - Present
						</td>
						<td style="padding-right: 2.4em;">
							<a href="https://research.adobe.com/" target="_blank"><img style="width:80px"  src="../images/adobe-logo-small.png"></a>
							<br>
							Adobe Research<br> May 2023 - Aug 2023
						</td>
						<td style="padding-right: 2.4em;">
							<a href="https://www.kaust.edu.sa/en/" target="_blank"><img style="width:120px"  src="../images/kaust-logo.jpg"></a>
							<br>
							KAUST<br> Jan 2024 - Present
						</td>
						<td style="padding-right: 2.4em;">
							<a href="https://research.google/" target="_blank"><img style="width:120px"  src="../images/google research.png"></a>
							<br>
							Google Research<br> Feb 2024 - May 2024
						</td>
						<td style="padding-right: 2.4em;">
							<a href="https://about.meta.com/realitylabs/" target="_blank"><img style="width:120px"  src="../images/metaAI_pic.png"></a>
							<br>
							Meta AI<br> May 2024 - Aug 2024
						</td>
					</tr>
					<tr>
<!--             <td width="15%" align="center"><font size="3">IIT Kharagpur<br> Apr-Sep 2016</font></td>
            <td width="15%" align="center"><font size="3">ISI Kolkata<br> Feb-July 2017</font></td>
            <td width="15%" align="center"><font size="3">IIIT Hyderabad<br> Aug 2017 - May 2019</font></td>
	    <td width="15%" align="center"><font size="3">Mentor Graphics Hyderabad<br> May - July 2018</font></td>
            <td width="15%" align="center"><font size="3">Samsung Research Bangalore<br> June 2019 - June 2021</font></td>
            <td width="15%" align="center"><font size="3">ShareChat Bangalore<br> June 2021 - May 2022</font></td>
            <td width="15%" align="center"><font size="3">UMD College Park<br> Aug 2022 - Present</font></td>
            <td width="15%" align="center"><font size="3">Adobe Research<br> May 2023 - Aug 2023</font></td>
	    <td width="15%" align="center"><font size="3">KAUST<br> Jan 2024 - Present</font></td>
            <td width="15%" align="center"><font size="3">Google Research<br> Feb 2024 - May 2024</font></td>
            <td width="15%" align="center"><font size="3">Meta AI<br> May 2024 - Aug 2024</font></td> -->
        </tr>
	      			</table>

<!-- 	      </table> -->
			</div>
	      
            </td>
          </tr>
        </table>



<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <heading>Affiliations</heading>
        </td>
      </tr>
      </tbody></table>
    <table align="center">
        <tbody>
        <tr>
            <td width="15%" align="center">
                <a href="http://www.iitkgp.ac.in/" target="_blank">
                <img style="width:120px"  src="../images/iit-kgp.jpeg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://www.isical.ac.in/" target="_blank">
                <img style="width:120px"  src="../images/isi.jpeg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://www.iiit.ac.in/" target="_blank">
                <img style="width:120px" src="../images/iiith.jpeg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://eda.sw.siemens.com/en-US/" target="_blank">
                <img style="width:120px" src="../images/mentorgraphics.jpeg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://research.samsung.com/sri-b" target="_blank">
                <img style="width:120px" src="../images/SamsungResearch Logo.jpeg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://sharechat.com/about" target="_blank">
                <img style="width:120px" src="../images/ShareChat_logo.png"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://www.cs.umd.edu/" target="_blank">
                <img style="width:120px" src="../images/umd-logo.jpg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://research.adobe.com/" target="_blank">
                <img style="width:120px" src="../images/adobe-logo-small.png"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://www.kaust.edu.sa/en/" target="_blank">
                <img style="width:120px" src="../images/kaust-logo.jpg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://research.google/" target="_blank">
                <img style="width:120px" src="../images/google research.png"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://about.meta.com/realitylabs/" target="_blank">
                <img style="width:120px" src="../images/metaAI_pic.png"></a>&nbsp &nbsp
            </td>
        </tr>
        <tr>
            <td width="15%" align="left"><font size="3">IIT Kharagpur<br> Apr-Sep 2016</font></td>
            <td width="15%" align="center"><font size="3">ISI Kolkata<br> Feb-July 2017</font></td>
            <td width="15%" align="center"><font size="3">IIIT Hyderabad<br> Aug 2017 - May 2019</font></td>
	    <td width="15%" align="center"><font size="3">Mentor Graphics Hyderabad<br> May - July 2018</font></td>
            <td width="15%" align="center"><font size="3">Samsung Research Bangalore<br> June 2019 - June 2021</font></td>
            <td width="15%" align="center"><font size="3">ShareChat Bangalore<br> June 2021 - May 2022</font></td>
            <td width="15%" align="center"><font size="3">UMD College Park<br> Aug 2022 - Present</font></td>
            <td width="15%" align="center"><font size="3">Adobe Research<br> May 2023 - Aug 2023</font></td>
	    <td width="15%" align="center"><font size="3">KAUST<br> Jan 2024 - Present</font></td>
            <td width="15%" align="center"><font size="3">Google Research<br> Feb 2024 - May 2024</font></td>
            <td width="15%" align="center"><font size="3">Meta AI<br> May 2024 - Aug 2024</font></td>
        </tr>
        </tbody>
    </table> -->




        
       
        
        
        
        <!-- credits -->
        
        <table style="width:80%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:0;;margin-left:auto;">
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template credits: <a style="font-size:small;margin-right" href="https://jonbarron.info/">Jon Barron</a> and thanks to <a href="https://www.linkedin.com/in/richa-kushwaha/">Richa </a>for making this.
              </p>
            </td>
          </tr>
        </table>

	<table style="width:20%;border:80px;border-spacing:0px;border-collapse:separate;margin-right:0;margin-left:auto;">
          <tr>
<!--             <td style="padding:2.5%;width:80%;vertical-align:middle"> -->
	      
<!-- 	 <table style="width:80%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:0;">   -->
<!-- 	 <tr>	  -->
	 <script style="width:30px;height:20px;align:right" type="text/javascript"  id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=pgVlF5ljpLWpcNcZ70hmBZf0I-uNxgD0lqx-sfyNPZQ&cl=ffffff&w=a"></script>      
	 </tr>
	 </table>	
		
        
  </table>
</body>

</html>

