<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Sanjoy's webpage</title>
  <style>
    .code-box {
      border: 2px solid #4CAF50; /* Green border for the box */
      padding: 15px 20px; /* Add padding inside the box */
      border-radius: 8px; /* Rounded corners */
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); /* Add a soft shadow for depth */
      font-family: 'Courier New', Courier, monospace; /* Monospace font for a code look */
      background-color: #f9f9f9; /* Light background color */
      display: inline-block; /* Make the box fit around the content */
    }
    .center-content {
      text-align: center; /* Center align the content */
    }
    .red-text {
      color: red; /* Red color for the bold text */
    }
  </style>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="Sanjoy" content="Sanjoy's webpage" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="{{ site.baseurl }}/style.css" />
  <link rel="icon" type="image/png" href="../images/IMG-20200202-WA0044_2.jpg">
  <link rel="apple-touch-icon" type="image/png" href="img/apple-touch-icon.png">
  <link rel="canonical" href="{{ page.url | replace:'index.html','' | prepend: site.baseurl | prepend: site.url }}">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">


</head>



<body>
  <table style="width:80%;max-width:640px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:70%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <h1>
                Sanjoy Chowdhury
              </h1>
              <p>I am a 3<sup>rd</sup> year CS PhD student at <a href="https://www.cs.umd.edu/"> University of Maryland, College Park </a> advised by <a href="https://www.cs.umd.edu/people/dmanocha">Prof. Dinesh Manocha</a>. I am broadly interested in multi-modal learning and its different applications. My research primarily involves studying the interplay between the vision and audio modalities and developing systems equipped with their comprehensive understanding.
	      </p>
	       <p>I am currently working as a ML Research intern at Apple MLR hosted by <a href="https://chunliangli.github.io/">Chun-Liang Li</a>  and <a href="https://karreny.github.io/">Karren Yang</a> . I spent the summer of '24 at Meta Reality Labs working as a research scientist intern hosted by <a href = "https://ruohangao.github.io/"> Ruohan Gao </a>. Before this, I was a student researcher at <a href="https://research.google/">Google Research </a> with <a href = "https://scholar.google.co.in/citations?user=4zgNd2UAAAAJ&hl=en">Avisek Lahiri </a> and <a href = "https://research.google/people/vivek-kwatra/">Vivek Kwatra </a> in the Talking heads team on speech driven facial synthesis. Previously, I spent a wonderful summer with <a href = "https://research.adobe.com/">Adobe Research </a> working with <a href = "https://josephkj.in/"> Joseph K J </a> in the Multi-modal AI team as a research PhD intern on multi-modal audio generation. I am also fortunate to have had the chance to work with <a href = "https://www.cs.utexas.edu/users/grauman/"> Prof. Kristen Grauman </a>, <a href = "https://mbzuai.ac.ae/study/faculty/salman-khan/"> Prof. Salman Khan </a>, <a href = "https://www.mohamed-elhoseiny.com/"> Prof. Mohamed Elhoseiny </a> among other wonderful mentors and collaborators. 
		</p>        
		<p>Before joining for PhD, I was working as a Machine Learning Scientist with the Camera and Video AI team at <a href="https://sharechat.com/about">ShareChat</a>, India. I was also a visiting researcher at the Computer Vision and Pattern Recognition Unit at Indian Statistical Institute Kolkata under <a href="https://www.isical.ac.in/~ujjwal/"> Prof. Ujjwal Bhattacharya</a>. Even before, I was a Senior Research Engineer with the Vision Intelligence Group at <a href="https://research.samsung.com/sri-b">Samsung R&D Institute Bangalore</a>. I primarily worked on developing novel AI-powered solutions for different smart devices of Samsung. 
              </p>
              <p>
                I received my MTech in Computer Science & Engineering from <a href="https://www.iiit.ac.in/">IIIT Hyderabad</a> where I was fortunate to be advised by <a href="https://faculty.iiit.ac.in/~jawahar/">Prof. C V Jawahar</a>. During my undergrad, I worked as a research intern under <a href="http://cse.iitkgp.ac.in/~pabitra/">Prof. Pabitra Mitra </a> at IIT Kharagpur and the CVPR Unit at ISI Kolkata. 
	      </p>


	  <div class="center-content">
	    <div class="code-box">
	      <code>
	        <b class="red-text">Feel free to contact me if you're interested in research collaboration!</b>
	      </code>
	    </div>
	  </div>

		    
              <p style="text-align:center">
                <a target="_blank" href="https://mailhide.io/e/PbuZVINh">Email</a> &nbsp;/&nbsp;
                <a href="https://github.com/schowdhury671">GitHub</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=CEdJKCIAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/sanjoy2528/"> LinkedIn</a> &nbsp;/&nbsp;
		<a href="https://twitter.com/schowdhury671"> Twitter </a>      
              </p>
            </td>
            <td style="padding:2.5%;width:80%;max-width:80%">
              <img style="width:100%;max-width:100%;border-radius: 50%" alt="profile photo" src="../images/IMG-20200202-WA0044_2.jpg">
		    <br> <br>
<!-- 		    &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp <a href="https://iribe.umd.edu/"><font color="#800080">Iribe #5116, 8125 Paint Branch Dr</font> <a href> <br>
		    &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  <font color="#800080">College Park, MD 20742 </font> <br> 
		    &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp <font color="#800080">sanjoyc[at]umd[dot]edu </font>     -->
            </td>
          </tr>
        </table>
        
	      
	  <table style="width:70%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              
        

	      
<h2><font color="blue">Updates </font></h2>
      <p>
            <div style="width:80%;overflow-y:scroll; height:250px;">
                <ul id="News">
        <p>
        </p>
	<li> 
Jun 2025 - Co-organising  <a href="https://gen4avc.github.io/"> <font color="#ff0000">Gen4AVC workshop</font> </a>  to be held in conjunction with ICCV 2025. <img src="../images/new.png" alt="project image" width="20" height="20" />
	</li>
	<li> 
Mar 2025 - Joined <font color="#ff0000"> Apple MLR </font> as a ML Research intern. <img src="../images/new.png" alt="project image" width="20" height="20" />
	</li>
	<li>
Feb 2025 - Invited talk at <a href="https://cs.nyu.edu/~fouhey/NYCVision2025/#:~:text=NYC%20Computer%20Vision%20Day%20is%20an%20invite-only%20event,visibility%20for%20graduate%20students%20and%20early%20career%20researchers."><font color="#ff0000">NYC Computer Vision Day 2025 </font></a> organised by New York University. 
        </li>
	<li>
Oct 2024 - Invited talk on assessing and addressing the gaps in existing Audio-Visual LLMs at <a href="https://labsites.rochester.edu/air/index.html">AIR lab</a> at University of Rochester  
        </li>
	<li>
July 2024 - Work on Audio-Visual LLM got accepted to <a href="https://eccv.ecva.net/"> <font color="#ff0000"> ECCV 2024 </font></a> <img src="../images/new.png" alt="project image" width="20" height="20" />
        </li>
		<li>
June 2024 - Invited talk at the <a href="https://sightsound.org/"> <font color="#ff0000"> Sight and Sound workshop </font></a> at CVPR 2024
        </li>

        <li> 
May 2024 - Joined <font color="#ff0000"> Meta Reality Labs </font> as a Research Scientist intern.
	</li>

	<li> 
May 2024 - <a href="https://arxiv.org/pdf/2308.10103"> Paper </a> on Improving Robustness Against Spurious Correlations got accepted to <a href=""> <font color="#ff0000"> ACL 2024 Findings </font></a> 
	</li>
        
        <li>

May 2024 - Our <a href="https://www.nature.com/articles/s41598-024-60299-w.pdf"> paper </a> on determining perceived audience intent from multi-modal social media posts got accepted to <a href="https://www.nature.com/srep/"> <font color="#ff0000"> Nature Scientific Reports</font></a>
        </li>

        <li>
Mar 2024 - <a href="https://arxiv.org/pdf/2403.11487.pdf"> Paper </a> on LLM guided navigational instruction generation got accepted to <a href="https://2024.naacl.org/"> <font color="#ff0000"> NAACL 2024 </font></a> 

</li>


        <li>
Feb 2024 - MeLFusion (<font color="#ff0000"> <b> Highlight, Top 2.8% </b> </font>) got accepted to <a href="https://cvpr.thecvf.com/"> <font color="#ff0000"> CVPR 2024 </font></a>
        </li>

        <li>
Feb 2024 - Joined <font color="#ff0000"> Google Research </font> as a student researcher.

        </li>
        <li>
Oct 2023 - APoLLo gets accepted to <a href="https://2023.emnlp.org/"><font color="#ff0000">EMNLP 2023</font></a>

        </li>
        
        <li>
Oct 2023 - Invited talk on AdVerb at <a href="https://av4d.org/"> <font color="#ff0000">AV4D Workshop, ICCV 2023</font></a>

        </li>
        <li>
July 2023 - AdVerb got accepted to <a href="https://iccv2023.thecvf.com/"><font color="#ff0000">ICCV 2023</font></a>

        </li>
        <li>
May 2023 - Joined <a href="https://research.adobe.com/"><font color="#ff0000">Adobe Research</font></a> as a research intern.</a>
        </li>
        
        <li>
Aug 2022 - Joined as a CS PhD student at <a href="https://www.cs.umd.edu/"><font color="#ff0000">University of Maryland College Park</font> </a>. Awarded <font color="red"> Dean's fellowship. </font>

        </li>

        <li>
Oct 2021 - Paper on audio-visual summarization accepted in <font color="#ff0000">BMVC 2021</font>.
        </li>

        <li>
Sep 2021 - <a href=""><font color="#ff0000">Blog</font> </a> on Video Quality Enhancement released at Tech @ ShareChat.

        </li>


        <li>
July 2021 - Paper on reflection removal got accepted in <font color="#ff0000">ICCV 2021</font>.


        </li>

        <li>
June 2021 - Joined <font color="#ff0000">ShareChat</font> Data Science team.

        </li>

        <li>
May 2021 - Paper on audio-visual joint segmentation accepted in <font color="#ff0000">ICIP 2021</font>.

        </li>

        <li>
Dec 2018 - Accepted <font color="#ff0000">Samsung Research</font> offer. Will be joining in June'19.
        </li>

        <li>

Sep 2018 - Received <font color="#ff0000">Dean's Merit List Award </font> for academic excellence at IIIT Hyderabad.
        </li>


        <li>
Oct 2017 - Our work on a multi-scale, low-latency face detection framework received <font color="#ff0000">Best Paper Award</font> at NGCT-2017.

        </li>


            </ul>
            </div>
      </p>
    </td>
          </tr>
        </table>
	      
	<br><br>     
        <table style="width:70%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <h2><font color="blue">Selected publications </font></h2>
              <p>
<!--                 I am interested in solving computer vision, computer audition, and machine learning problems and applying them to broad AI applications. 
		      My research focuses on applying multi-modal learning (Vision + X) for generative modeling and holistic cross-modal understanding with minimal 
		      supervision. Representative papers are <span style="background-color: #ffffe6">highlighted.</span> -->

		I am interested in solving <b>Computer Vision</b>, <b>Computer Audition</b>, and <b>Machine Learning</b>b problems and applying them to broad AI 
		      applications. My research focuses on applying multi-modal learning (Vision + X) for generative modeling and holistic cross-modal understanding 
		      with minimal supervision. In the past, I have focused on computational photography, tackling 
		challenges such as image reflection removal, intrinsic image decomposition, inverse rendering and video quality assessment.
		Representative papers are highlighted. For full list of publications, please refer to 
		      my <a href="https://scholar.google.com/citations?user=CEdJKCIAAAAJ&hl=en">Google Scholar</a>.

	      </p>
            </td>
          </tr>
        </table>

	<table style="width:70%;border:00px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

	  <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/magnet.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><img src="../images/new.png" alt="project image" width="38" height="38" /> <font color="#0000FF">MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks </font> </h3>
              <br>
              <strong>Sanjoy Chowdhury</strong>, Mohamed Elmoghany, Yohan Abeysinghe, Junjie Fei, Sayan Nag, Salman Khan, Mohamed Elhoseiny, Dinesh Manocha

               <br>
<!--               <em>European Conference on Computer Vision (<font color="#ff0000">ECCV</font>)</em>, 2024 -->
              <br> 
              
               <a href="https://www.arxiv.org/pdf/2506.07016">ArXiv PrePrint</a> /
	       <a href="https://schowdhury671.github.io/magnet_project/">Project Page</a> 
<!-- 	       <a href="https://drive.google.com/file/d/1P0thVIrkMjwRttn1oKBX_X6owiTQ3GR-/view?usp=sharing">Poster</a> /
	       <a href="https://youtu.be/gLFf6hxIj7k">Video</a> /
	       <a href="https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVFIT%20dataset&ga=1">Dataset</a> /
 	       <a href="https://github.com/schowdhury671/meerkat/tree/main">Code</a>  -->
              
<!--               <p></p>
              <p> 
	      </p> -->

            </td>
          </tr>
		
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/aurelia.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><img src="../images/new.png" alt="project image" width="38" height="38" /> <font color="#0000FF">AURELIA: Test-time Reasoning Distillation in Audio-Visual LLMs </font> </h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Hanan Gani*, Nishit Anand, Sayan Nag, Ruohan Gao, Mohamed Elhoseiny, Salman Khan, Dinesh Manocha 

               <br>
<!--               <em>European Conference on Computer Vision (<font color="#ff0000">ECCV</font>)</em>, 2024 -->
              <br> 
              
               <a href="https://arxiv.org/pdf/2503.23219">ArXiv PrePrint</a> /
	       <a href="https://github.com/schowdhury671/aurelia">Project Page and Dataset (Coming soon!)</a> 
<!-- 	       <a href="https://drive.google.com/file/d/1P0thVIrkMjwRttn1oKBX_X6owiTQ3GR-/view?usp=sharing">Poster</a> /
	       <a href="https://youtu.be/gLFf6hxIj7k">Video</a> /
	       <a href="https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVFIT%20dataset&ga=1">Dataset</a> /
 	       <a href="https://github.com/schowdhury671/meerkat/tree/main">Code</a>  -->
              
<!--               <p></p>
              <p> In this paper, we introduce AURELIA, a novel actor-critic based audio-visual reasoning framework that distils structured, step-by-step reasoning into 
		AVLLMs at test time, improving their ability to process
		complex multi-modal inputs without additional training or
		fine-tuning. To further advance AVLLM reasoning skills, we
		present AVReasonBench, a challenging benchmark comprising 4500 audio-visual questions, each paired with detailed
		step-by-step reasoning. Our benchmark spans six distinct
		tasks, including AV-GeoIQ, which evaluates AV reasoning
		combined with geographical and cultural knowledge
	      </p> -->

            </td>
          </tr>

		
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/avtrustbench_teaser.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><img src="../images/new.png" alt="project image" width="38" height="38" /> <font color="#0000FF">AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs </font> </h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sayan Nag*, Subhrajyoti Dasgupta, Yaoting Wang, Mohamed Elhoseiny, Ruohan Gao, Dinesh Manocha 

               <br>
<!--               <em>European Conference on Computer Vision (<font color="#ff0000">ECCV</font>)</em>, 2024 -->
              <br> 
              
               <a href="https://arxiv.org/abs/2501.02135">Paper (ArXiv)</a> /
	       <a href="">Project Page and Dataset (Coming soon!)</a> 
<!-- 	       <a href="https://drive.google.com/file/d/1P0thVIrkMjwRttn1oKBX_X6owiTQ3GR-/view?usp=sharing">Poster</a> /
	       <a href="https://youtu.be/gLFf6hxIj7k">Video</a> /
	       <a href="https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVFIT%20dataset&ga=1">Dataset</a> /
 	       <a href="https://github.com/schowdhury671/meerkat/tree/main">Code</a>  -->
              
<!--               <p></p>
              <p> We introduce Audio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency. Using our benchmark we extensively evaluate 16 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human-like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model-agnostic calibrated audio-visual preference optimization based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks.  
	      </p> -->

            </td>
          </tr>
		
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/meerkat_overview.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3> <img src="../images/meerkat_bg_removed.png" alt="project image" width="20" height="20" /><font color="#0000FF">Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time </font> </h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sayan Nag*, Subhrajyoti Dasgupta*, Jun Chen, Mohamed Elhoseiny, Ruohan Gao, Dinesh Manocha 

              <br>
              <em>European Conference on Computer Vision (<font color="#ff0000">ECCV</font>)</em>, 2024
              <br>
              
               <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08071.pdf">Paper</a>/ 
	       <a href="https://schowdhury671.github.io/meerkat_project/">Project Page</a> /
	       <a href="https://drive.google.com/file/d/1P0thVIrkMjwRttn1oKBX_X6owiTQ3GR-/view?usp=sharing">Poster</a> /
	       <a href="https://youtu.be/gLFf6hxIj7k">Video</a> /
	       <a href="https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVFIT%20dataset&ga=1">Dataset</a> /
 	       <a href="https://github.com/schowdhury671/meerkat/tree/main">Code</a> 
              
<!--               <p></p>
              <p>We present Meerkat, an audio-visual LLM equipped with a
		fine-grained understanding of image and audio both spatially and temporally.
		With a new modality alignment module based on optimal transport and a
		cross-attention module that enforces audio-visual consistency, Meerkat can
		tackle challenging tasks such as audio referred image grounding, image guided
		audio temporal localization, and audio-visual fact-checking. Moreover, we
		carefully curate a large dataset AVFIT that comprises 3M instruction tuning
		samples collected from open-source datasets, and introduce MeerkatBench that
		unifies five challenging audio-visual tasks.  
	      </p> -->

            </td>
          </tr>

	<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/intent-o-meter.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">Towards Determining Perceived Human Intent for Multimodal Social Media Posts using The Theory of Reasoned Action </font></h3>
              <br>
              Trisha Mittal, <strong>Sanjoy Chowdhury</strong>, Pooja Guhan, Snikhita Chelluri, Dinesh Manocha 

              <br>
              <em><font color="#ff0000">Nature Scientific Reports</font></em>
              <br>
              
               <a href="https://www.nature.com/articles/s41598-024-60299-w.pdf">Paper</a> /
	       <a href="https://drive.google.com/drive/folders/1hQ1pdsuqBmKJkQ-bTQXdKuU2TkazCtbh">Dataset</a>    
              
              <p></p>
<!--               <p>We propose Intent-o-meter, a perceived human intent prediction model for multimodal (image and text) social media posts. Intent-o-meter models ideas from psychology and cognitive modeling literature, in addition to using the visual and textual features for an improved perceived intent prediction. </p> -->

            </td>
          </tr>

	<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/aspire.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">ASPIRE: Language-Guided Data Augmentation for Improving Robustness Against Spurious Correlations </font></h3>
              <br>
              Sreyan Ghosh*, Chandra Kiran Reddy Evuru*, Sonal Kumar, Utkarsh Tyagi, Sakshi Singh, <strong>Sanjoy Chowdhury</strong>, Dinesh Manocha

              <br>
              <em><font color="#ff0000">ACL Findings 2024</font></em>
              <br>
              
               <a href="https://aclanthology.org/2024.findings-acl.22.pdf">Paper</a> /
	       <a href="https://github.com/Sreyan88/ASPIRE">Code</a>    
              
<!--               <p></p>
              <p>The paper proposes a simple yet effective solution for supplementing the training dataset with images without spurious features, for robust learning against spurious correlations via better generalization </p> -->

            </td>
          </tr>

		
	<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/llmnav.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">Can LLMâ€™s Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis </font></h3>
              <br>
              Vishnu Sashank Dorbala, <strong>Sanjoy Chowdhury</strong>, Dinesh Manocha 

              <br>
              <em>Annual Conference of the North American Chapter of the Association for Computational Linguistics (<font color="#ff0000">NAACL</font>)</em>, 2024
              <br>
              
               <a href="https://arxiv.org/pdf/2403.11487v1">Paper</a> 
              
              <p></p>
<!--               <p>We present a novel approach to automatically synthesize "wayfinding instructions" for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references. </p> -->

            </td>
          </tr>
			
        <table style="width:70%;border:00px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/melfusion-diagram.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">MeLFusion: Synthesizing Music from Image and Language Cues using Diffusion Models </font> (<font color="#ff0000"> Highlight, Top 2.8% </font>) </h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sayan Nag*, Joseph KJ, Balaji Vasan Srinivasan, Dinesh Manocha 

              <br>
              <em>Conference on Computer Vision and Pattern Recognition (<font color="#ff0000">CVPR</font>)</em>, 2024
              <br>
              
               <a href="https://www.arxiv.org/pdf/2406.04673">Paper</a>/ 
	       <a href="https://schowdhury671.github.io/melfusion_cvpr2024/">Project Page</a> /
	       <a href="https://drive.google.com/file/d/1b_f-DFGWAQvL1ZbPX2gW_BJAweBm5xS-/view">Poster</a> /
	       <a href="https://youtu.be/_CNJeiYwLsE?si=1QnqvgwbC6lq_86e">Video</a> /
	       <a href="https://umd0-my.sharepoint.com/:f:/g/personal/sanjoyc_umd_edu/Eok6RG9QIZhNlGubG8-VsDIBhNMK6OOVAWuHpryEC3VnJw">Dataset</a> /
	       <a href="https://github.com/schowdhury671/melfusion/tree/main">Code</a> 
              
              <p></p>
<!--               <p>We propose MeLFusion, a model that can effectively use cues from a textual description and the corresponding image to synthesize music. MeLFusion is a text-to-music diffusion model with a novel "visual synapse", which effectively infuses the semantics from the visual modality into the generated music. To facilitate research in this area, we introduce a new dataset MeLBench, and propose a new evaluation metric IMSM. </p> -->

            </td>
          </tr>	

	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/apollo.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">APoLLo <img src="../images/rocket1.png" alt="project image" width="20" height="20" />: Unified Adapter and Prompt Learning for Vision Language Models </font></h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sayan Nag*, Dinesh Manocha 

              <br>
              <em>Conference on Empirical Methods in Natural Language Processing (<font color="#ff0000">EMNLP</font>)</em>, 2023
              <br>
              
               <a href="https://aclanthology.org/2023.emnlp-main.629.pdf">Paper</a> / 
	       <a href="https://gamma.umd.edu/pro/vision_language/apollo/">Project Page</a> /
	       <a href="https://drive.google.com/file/d/1KiE_LZAvFEHREibZ4G9vTpMrCtk1UEt8/view?usp=sharing">Poster</a> /
	       <a href="https://youtu.be/5OHwq3VCusA">Video</a> /
<!-- 	       <a href="https://drive.google.com/file/d/1mudcCl0UR9tehffOk-PKxKWDzCcRChIO/view">Poster</a> / -->
	       <a href="https://github.com/schowdhury671/APoLLo">Code</a>
              
              <p></p>
<!--               <p>Our method is designed to substantially improve the generalization capabilities of VLP models when they are fine-tuned in a few-shot setting. We introduce trainable cross-attention-based adapter layers in conjunction with vision and language encoders to strengthen the alignment between the two modalities. </p> -->

            </td>
          </tr>

	   <table style="width:70%;border:00px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/adverb.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">AdVerb: Visually Guided Audio Dereverberation </font></h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sreyan Ghosh*, Subhrajyoti Dasgupta, Anton Ratnarajah, Utkarsh Tyagi, Dinesh Manocha 

              <br>
              <em>International Conference on Computer Vision (<font color="#ff0000">ICCV</font>)</em>, 2023
              <br>
              
               <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chowdhury_AdVerb_Visually_Guided_Audio_Dereverberation_ICCV_2023_paper.pdf">Paper</a> / 
	       <a href="https://schowdhury671.github.io/adverb/">Project Page</a> /
	       <a href="https://www.youtube.com/watch?v=dZuR-pZ9uM0">Video</a> /
	       <a href="https://drive.google.com/file/d/1mudcCl0UR9tehffOk-PKxKWDzCcRChIO/view">Poster</a> /
	       <a href="https://github.com/Sreyan88/AdVerb-dereverb">Code</a>
              
              <p></p>
<!--               <p>We present a novel audio-visual dereverberation framework that uses visual cues in addition to the reverberant sound to estimate clean audio. </p> -->

            </td>
          </tr>
		
	   <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/iccp.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">Measured Albedo in the Wild: Filling the Gap in Intrinsics Evaluation </font></h3>
              <br>
              Jiaye Wu, <strong>Sanjoy Chowdhury</strong>, Hariharmano Shanmugaraja, David Jacobs, Soumyadip Sengupta 

              <br>
              <em>International Conference on Computational Photography (<font color="#ff0000">ICCP</font>)</em>, 2023
              <br>
              
               <a href="https://arxiv.org/pdf/2306.15662.pdf">Paper</a> / 
	       <a href="https://measuredalbedo.github.io/">Project Page</a> /     
               <a href="https://umd.app.box.com/s/rzuzf12ooqnaxyojjgam09zctgp8fr2c">Dataset</a> 
              
              <p></p>
<!--               <p> In order to comprehensively evaluate albedo, we collect a new dataset, Measured Albedo in the Wild (MAW), and propose three new metrics that complement WHDR</p> -->

            </td>
          </tr>
		
		
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/audViSum_bmvc.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">AudViSum: Self-Supervised Deep Reinforcement Learning for Diverse Audio-Visual Summary Generation </font></h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Aditya P. Patra*, Subhrajyoti Dasgupta, Ujjwal Bhattacharya

              <br>
              <em>British Machine Vision Conference (<font color="#ff0000">BMVC</font>)</em>, 2021
              <br>
              
              <a href="https://www.bmvc2021-virtualconference.com/assets/papers/1430.pdf">Paper</a> / 
              
              <a href="https://github.com/schowdhury671/AudViSum">Code</a> / 
		    
              <a href="https://www.youtube.com/watch?v=Hier-zMWcc0">Presentation</a> 
              
              <p></p>
<!--               <p>Introduced a novel deep reinforcement learning-based self-supervised audio-visual summarization model that leverages both audio and visual information to generate diverse yet semantically meaningful summaries.</p> -->

            </td>
          </tr>
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/rr_iccv.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">V-DESIRR: Very Fast Deep Embedded Single Image Reflection Removal </font></h3>
              <br>
              B H Pawan Prasad, Green Rosh K S, Lokesh R B, Kaushik Mitra, <strong>Sanjoy Chowdhury</strong>

              <br>
              <em>International Conference on Computer Vision (<font color="#ff0000">ICCV</font>)</em>, 2021
              <br>
              
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Prasad_V-DESIRR_Very_Fast_Deep_Embedded_Single_Image_Reflection_Removal_ICCV_2021_paper.pdf">Paper</a> /
              
              
              
              <a href="https://github.com/ee19d005/vdesirr">Code</a> 
              
              
              
<!--               <a href="https://github.com/ee19d005/vdesirr">slides</a>  -->
              
              <p></p>
<!--               <p>We have proposed a multi-scale end-to-end architecture for detecting and removing weak, medium, and strong reflections from naturally occurring images.</p> -->

            </td>
          </tr>
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/audVi_coseg_icip.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">Listen to the Pixels </font></h3>
              <br>
              <strong>Sanjoy Chowdhury</strong>, Subhrajyoti Dasgupta, Sudip Das, Ujjwal Bhattacharya

              <br>
              <em>International Conference on Image Processing (<font color="#ff0000">ICIP</font>)</em>, 2021
              <br>
              
              <a href="https://ieeexplore.ieee.org/document/9506019">Paper</a> /
              
              
              
              <a href="https://github.com/schowdhury671/Audio-visual-joint-segmentation">Code</a> /
              
              
              
              <a href="https://www.youtube.com/watch?v=xUwzSQaQ9oQ">Presentation</a> 
              
<!--               <p></p>
              <p>In this study, we exploited the concurrency between audio and visual modalities in an attempt to solve the joint audio-visual segmentation problem in a self-supervised manner.</p> -->

            </td>
          </tr>
          
          <!--<tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/fuzzy.jpeg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>A Survey on Fuzzy Set Theoretic Approaches for Image Segmentation</h3>
              <br>
              Ajoy Mondal*, <strong>Sanjoy Chowdhury*</strong>

              <br>
              <em><font color="#ff0000">Soft Computing</font></em>, 2022 (Under review)
              <br>
              
               <a href="https://ieeexplore.ieee.org/document/9506019">Paper</a> / 
              
              
              
               <a href="https://github.com/schowdhury671/Audio-visual-joint-segmentation">code</a> / 
              
              
              
               <a href="https://sigport.org/documents/listen-pixels">slides</a> / 
              
              <p></p>
              <p>The survey paper performs an in-depth comparison and analysis on fuzzy set theory-based image segmentation techniques.</p>

            </td>
          </tr>-->
          
          <!--<tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/not_too_deep_cnn.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Not Too Deep CNN for Face Detection in Real-Life Scenario</h3>
              <br>
              <strong>Sanjoy Chowdhury</strong>, Parthasarathi Mukherjee, Ujjwal Bhattacharya

              <br>
              <em>International Conference on Next Generation Computing Technologies, Springer</em>, 2017 (<font color="#ff0000">Best paper award, Oral</font>)
              <br>
              
              <a href="https://link.springer.com/chapter/10.1007/978-981-10-8660-1_66">Paper</a> /
              
              
              
              <a href="https://github.com/schowdhury671/Not-too-deep-CNN">Code</a> 
              
              
              
               <a href="https://github.com/schowdhury671/Not-too-deep-CNN">Slides</a>  
              
              <p></p>
              <p>Proposed a multi-scale face detection framework that is capable of detecting faces of multiple sizes and different orientations in low-resolution images while achieving sufficiently low latency and modest detection rates in the wild.</p>

            </td>
          </tr>-->
          
         <!--<tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/citation.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Classification of Citation in Scientific Articles</h3>
              <br>
              <strong>Sanjoy Chowdhury</strong>, Harsh Vardhan, Pabitra Mitra, Dinabandhu Bhandari

              <br>
              <em>National Conference on Recent Advances in Science and Technology</em>, 2016 (<font color="#ff0000">Oral</font>)
              <br>
              
              <a href="https://github.com/schowdhury671/Citation-classification/blob/main/Citation_Classification_Abstract.docx">Abstract</a> /
              
              
              
              <a href="https://github.com/schowdhury671/Citation-classification">Code</a> 
              
              
              
              <p></p>
              <p>Designed a multi-class classification system to find out the type of citation i.e. a citation belongs to which facet. We aimed to
achieve this by extracting and analyzing citation information from the text.</p>

            </td>
          </tr>-->
          
          </table>
          
        <br><br><br>
	      
	      
<!-- 	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"> -->
	<table style="width:70%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <h2><font color="blue">Blog(s)</font></h2>
              <p>
                Have tried my hand at writing technical blogs.
              </p>
            </td>
          </tr>
        </table>
	            
        <table style="width:70%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/Screenshot 2021-12-01 at 1.12.53 PM.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>The devil is in the details: Video Quality Enhancement Approaches</h3>
              <br>
              
              
              <a href="https://medium.com/sharechat-techbyte/the-devil-is-in-the-details-video-quality-enhancement-approaches-c382e42bf7ed">Link</a> 
              <p></p>
              <p>The blog contextualizes the problem of video enhancement in present-day scenarios and talks about a couple of interesting approaches to handle this challenging task.</p>
              
              <p></p>
              <p></p>

            </td>
          </tr>
          </table>
	 

	 <table style="width:70%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <h2><font color="blue">Academic services</font></h2>
              <p>
                I have served as a reviewer for the following conferences:<br><br>
		<strong>CVPR:</strong> 2023, '24, '25<br><br>
		<strong>ICCV:</strong> 2023, '25<br><br>
		<strong>ECCV:</strong> 2024<br><br>
		<strong>NeurIPS:</strong> 2024, '25<br><br>
		<strong>AAAI:</strong> 2025<br><br>      
		<strong>WACV:</strong> 2022, '23, '24<br><br>
		<strong>ACMMM:</strong> 2023, '24 <br><br>
		<strong>ACL:</strong> 2024
		
              </p>
            </td>
          </tr>
        </table>
	      
<!-- 	  <br><br><br><br><br><br> 
	      
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Selected projects</h2>
              <p>
                These include coursework, side projects, and unpublished research work.
              </p>
            </td>
          </tr>
        </table>
	            
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/doc_unwarping.jpeg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Document Image Unwarping</h3>
              <br>
              
              
              <a href="https://github.com/schowdhury671/Document-unwarping-">Code</a> 
              <p></p>
              <p>Worked towards proposing a novel end-to-end Deep Learning based method to unwarp arbitrarily curved and folded paper documents captured in the wild and extract text from it.</p>
              
              <p></p>
              <p></p>

            </td>
          </tr>
          </table>
	      
	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/Screenshot 2021-11-09 at 12.38.43 AM.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Semi-Supervised Multi-View Correlation Feature Learning with Application to Webpage Classification</h3>
              <br>
              
              
              <a href="https://github.com/schowdhury671/Semi-Supervised-MultiView-Feature-Learning">Code</a> 
              
              <p>Implemented a semi-supervised multi-view correlation feature learning (SMCFL) approach, for webpage classification. SMCFL seeks for a discriminant common space by learning a multi-view shared transformation in a semi-supervised manner. This was done as a part of course project and contains implementation of <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14582/13925">paper</a> </p>
              

            </td>
          </tr>
          </table>  
	      
	      
	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/Screenshot 2021-11-30 at 2.29.57 PM.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Bias-Free-Hatespeech-Detection</h3>
              <br>
              
              
              <a href="https://github.com/schowdhury671/Bias-Free-Hatespeech-Detection">Code</a> / 
	      <a href="https://arxiv.org/pdf/1707.00075.pdf"> Original paper </a>	    
              
              <p> Implemented a bias-free hate-speech detection system leveraging adversarial learning. </p>
              

            </td>
          </tr>
          </table>     -->
        
	      <br><br><br>

	<table style="width:50%;border:00px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle">
	
      <div class="affiliations-outer-container">
				<h2 class="section-heading"><font color="blue">Affiliations</font></h2>
	      <br><br>
	      			<table>
					<tr>
						<td style="padding-right: 3.4em;">
							<a href="http://www.iitkgp.ac.in/" target="_blank"><img style="width:70px"  src="../images/iit-kgp.jpeg"></a>
							<br>
							IIT Kharagpur<br> Apr-Sep 2016
						</td>
							
						<td style="padding-right: 3.4em;">
							<a href="https://www.isical.ac.in/" target="_blank"><img style="width:70 px"  src="../images/isi.jpeg"></a>
							<br>
							ISI Kolkata<br> Feb-July 2017
						</td>
						
						<td style="padding-right: 3.4em;">
							<a href="https://www.iiit.ac.in/" target="_blank"><img style="width:70px", "height:140px"  src="../images/iiith.jpeg"></a>
							<br>
							IIIT Hyderabad<br> Aug 2017 - May 2019
						</td>
						
						<td style="padding-right: 3.4em;">
							<a href="https://www.iiit.ac.in/" target="_blank"><img style="width:70px", "height:120px"  src="../images/mentorgraphics.jpeg"></a>
							<br>
							Mentor Graphics Hyderabad<br> May - July 2018
						</td>
						
						<td style="padding-right: 3.4em;">
							<a href="https://research.samsung.com/sri-b" target="_blank"><img style="width:70px"  src="../images/SamsungResearch Logo.jpeg"></a>
							<br>
							Samsung Research Bangalore<br> June 2019 - June 2021
						</td>
						
						<td style="padding-right: 2.4em;">
							<a href="https://sharechat.com/about" target="_blank"><img style="width:70px"  src="../images/ShareChat_logo.png"></a>
							<br>
							ShareChat Bangalore<br> June 2021 - May 2022
						</td>
						
						<td style="padding-right: 2.4em;">
							<a href="https://www.cs.umd.edu/" target="_blank"><img style="width:70px"  src="../images/umd-logo.jpg"></a>
							<br>
							UMD College Park<br> Aug 2022 - Present
						</td>
						<td style="padding-right: 2.4em;">
							<a href="https://research.adobe.com/" target="_blank"><img style="width:70px"  src="../images/adobe-logo-small.png"></a>
							<br>
							Adobe Research<br> May 2023 - Aug 2023
						</td>
						<td style="padding-right: 2.4em;">
							<a href="https://www.kaust.edu.sa/en/" target="_blank"><img style="width:110px"  src="../images/kaust-logo.jpg"></a>
							<br>
							KAUST<br> Jan 2024 - Present
						</td>
						<td style="padding-right: 2.4em;">
							<a href="https://research.google/" target="_blank"><img style="width:110px"  src="../images/google research.png"></a>
							<br>
							Google Research<br> Feb 2024 - May 2024
						</td>
						<td style="padding-right: 2.4em;">
							<a href="https://about.meta.com/realitylabs/" target="_blank"><img style="width:110px"  src="../images/metaAI_pic.png"></a>
							<br>
							Meta AI<br> May 2024 - Nov 2024
						</td>
						<td style="padding-right: 2.4em;">
							<a href="https://www.apple.com/careers/us/work-at-apple/seattle.html" target="_blank"><img style="width:110px"  src="../images/apple_logo.png"></a>
							<br>
							Apple MLR<br> Mar 2025 - Aug 2025
						</td>
					</tr>
					<tr>
<!--             <td width="15%" align="center"><font size="3">IIT Kharagpur<br> Apr-Sep 2016</font></td>
            <td width="15%" align="center"><font size="3">ISI Kolkata<br> Feb-July 2017</font></td>
            <td width="15%" align="center"><font size="3">IIIT Hyderabad<br> Aug 2017 - May 2019</font></td>
	    <td width="15%" align="center"><font size="3">Mentor Graphics Hyderabad<br> May - July 2018</font></td>
            <td width="15%" align="center"><font size="3">Samsung Research Bangalore<br> June 2019 - June 2021</font></td>
            <td width="15%" align="center"><font size="3">ShareChat Bangalore<br> June 2021 - May 2022</font></td>
            <td width="15%" align="center"><font size="3">UMD College Park<br> Aug 2022 - Present</font></td>
            <td width="15%" align="center"><font size="3">Adobe Research<br> May 2023 - Aug 2023</font></td>
	    <td width="15%" align="center"><font size="3">KAUST<br> Jan 2024 - Present</font></td>
            <td width="15%" align="center"><font size="3">Google Research<br> Feb 2024 - May 2024</font></td>
            <td width="15%" align="center"><font size="3">Meta AI<br> May 2024 - Aug 2024</font></td> -->
        </tr>
	      			</table>

<!-- 	      </table> -->
			</div>
	      
            </td>
          </tr>
        </table>



<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <heading>Affiliations</heading>
        </td>
      </tr>
      </tbody></table>
    <table align="center">
        <tbody>
        <tr>
            <td width="15%" align="center">
                <a href="http://www.iitkgp.ac.in/" target="_blank">
                <img style="width:120px"  src="../images/iit-kgp.jpeg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://www.isical.ac.in/" target="_blank">
                <img style="width:120px"  src="../images/isi.jpeg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://www.iiit.ac.in/" target="_blank">
                <img style="width:120px" src="../images/iiith.jpeg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://eda.sw.siemens.com/en-US/" target="_blank">
                <img style="width:120px" src="../images/mentorgraphics.jpeg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://research.samsung.com/sri-b" target="_blank">
                <img style="width:120px" src="../images/SamsungResearch Logo.jpeg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://sharechat.com/about" target="_blank">
                <img style="width:120px" src="../images/ShareChat_logo.png"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://www.cs.umd.edu/" target="_blank">
                <img style="width:120px" src="../images/umd-logo.jpg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://research.adobe.com/" target="_blank">
                <img style="width:120px" src="../images/adobe-logo-small.png"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://www.kaust.edu.sa/en/" target="_blank">
                <img style="width:120px" src="../images/kaust-logo.jpg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://research.google/" target="_blank">
                <img style="width:120px" src="../images/google research.png"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://about.meta.com/realitylabs/" target="_blank">
                <img style="width:120px" src="../images/metaAI_pic.png"></a>&nbsp &nbsp
            </td>
	    <td width="15%" align="center">
                <a href="https://www.apple.com/careers/us/work-at-apple/seattle.html" target="_blank">
                <img style="width:120px" src="../images/apple_logo.png"></a>&nbsp &nbsp
            </td>
        </tr>
        <tr>
            <td width="15%" align="left"><font size="3">IIT Kharagpur<br> Apr-Sep 2016</font></td>
            <td width="15%" align="center"><font size="3">ISI Kolkata<br> Feb-July 2017</font></td>
            <td width="15%" align="center"><font size="3">IIIT Hyderabad<br> Aug 2017 - May 2019</font></td>
	    <td width="15%" align="center"><font size="3">Mentor Graphics Hyderabad<br> May - July 2018</font></td>
            <td width="15%" align="center"><font size="3">Samsung Research Bangalore<br> June 2019 - June 2021</font></td>
            <td width="15%" align="center"><font size="3">ShareChat Bangalore<br> June 2021 - May 2022</font></td>
            <td width="15%" align="center"><font size="3">UMD College Park<br> Aug 2022 - Present</font></td>
            <td width="15%" align="center"><font size="3">Adobe Research<br> May 2023 - Aug 2023</font></td>
	    <td width="15%" align="center"><font size="3">KAUST<br> Jan 2024 - Present</font></td>
            <td width="15%" align="center"><font size="3">Google Research<br> Feb 2024 - May 2024</font></td>
            <td width="15%" align="center"><font size="3">Meta AI<br> May 2024 - Aug 2024</font></td>
	    <td width="15%" align="center"><font size="3">Apple MLR<br> Mar 2025 - Aug 2025</font></td>
        </tr>
        </tbody>
    </table> -->




        
       
        
        
        
        <!-- credits -->
        
        <table style="width:80%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:0;;margin-left:auto;">
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template credits: <a style="font-size:small;margin-right" href="https://jonbarron.info/">Jon Barron</a> and thanks to <a href="https://www.linkedin.com/in/richa-kushwaha/">Richa </a>for making this.
              </p>
            </td>
          </tr>
        </table>

	<table style="width:20%;border:80px;border-spacing:0px;border-collapse:separate;margin-right:0;margin-left:auto;">
          <tr>
<!--             <td style="padding:2.5%;width:80%;vertical-align:middle"> -->
	      
<!-- 	 <table style="width:80%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:0;">   -->
<!-- 	 <tr>	  -->
	 <script style="width:30px;height:20px;align:right" type="text/javascript"  id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=pgVlF5ljpLWpcNcZ70hmBZf0I-uNxgD0lqx-sfyNPZQ&cl=ffffff&w=a"></script>      
	 </tr>
	 </table>	
		
        
  </table>
</body>

</html>
