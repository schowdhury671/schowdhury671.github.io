<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Sanjoy's webpage</title>
  <style>
    .code-box {
      border: 2px solid #4CAF50; /* Green border for the box */
      padding: 15px 20px; /* Add padding inside the box */
      border-radius: 8px; /* Rounded corners */
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); /* Add a soft shadow for depth */
      font-family: 'Courier New', Courier, monospace; /* Monospace font for a code look */
      background-color: #f9f9f9; /* Light background color */
      display: inline-block; /* Make the box fit around the content */
    }
    .center-content {
      text-align: center; /* Center align the content */
    }
    .red-text {
      color: red; /* Red color for the bold text */
    }
  </style>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="Sanjoy" content="Sanjoy's webpage" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="{{ site.baseurl }}/style.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="UTF-8">
  <title>Simple Research Garden</title>
  <link rel="stylesheet" href="main.css">
  <link rel="stylesheet" href="garden.css">
  <style id="flower_css"></style>
  <link rel="canonical" href="{{ page.url | replace:'index.html','' | prepend: site.baseurl | prepend: site.url }}">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">


</head>



<body>
  <table style="width:80%;max-width:640px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:70%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <h1>
                Sanjoy Chowdhury
              </h1>
              <p>I am a third year CS PhD student at <a href="https://www.cs.umd.edu/"> University of Maryland, College Park </a> advised by <a href="https://www.cs.umd.edu/people/dmanocha">Prof. Dinesh Manocha</a>. I am broadly interested in multi-modal learning and its different applications. My research primarily involves studying the interplay between the vision and audio modalities and developing systems equipped with their comprehensive understanding.
	      </p>
	       <p>I am currently working as an ML Research intern at Apple MLR hosted by <a href="https://chunliangli.github.io/">Chun-Liang Li</a>  and <a href="https://karreny.github.io/">Karren Yang</a> . I spent the summer of '24 at Meta Reality Labs working as a research scientist intern hosted by <a href = "https://ruohangao.github.io/"> Ruohan Gao </a>. Before this, I was a student researcher at <a href="https://research.google/">Google Research </a> with <a href = "https://scholar.google.co.in/citations?user=4zgNd2UAAAAJ&hl=en">Avisek Lahiri </a> and <a href = "https://research.google/people/vivek-kwatra/">Vivek Kwatra </a> in the Talking heads team on speech driven facial synthesis. Previously, I spent a wonderful summer with <a href = "https://research.adobe.com/">Adobe Research </a> working with <a href = "https://josephkj.in/"> Joseph K J </a> in the Multi-modal AI team as a research PhD intern on multi-modal audio generation. I am also fortunate to have had the chance to work with <a href = "https://www.cs.utexas.edu/users/grauman/"> Prof. Kristen Grauman </a>, <a href = "https://mbzuai.ac.ae/study/faculty/salman-khan/"> Prof. Salman Khan </a>, <a href = "https://www.mohamed-elhoseiny.com/"> Prof. Mohamed Elhoseiny </a> among other wonderful mentors and collaborators. 
		</p>        
		<p>Before joining for PhD, I was working as a Machine Learning Scientist with the Camera and Video AI team at <a href="https://sharechat.com/about">ShareChat</a>, India. I was also a visiting researcher at the Computer Vision and Pattern Recognition Unit at Indian Statistical Institute Kolkata under <a href="https://www.isical.ac.in/~ujjwal/"> Prof. Ujjwal Bhattacharya</a>. Even before, I was a Senior Research Engineer with the Vision Intelligence Group at <a href="https://research.samsung.com/sri-b">Samsung R&D Institute Bangalore</a>. I primarily worked on developing novel AI-powered solutions for different smart devices of Samsung. 
              </p>
              <p>
                I received my MTech in Computer Science & Engineering from <a href="https://www.iiit.ac.in/">IIIT Hyderabad</a> where I was fortunate to be advised by <a href="https://faculty.iiit.ac.in/~jawahar/">Prof. C V Jawahar</a>. During my undergrad, I worked as a research intern under <a href="http://cse.iitkgp.ac.in/~pabitra/">Prof. Pabitra Mitra </a> at IIT Kharagpur and the CVPR Unit at ISI Kolkata. 
	      </p>


	  <div class="center-content">
	    <div class="code-box">
	      <code>
	        <b class="red-text">Feel free to contact me if you're interested in research collaboration!</b>
	      </code>
	    </div>
	  </div>

		    
              <p style="text-align:center">
                <a target="_blank" href="https://mailhide.io/e/PbuZVINh">Email</a> &nbsp;/&nbsp;
                <a href="https://github.com/schowdhury671">GitHub</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=CEdJKCIAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/sanjoy2528/"> LinkedIn</a> &nbsp;/&nbsp;
		<a href="https://twitter.com/schowdhury671"> Twitter </a>      
              </p>
            </td>
            <td style="padding:2.5%;width:80%;max-width:80%">
              <img style="width:100%;max-width:100%;border-radius: 50%" alt="profile photo" src="../images/IMG-20200202-WA0044_2.jpg">
		    <br> <br>
<!-- 		    &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp <a href="https://iribe.umd.edu/"><font color="#800080">Iribe #5116, 8125 Paint Branch Dr</font> <a href> <br>
		    &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  <font color="#800080">College Park, MD 20742 </font> <br> 
		    &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp <font color="#800080">sanjoyc[at]umd[dot]edu </font>     -->
            </td>
          </tr>
        </table>
        
	      
	  <table style="width:70%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              
        

	<!-- Research Garden -->
	    <svg id="flower_template" xmlns="http://www.w3.org/2000/svg" viewBox="-10 -10 20 20">
	        <ellipse rx="10" ry="20" transform="rotate(0)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
	        <ellipse rx="10" ry="20" transform="rotate(45)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
	        <ellipse rx="10" ry="20" transform="rotate(90)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
	        <ellipse rx="10" ry="20" transform="rotate(135)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
	        <ellipse rx="10" ry="20" transform="rotate(180)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
	        <ellipse rx="10" ry="20" transform="rotate(225)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
	        <ellipse rx="10" ry="20" transform="rotate(270)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
	        <ellipse rx="10" ry="20" transform="rotate(315)" fill="[[COLOR]]" class="petal [[FLOWER_CLASS]]"/>
	        <circle r="6" fill="white"/>
	    </svg>
	    <div id="garden_title">Sanjoy's Research Garden</div>
	    <div id="garden_container">
	        <svg id="garden" preserveAspectRatio="xMinYMin meet"></svg>
	     </div>

	

















		    
	      
<h2><font color="blue">Updates </font></h2>
      <p>
            <div style="width:80%;overflow-y:scroll; height:250px;">
                <ul id="News">
        <p>
        </p>
	<li> 
Mar 2025 - Joined <font color="#ff0000"> Apple MLR </font> as a ML Research intern. <img src="../images/new.png" alt="project image" width="20" height="20" />
	</li>
	<li>
Feb 2025 - Invited talk at <a href="https://cs.nyu.edu/~fouhey/NYCVision2025/#:~:text=NYC%20Computer%20Vision%20Day%20is%20an%20invite-only%20event,visibility%20for%20graduate%20students%20and%20early%20career%20researchers.">NYC Computer Vision Day 2025</a> organised by New York University. 
        </li>
	<li>
Oct 2024 - Invited talk on assessing and addressing the gaps in existing Audio-Visual LLMs at <a href="https://labsites.rochester.edu/air/index.html">AIR lab</a> at University of Rochester  
        </li>
	<li>
July 2024 - Work on Audio-Visual LLM got accepted to <a href="https://eccv.ecva.net/"> <font color="#ff0000"> ECCV 2024 </font></a> <img src="../images/new.png" alt="project image" width="20" height="20" />
        </li>
		<li>
June 2024 - Invited talk at the <a href="https://sightsound.org/"> <font color="#ff0000"> Sight and Sound workshop </font></a> at CVPR 2024
        </li>

        <li> 
May 2024 - Joined <font color="#ff0000"> Meta Reality Labs </font> as a Research Scientist intern.
	</li>

	<li> 
May 2024 - <a href="https://arxiv.org/pdf/2308.10103"> Paper </a> on Improving Robustness Against Spurious Correlations got accepted to <a href=""> <font color="#ff0000"> ACL 2024 Findings </font></a> 
	</li>
        
        <li>

May 2024 - Our <a href="https://www.nature.com/articles/s41598-024-60299-w.pdf"> paper </a> on determining perceived audience intent from multi-modal social media posts got accepted to <a href="https://www.nature.com/srep/"> <font color="#ff0000"> Nature Scientific Reports</font></a>
        </li>

        <li>
Mar 2024 - <a href="https://arxiv.org/pdf/2403.11487.pdf"> Paper </a> on LLM guided navigational instruction generation got accepted to <a href="https://2024.naacl.org/"> <font color="#ff0000"> NAACL 2024 </font></a> 

</li>


        <li>
Feb 2024 - MeLFusion (<font color="#ff0000"> <b> Highlight, Top 2.8% </b> </font>) got accepted to <a href="https://cvpr.thecvf.com/"> <font color="#ff0000"> CVPR 2024 </font></a>
        </li>

        <li>
Feb 2024 - Joined <font color="#ff0000"> Google Research </font> as a student researcher.

        </li>
        <li>
Oct 2023 - APoLLo gets accepted to <a href="https://2023.emnlp.org/"><font color="#ff0000">EMNLP 2023</font></a>

        </li>
        
        <li>
Oct 2023 - Invited talk on AdVerb at <a href="https://av4d.org/"> <font color="#ff0000">AV4D Workshop, ICCV 2023</font></a>

        </li>
        <li>
July 2023 - AdVerb got accepted to <a href="https://iccv2023.thecvf.com/"><font color="#ff0000">ICCV 2023</font></a>

        </li>
        <li>
May 2023 - Joined <a href="https://research.adobe.com/"><font color="#ff0000">Adobe Research</font></a> as a research intern.</a>
        </li>
        
        <li>
Aug 2022 - Joined as a CS PhD student at <a href="https://www.cs.umd.edu/"><font color="#ff0000">University of Maryland College Park</font> </a>. Awarded <font color="red"> Dean's fellowship. </font>

        </li>

        <li>
Oct 2021 - Paper on audio-visual summarization accepted in <font color="#ff0000">BMVC 2021</font>.
        </li>

        <li>
Sep 2021 - <a href=""><font color="#ff0000">Blog</font> </a> on Video Quality Enhancement released at Tech @ ShareChat.

        </li>


        <li>
July 2021 - Paper on reflection removal got accepted in <font color="#ff0000">ICCV 2021</font>.


        </li>

        <li>
June 2021 - Joined <font color="#ff0000">ShareChat</font> Data Science team.

        </li>

        <li>
May 2021 - Paper on audio-visual joint segmentation accepted in <font color="#ff0000">ICIP 2021</font>.

        </li>

        <li>
Dec 2018 - Accepted <font color="#ff0000">Samsung Research</font> offer. Will be joining in June'19.
        </li>

        <li>

Sep 2018 - Received <font color="#ff0000">Dean's Merit List Award </font> for academic excellence at IIIT Hyderabad.
        </li>


        <li>
Oct 2017 - Our work on a multi-scale, low-latency face detection framework received <font color="#ff0000">Best Paper Award</font> at NGCT-2017.

        </li>


            </ul>
            </div>
      </p>
    </td>
          </tr>
        </table>
	      
	<br><br>     
        <table style="width:70%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <h2><font color="blue">Selected publications </font></h2>
              <p>
                I am interested in solving computer vision, computer audition, and machine learning problems and applying them to broad AI applications. My research focuses on applying multi-modal learning (Vision + X) for generative modeling and holistic cross-modal understanding with minimal supervision. Representative papers are <span style="background-color: #ffffe6">highlighted.</span>	      
	      </p>
            </td>
          </tr>
        </table>

	<table style="width:70%;border:00px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/aurelia.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><img src="../images/new.png" alt="project image" width="38" height="38" /> <font color="#0000FF">AURELIA: Test-time Reasoning Distillation in Audio-Visual LLMs </font> </h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Hanan Gani*, Nishit Anand, Sayan Nag, Ruohan Gao, Mohamed Elhoseiny, Salman Khan, Dinesh Manocha 

               <br>
<!--               <em>European Conference on Computer Vision (<font color="#ff0000">ECCV</font>)</em>, 2024 -->
              <br> 
              
               <a href="https://arxiv.org/pdf/2503.23219">ArXiv PrePrint</a> /
	       <a href="https://github.com/schowdhury671/aurelia">Project Page and Dataset (Coming soon!)</a> 
<!-- 	       <a href="https://drive.google.com/file/d/1P0thVIrkMjwRttn1oKBX_X6owiTQ3GR-/view?usp=sharing">Poster</a> /
	       <a href="https://youtu.be/gLFf6hxIj7k">Video</a> /
	       <a href="https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVFIT%20dataset&ga=1">Dataset</a> /
 	       <a href="https://github.com/schowdhury671/meerkat/tree/main">Code</a>  -->
              
              <p></p>
              <p> In this paper, we introduce AURELIA, a novel actor-critic based audio-visual reasoning framework that distils structured, step-by-step reasoning into 
		AVLLMs at test time, improving their ability to process
		complex multi-modal inputs without additional training or
		fine-tuning. To further advance AVLLM reasoning skills, we
		present AVReasonBench, a challenging benchmark comprising 4500 audio-visual questions, each paired with detailed
		step-by-step reasoning. Our benchmark spans six distinct
		tasks, including AV-GeoIQ, which evaluates AV reasoning
		combined with geographical and cultural knowledge
	      </p>

            </td>
          </tr>
		
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/avtrustbench_teaser.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><img src="../images/new.png" alt="project image" width="38" height="38" /> <font color="#0000FF">AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs </font> </h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sayan Nag*, Subhrajyoti Dasgupta, Yaoting Wang, Mohamed Elhoseiny, Ruohan Gao, Dinesh Manocha 

               <br>
<!--               <em>European Conference on Computer Vision (<font color="#ff0000">ECCV</font>)</em>, 2024 -->
              <br> 
              
               <a href="https://arxiv.org/abs/2501.02135">ArXiv PrePrint</a> /
	       <a href="https://github.com/schowdhury671/avtrustbench-">Project Page and Dataset (Coming soon!)</a> 
<!-- 	       <a href="https://drive.google.com/file/d/1P0thVIrkMjwRttn1oKBX_X6owiTQ3GR-/view?usp=sharing">Poster</a> /
	       <a href="https://youtu.be/gLFf6hxIj7k">Video</a> /
	       <a href="https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVFIT%20dataset&ga=1">Dataset</a> /
 	       <a href="https://github.com/schowdhury671/meerkat/tree/main">Code</a>  -->
              
              <p></p>
              <p> We introduce Audio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency. Using our benchmark we extensively evaluate 16 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human-like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model-agnostic calibrated audio-visual preference optimization based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks.  
	      </p>

            </td>
          </tr>
		
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/meerkat_overview.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><img src="../images/new.png" alt="project image" width="38" height="38" /> <img src="../images/meerkat_bg_removed.png" alt="project image" width="20" height="20" /><font color="#0000FF">Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time </font> </h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sayan Nag*, Subhrajyoti Dasgupta*, Jun Chen, Mohamed Elhoseiny, Ruohan Gao, Dinesh Manocha 

              <br>
              <em>European Conference on Computer Vision (<font color="#ff0000">ECCV</font>)</em>, 2024
              <br>
              
               <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08071.pdf">Paper</a>/ 
	       <a href="https://schowdhury671.github.io/meerkat_project/">Project Page</a> /
	       <a href="https://drive.google.com/file/d/1P0thVIrkMjwRttn1oKBX_X6owiTQ3GR-/view?usp=sharing">Poster</a> /
	       <a href="https://youtu.be/gLFf6hxIj7k">Video</a> /
	       <a href="https://umd0-my.sharepoint.com/personal/sanjoyc_umd_edu/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fsanjoyc%5Fumd%5Fedu%2FDocuments%2FAVFIT%20dataset&ga=1">Dataset</a> /
 	       <a href="https://github.com/schowdhury671/meerkat/tree/main">Code</a> 
              
              <p></p>
              <p>We present Meerkat, an audio-visual LLM equipped with a
		fine-grained understanding of image and audio both spatially and temporally.
		With a new modality alignment module based on optimal transport and a
		cross-attention module that enforces audio-visual consistency, Meerkat can
		tackle challenging tasks such as audio referred image grounding, image guided
		audio temporal localization, and audio-visual fact-checking. Moreover, we
		carefully curate a large dataset AVFIT that comprises 3M instruction tuning
		samples collected from open-source datasets, and introduce MeerkatBench that
		unifies five challenging audio-visual tasks.  
	      </p>

            </td>
          </tr>

	<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/intent-o-meter.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">Towards Determining Perceived Human Intent for Multimodal Social Media Posts using The Theory of Reasoned Action </font></h3>
              <br>
              Trisha Mittal, <strong>Sanjoy Chowdhury</strong>, Pooja Guhan, Snikhita Chelluri, Dinesh Manocha 

              <br>
              <em><font color="#ff0000">Nature Scientific Reports</font></em>
              <br>
              
               <a href="https://www.nature.com/articles/s41598-024-60299-w.pdf">Paper</a> /
	       <a href="https://drive.google.com/drive/folders/1hQ1pdsuqBmKJkQ-bTQXdKuU2TkazCtbh">Dataset</a>    
              
              <p></p>
              <p>We propose Intent-o-meter, a perceived human intent prediction model for multimodal (image and text) social media posts. Intent-o-meter models ideas from psychology and cognitive modeling literature, in addition to using the visual and textual features for an improved perceived intent prediction. </p>

            </td>
          </tr>

	<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/llmnav.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">Can LLM’s Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis </font></h3>
              <br>
              Vishnu Sashank Dorbala, <strong>Sanjoy Chowdhury</strong>, Dinesh Manocha 

              <br>
              <em>Annual Conference of the North American Chapter of the Association for Computational Linguistics (<font color="#ff0000">NAACL</font>)</em>, 2024
              <br>
              
               <a href="https://arxiv.org/pdf/2403.11487v1">Paper</a> 
              
              <p></p>
              <p>We present a novel approach to automatically synthesize “wayfinding instructions" for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references. </p>

            </td>
          </tr>
			
        <table style="width:70%;border:00px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/melfusion-diagram.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">MeLFusion: Synthesizing Music from Image and Language Cues using Diffusion Models </font> (<font color="#ff0000"> Highlight, Top 2.8% </font>) </h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sayan Nag*, Joseph KJ, Balaji Vasan Srinivasan, Dinesh Manocha 

              <br>
              <em>Conference on Computer Vision and Pattern Recognition (<font color="#ff0000">CVPR</font>)</em>, 2024
              <br>
              
               <a href="https://www.arxiv.org/pdf/2406.04673">Paper</a>/ 
	       <a href="https://schowdhury671.github.io/melfusion_cvpr2024/">Project Page</a> /
	       <a href="https://drive.google.com/file/d/1b_f-DFGWAQvL1ZbPX2gW_BJAweBm5xS-/view">Poster</a> /
	       <a href="https://youtu.be/_CNJeiYwLsE?si=1QnqvgwbC6lq_86e">Video</a> /
	       <a href="https://umd0-my.sharepoint.com/:f:/g/personal/sanjoyc_umd_edu/Eok6RG9QIZhNlGubG8-VsDIBhNMK6OOVAWuHpryEC3VnJw">Dataset</a> /
	       <a href="https://github.com/schowdhury671/melfusion/tree/main">Code</a> 
              
              <p></p>
              <p>We propose MeLFusion, a model that can effectively use cues from a textual description and the corresponding image to synthesize music. MeLFusion is a text-to-music diffusion model with a novel "visual synapse", which effectively infuses the semantics from the visual modality into the generated music. To facilitate research in this area, we introduce a new dataset MeLBench, and propose a new evaluation metric IMSM. </p>

            </td>
          </tr>	

	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/apollo.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">APoLLo <img src="../images/rocket1.png" alt="project image" width="20" height="20" />: Unified Adapter and Prompt Learning for Vision Language Models </font></h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sayan Nag*, Dinesh Manocha 

              <br>
              <em>Conference on Empirical Methods in Natural Language Processing (<font color="#ff0000">EMNLP</font>)</em>, 2023
              <br>
              
               <a href="https://aclanthology.org/2023.emnlp-main.629.pdf">Paper</a> / 
	       <a href="https://gamma.umd.edu/pro/vision_language/apollo/">Project Page</a> /
	       <a href="https://drive.google.com/file/d/1KiE_LZAvFEHREibZ4G9vTpMrCtk1UEt8/view?usp=sharing">Poster</a> /
	       <a href="https://youtu.be/5OHwq3VCusA">Video</a> /
<!-- 	       <a href="https://drive.google.com/file/d/1mudcCl0UR9tehffOk-PKxKWDzCcRChIO/view">Poster</a> / -->
	       <a href="https://github.com/schowdhury671/APoLLo">Code</a>
              
              <p></p>
              <p>Our method is designed to substantially improve the generalization capabilities of VLP models when they are fine-tuned in a few-shot setting. We introduce trainable cross-attention-based adapter layers in conjunction with vision and language encoders to strengthen the alignment between the two modalities. </p>

            </td>
          </tr>

	   <table style="width:70%;border:00px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
	   <tr bgcolor="#ffffe2">
	   <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/adverb.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">AdVerb: Visually Guided Audio Dereverberation </font></h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Sreyan Ghosh*, Subhrajyoti Dasgupta, Anton Ratnarajah, Utkarsh Tyagi, Dinesh Manocha 

              <br>
              <em>International Conference on Computer Vision (<font color="#ff0000">ICCV</font>)</em>, 2023
              <br>
              
               <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chowdhury_AdVerb_Visually_Guided_Audio_Dereverberation_ICCV_2023_paper.pdf">Paper</a> / 
	       <a href="https://schowdhury671.github.io/adverb/">Project Page</a> /
	       <a href="https://www.youtube.com/watch?v=dZuR-pZ9uM0">Video</a> /
	       <a href="https://drive.google.com/file/d/1mudcCl0UR9tehffOk-PKxKWDzCcRChIO/view">Poster</a> /
	       <a href="https://github.com/Sreyan88/AdVerb-dereverb">Code</a>
              
              <p></p>
              <p>We present a novel audio-visual dereverberation framework that uses visual cues in addition to the reverberant sound to estimate clean audio. </p>

            </td>
          </tr>
		
	   <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/iccp.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">Measured Albedo in the Wild: Filling the Gap in Intrinsics Evaluation </font></h3>
              <br>
              Jiaye Wu, <strong>Sanjoy Chowdhury</strong>, Hariharmano Shanmugaraja, David Jacobs, Soumyadip Sengupta 

              <br>
              <em>International Conference on Computational Photography (<font color="#ff0000">ICCP</font>)</em>, 2023
              <br>
              
               <a href="https://arxiv.org/pdf/2306.15662.pdf">Paper</a> / 
	       <a href="https://measuredalbedo.github.io/">Project Page</a> /     
               <a href="https://umd.app.box.com/s/rzuzf12ooqnaxyojjgam09zctgp8fr2c">Dataset</a> 
              
              <p></p>
              <p> In order to comprehensively evaluate albedo, we collect a new dataset, Measured Albedo in the Wild (MAW), and propose three new metrics that complement WHDR</p>

            </td>
          </tr>
		
		
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/audViSum_bmvc.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">AudViSum: Self-Supervised Deep Reinforcement Learning for Diverse Audio-Visual Summary Generation </font></h3>
              <br>
              <strong>Sanjoy Chowdhury*</strong>, Aditya P. Patra*, Subhrajyoti Dasgupta, Ujjwal Bhattacharya

              <br>
              <em>British Machine Vision Conference (<font color="#ff0000">BMVC</font>)</em>, 2021
              <br>
              
              <a href="https://www.bmvc2021-virtualconference.com/assets/papers/1430.pdf">Paper</a> / 
              
              <a href="https://github.com/schowdhury671/AudViSum">Code</a> / 
		    
              <a href="https://www.youtube.com/watch?v=Hier-zMWcc0">Presentation</a> 
              
              <p></p>
              <p>Introduced a novel deep reinforcement learning-based self-supervised audio-visual summarization model that leverages both audio and visual information to generate diverse yet semantically meaningful summaries.</p>

            </td>
          </tr>
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/rr_iccv.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">V-DESIRR: Very Fast Deep Embedded Single Image Reflection Removal </font></h3>
              <br>
              B H Pawan Prasad, Green Rosh K S, Lokesh R B, Kaushik Mitra, <strong>Sanjoy Chowdhury</strong>

              <br>
              <em>International Conference on Computer Vision (<font color="#ff0000">ICCV</font>)</em>, 2021
              <br>
              
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Prasad_V-DESIRR_Very_Fast_Deep_Embedded_Single_Image_Reflection_Removal_ICCV_2021_paper.pdf">Paper</a> /
              
              
              
              <a href="https://github.com/ee19d005/vdesirr">Code</a> 
              
              
              
<!--               <a href="https://github.com/ee19d005/vdesirr">slides</a>  -->
              
              <p></p>
              <p>We have proposed a multi-scale end-to-end architecture for detecting and removing weak, medium, and strong reflections from naturally occurring images.</p>

            </td>
          </tr>
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/audVi_coseg_icip.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3><font color="#0000FF">Listen to the Pixels </font></h3>
              <br>
              <strong>Sanjoy Chowdhury</strong>, Subhrajyoti Dasgupta, Sudip Das, Ujjwal Bhattacharya

              <br>
              <em>International Conference on Image Processing (<font color="#ff0000">ICIP</font>)</em>, 2021
              <br>
              
              <a href="https://ieeexplore.ieee.org/document/9506019">Paper</a> /
              
              
              
              <a href="https://github.com/schowdhury671/Audio-visual-joint-segmentation">Code</a> /
              
              
              
              <a href="https://www.youtube.com/watch?v=xUwzSQaQ9oQ">Presentation</a> 
              
              <p></p>
              <p>In this study, we exploited the concurrency between audio and visual modalities in an attempt to solve the joint audio-visual segmentation problem in a self-supervised manner.</p>

            </td>
          </tr>
          
          <!--<tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/fuzzy.jpeg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>A Survey on Fuzzy Set Theoretic Approaches for Image Segmentation</h3>
              <br>
              Ajoy Mondal*, <strong>Sanjoy Chowdhury*</strong>

              <br>
              <em><font color="#ff0000">Soft Computing</font></em>, 2022 (Under review)
              <br>
              
               <a href="https://ieeexplore.ieee.org/document/9506019">Paper</a> / 
              
              
              
               <a href="https://github.com/schowdhury671/Audio-visual-joint-segmentation">code</a> / 
              
              
              
               <a href="https://sigport.org/documents/listen-pixels">slides</a> / 
              
              <p></p>
              <p>The survey paper performs an in-depth comparison and analysis on fuzzy set theory-based image segmentation techniques.</p>

            </td>
          </tr>-->
          
          <!--<tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/not_too_deep_cnn.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Not Too Deep CNN for Face Detection in Real-Life Scenario</h3>
              <br>
              <strong>Sanjoy Chowdhury</strong>, Parthasarathi Mukherjee, Ujjwal Bhattacharya

              <br>
              <em>International Conference on Next Generation Computing Technologies, Springer</em>, 2017 (<font color="#ff0000">Best paper award, Oral</font>)
              <br>
              
              <a href="https://link.springer.com/chapter/10.1007/978-981-10-8660-1_66">Paper</a> /
              
              
              
              <a href="https://github.com/schowdhury671/Not-too-deep-CNN">Code</a> 
              
              
              
               <a href="https://github.com/schowdhury671/Not-too-deep-CNN">Slides</a>  
              
              <p></p>
              <p>Proposed a multi-scale face detection framework that is capable of detecting faces of multiple sizes and different orientations in low-resolution images while achieving sufficiently low latency and modest detection rates in the wild.</p>

            </td>
          </tr>-->
          
         <!--<tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/citation.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Classification of Citation in Scientific Articles</h3>
              <br>
              <strong>Sanjoy Chowdhury</strong>, Harsh Vardhan, Pabitra Mitra, Dinabandhu Bhandari

              <br>
              <em>National Conference on Recent Advances in Science and Technology</em>, 2016 (<font color="#ff0000">Oral</font>)
              <br>
              
              <a href="https://github.com/schowdhury671/Citation-classification/blob/main/Citation_Classification_Abstract.docx">Abstract</a> /
              
              
              
              <a href="https://github.com/schowdhury671/Citation-classification">Code</a> 
              
              
              
              <p></p>
              <p>Designed a multi-class classification system to find out the type of citation i.e. a citation belongs to which facet. We aimed to
achieve this by extracting and analyzing citation information from the text.</p>

            </td>
          </tr>-->
          
          </table>
          
        <br><br><br>
	      
	      
<!-- 	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"> -->
	<table style="width:70%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <h2><font color="blue">Blog(s)</font></h2>
              <p>
                Have tried my hand at writing technical blogs.
              </p>
            </td>
          </tr>
        </table>
	            
        <table style="width:70%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/Screenshot 2021-12-01 at 1.12.53 PM.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>The devil is in the details: Video Quality Enhancement Approaches</h3>
              <br>
              
              
              <a href="https://medium.com/sharechat-techbyte/the-devil-is-in-the-details-video-quality-enhancement-approaches-c382e42bf7ed">Link</a> 
              <p></p>
              <p>The blog contextualizes the problem of video enhancement in present-day scenarios and talks about a couple of interesting approaches to handle this challenging task.</p>
              
              <p></p>
              <p></p>

            </td>
          </tr>
          </table>
	 

	 <table style="width:70%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <h2><font color="blue">Academic services</font></h2>
              <p>
                I have served as a reviewer for the following venues:<br><br>
<!-- 		<strong>CVPR:</strong> 2023, '24, '25<br><br>
		<strong>ICCV:</strong> 2023, '25<br><br>
		<strong>ECCV:</strong> 2024<br><br>
		<strong>NeurIPS:</strong> 2024<br><br>
		<strong>AAAI:</strong> 2025<br><br>      
		<strong>WACV:</strong> 2022, '23, '24<br><br>
		<strong>ACMMM:</strong> 2023, '24 <br><br>
		<strong>ACL:</strong> 2024 -->
		<strong>CVPR</strong>, <strong>ICCV</strong>, <strong>ECCV</strong>, <strong>NeurIPS</strong>, <strong>AAAI</strong>, <strong>ICLR</strong>, <strong>WACV</strong>, <strong>ACL</strong>, <strong>ACMMM</strong>
		
              </p>
            </td>
          </tr>
        </table>
	      
<!-- 	  <br><br><br><br><br><br> 
	      
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Selected projects</h2>
              <p>
                These include coursework, side projects, and unpublished research work.
              </p>
            </td>
          </tr>
        </table>
	            
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/doc_unwarping.jpeg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Document Image Unwarping</h3>
              <br>
              
              
              <a href="https://github.com/schowdhury671/Document-unwarping-">Code</a> 
              <p></p>
              <p>Worked towards proposing a novel end-to-end Deep Learning based method to unwarp arbitrarily curved and folded paper documents captured in the wild and extract text from it.</p>
              
              <p></p>
              <p></p>

            </td>
          </tr>
          </table>
	      
	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/Screenshot 2021-11-09 at 12.38.43 AM.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Semi-Supervised Multi-View Correlation Feature Learning with Application to Webpage Classification</h3>
              <br>
              
              
              <a href="https://github.com/schowdhury671/Semi-Supervised-MultiView-Feature-Learning">Code</a> 
              
              <p>Implemented a semi-supervised multi-view correlation feature learning (SMCFL) approach, for webpage classification. SMCFL seeks for a discriminant common space by learning a multi-view shared transformation in a semi-supervised manner. This was done as a part of course project and contains implementation of <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14582/13925">paper</a> </p>
              

            </td>
          </tr>
          </table>  
	      
	      
	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="../images/Screenshot 2021-11-30 at 2.29.57 PM.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Bias-Free-Hatespeech-Detection</h3>
              <br>
              
              
              <a href="https://github.com/schowdhury671/Bias-Free-Hatespeech-Detection">Code</a> / 
	      <a href="https://arxiv.org/pdf/1707.00075.pdf"> Original paper </a>	    
              
              <p> Implemented a bias-free hate-speech detection system leveraging adversarial learning. </p>
              

            </td>
          </tr>
          </table>     -->
        
	      <br><br><br>

	<table style="width:50%;border:00px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:50%;vertical-align:middle">
	
      <div class="affiliations-outer-container">
				<h2 class="section-heading"><font color="blue">Affiliations</font></h2>
	      <br><br>
	      			<table>
					<tr>
						<td style="padding-right: 3.4em;">
							<a href="http://www.iitkgp.ac.in/" target="_blank"><img style="width:70px"  src="../images/iit-kgp.jpeg"></a>
							<br>
							IIT Kharagpur<br> Apr-Sep 2016
						</td>
							
						<td style="padding-right: 3.4em;">
							<a href="https://www.isical.ac.in/" target="_blank"><img style="width:70 px"  src="../images/isi.jpeg"></a>
							<br>
							ISI Kolkata<br> Feb-July 2017
						</td>
						
						<td style="padding-right: 3.4em;">
							<a href="https://www.iiit.ac.in/" target="_blank"><img style="width:70px", "height:140px"  src="../images/iiith.jpeg"></a>
							<br>
							IIIT Hyderabad<br> Aug 2017 - May 2019
						</td>
						
						<td style="padding-right: 3.4em;">
							<a href="https://www.iiit.ac.in/" target="_blank"><img style="width:70px", "height:120px"  src="../images/mentorgraphics.jpeg"></a>
							<br>
							Mentor Graphics Hyderabad<br> May - July 2018
						</td>
						
						<td style="padding-right: 3.4em;">
							<a href="https://research.samsung.com/sri-b" target="_blank"><img style="width:70px"  src="../images/SamsungResearch Logo.jpeg"></a>
							<br>
							Samsung Research Bangalore<br> June 2019 - June 2021
						</td>
						
						<td style="padding-right: 2.4em;">
							<a href="https://sharechat.com/about" target="_blank"><img style="width:70px"  src="../images/ShareChat_logo.png"></a>
							<br>
							ShareChat Bangalore<br> June 2021 - May 2022
						</td>
						
						<td style="padding-right: 2.4em;">
							<a href="https://www.cs.umd.edu/" target="_blank"><img style="width:70px"  src="../images/umd-logo.jpg"></a>
							<br>
							UMD College Park<br> Aug 2022 - Present
						</td>
						<td style="padding-right: 2.4em;">
							<a href="https://research.adobe.com/" target="_blank"><img style="width:70px"  src="../images/adobe-logo-small.png"></a>
							<br>
							Adobe Research<br> May 2023 - Aug 2023
						</td>
						<td style="padding-right: 2.4em;">
							<a href="https://www.kaust.edu.sa/en/" target="_blank"><img style="width:110px"  src="../images/kaust-logo.jpg"></a>
							<br>
							KAUST<br> Jan 2024 - Present
						</td>
						<td style="padding-right: 2.4em;">
							<a href="https://research.google/" target="_blank"><img style="width:110px"  src="../images/google research.png"></a>
							<br>
							Google Research<br> Feb 2024 - May 2024
						</td>
						<td style="padding-right: 2.4em;">
							<a href="https://about.meta.com/realitylabs/" target="_blank"><img style="width:110px"  src="../images/metaAI_pic.png"></a>
							<br>
							Meta AI<br> May 2024 - Nov 2024
						</td>
						<td style="padding-right: 2.4em;">
							<a href="https://www.apple.com/careers/us/work-at-apple/seattle.html" target="_blank"><img style="width:110px"  src="../images/apple_logo.png"></a>
							<br>
							Apple MLR<br> Mar 2025 - Aug 2025
						</td>
					</tr>
					<tr>
<!--             <td width="15%" align="center"><font size="3">IIT Kharagpur<br> Apr-Sep 2016</font></td>
            <td width="15%" align="center"><font size="3">ISI Kolkata<br> Feb-July 2017</font></td>
            <td width="15%" align="center"><font size="3">IIIT Hyderabad<br> Aug 2017 - May 2019</font></td>
	    <td width="15%" align="center"><font size="3">Mentor Graphics Hyderabad<br> May - July 2018</font></td>
            <td width="15%" align="center"><font size="3">Samsung Research Bangalore<br> June 2019 - June 2021</font></td>
            <td width="15%" align="center"><font size="3">ShareChat Bangalore<br> June 2021 - May 2022</font></td>
            <td width="15%" align="center"><font size="3">UMD College Park<br> Aug 2022 - Present</font></td>
            <td width="15%" align="center"><font size="3">Adobe Research<br> May 2023 - Aug 2023</font></td>
	    <td width="15%" align="center"><font size="3">KAUST<br> Jan 2024 - Present</font></td>
            <td width="15%" align="center"><font size="3">Google Research<br> Feb 2024 - May 2024</font></td>
            <td width="15%" align="center"><font size="3">Meta AI<br> May 2024 - Aug 2024</font></td> -->
        </tr>
	      			</table>

<!-- 	      </table> -->
			</div>
	      
            </td>
          </tr>
        </table>



<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <heading>Affiliations</heading>
        </td>
      </tr>
      </tbody></table>
    <table align="center">
        <tbody>
        <tr>
            <td width="15%" align="center">
                <a href="http://www.iitkgp.ac.in/" target="_blank">
                <img style="width:120px"  src="../images/iit-kgp.jpeg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://www.isical.ac.in/" target="_blank">
                <img style="width:120px"  src="../images/isi.jpeg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://www.iiit.ac.in/" target="_blank">
                <img style="width:120px" src="../images/iiith.jpeg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://eda.sw.siemens.com/en-US/" target="_blank">
                <img style="width:120px" src="../images/mentorgraphics.jpeg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://research.samsung.com/sri-b" target="_blank">
                <img style="width:120px" src="../images/SamsungResearch Logo.jpeg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://sharechat.com/about" target="_blank">
                <img style="width:120px" src="../images/ShareChat_logo.png"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://www.cs.umd.edu/" target="_blank">
                <img style="width:120px" src="../images/umd-logo.jpg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://research.adobe.com/" target="_blank">
                <img style="width:120px" src="../images/adobe-logo-small.png"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://www.kaust.edu.sa/en/" target="_blank">
                <img style="width:120px" src="../images/kaust-logo.jpg"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://research.google/" target="_blank">
                <img style="width:120px" src="../images/google research.png"></a>&nbsp &nbsp
            </td>
            <td width="15%" align="center">
                <a href="https://about.meta.com/realitylabs/" target="_blank">
                <img style="width:120px" src="../images/metaAI_pic.png"></a>&nbsp &nbsp
            </td>
	    <td width="15%" align="center">
                <a href="https://www.apple.com/careers/us/work-at-apple/seattle.html" target="_blank">
                <img style="width:120px" src="../images/apple_logo.png"></a>&nbsp &nbsp
            </td>
        </tr>
        <tr>
            <td width="15%" align="left"><font size="3">IIT Kharagpur<br> Apr-Sep 2016</font></td>
            <td width="15%" align="center"><font size="3">ISI Kolkata<br> Feb-July 2017</font></td>
            <td width="15%" align="center"><font size="3">IIIT Hyderabad<br> Aug 2017 - May 2019</font></td>
	    <td width="15%" align="center"><font size="3">Mentor Graphics Hyderabad<br> May - July 2018</font></td>
            <td width="15%" align="center"><font size="3">Samsung Research Bangalore<br> June 2019 - June 2021</font></td>
            <td width="15%" align="center"><font size="3">ShareChat Bangalore<br> June 2021 - May 2022</font></td>
            <td width="15%" align="center"><font size="3">UMD College Park<br> Aug 2022 - Present</font></td>
            <td width="15%" align="center"><font size="3">Adobe Research<br> May 2023 - Aug 2023</font></td>
	    <td width="15%" align="center"><font size="3">KAUST<br> Jan 2024 - Present</font></td>
            <td width="15%" align="center"><font size="3">Google Research<br> Feb 2024 - May 2024</font></td>
            <td width="15%" align="center"><font size="3">Meta AI<br> May 2024 - Aug 2024</font></td>
	    <td width="15%" align="center"><font size="3">Apple MLR<br> Mar 2025 - Aug 2025</font></td>
        </tr>
        </tbody>
    </table> -->




        
       
        
        
        
        <!-- credits -->
        
        <table style="width:80%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:0;;margin-left:auto;">
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template credits: <a style="font-size:small;margin-right" href="https://jonbarron.info/">Jon Barron</a> and thanks to <a href="https://www.linkedin.com/in/richa-kushwaha/">Richa </a>for making this.
              </p>
            </td>
          </tr>
        </table>

	<table style="width:20%;border:80px;border-spacing:0px;border-collapse:separate;margin-right:0;margin-left:auto;">
          <tr>
<!--             <td style="padding:2.5%;width:80%;vertical-align:middle"> -->
	      
<!-- 	 <table style="width:80%;border:20px;border-spacing:0px;border-collapse:separate;margin-right:0;">   -->
<!-- 	 <tr>	  -->
	 <script style="width:30px;height:20px;align:right" type="text/javascript"  id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=pgVlF5ljpLWpcNcZ70hmBZf0I-uNxgD0lqx-sfyNPZQ&cl=ffffff&w=a"></script>      
	 </tr>
	 </table>	
		
        
  </table>


    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="garden.js?v=2"></script>
    <script>
        var papers = [
	
        
	{"id": "meerkat", "title": "Meerkat", "venue": "ECCV 2024", "url": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08071.pdf", "root_node": 1, "root_name": "Audio-Visual LLMs\n2024-now", "root_color": "#D1BDFF", "coauthors": ["Sanjoy Chowdhury"], "full_title": "Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time", "summary": "Leveraging Large Language Models' remarkable proficiency in text-based tasks, recent works on Multi-modal LLMs (MLLMs) extend them to other modalities like vision and audio. However, the progress in these directions has been mostly focused on tasks that only require a coarse-grained understanding of the audio-visual semantics. We present Meerkat, an audio-visual LLM equipped with a fine-grained understanding of image and audio both spatially and temporally. With a new modality alignment module based on optimal transport and a cross-attention module that enforces audio-visual consistency, Meerkat can tackle challenging tasks such as audio referred image grounding, image guided audio temporal localization, and audio-visual fact-checking. Moreover, we carefully curate a large dataset AVFIT that comprises 3M instruction tuning samples collected from open-source datasets, and introduce MeerkatBench that unifies five challenging audio-visual tasks. We achieve state-of-the-art performance on all these downstream tasks with a relative improvement of up to 37.12%."},
        {"id": "aurelia", "title": "Aurelia", "venue": "arXiv", "url": "https://arxiv.org/pdf/2503.23219", "parent": "meerkat", "coauthors": ["Sanjoy Chowdhury"], "full_title": "Aurelia: Test-time Reasoning Distillation in Audio-Visual LLMs", "summary": "Recent advancements in reasoning optimization have greatly enhanced the performance of large language models (LLMs). However, existing work fails to address the complexities of audio-visual scenarios, underscoring the need for further research. In this paper, we introduce AURELIA, a novel actor-critic based audio-visual (AV) reasoning framework that distills structured, step-by-step reasoning into AVLLMs at test time, improving their ability to process complex multi-modal inputs without additional training or fine-tuning. To further advance AVLLM reasoning skills, we present AVReasonBench, a challenging benchmark comprising 4500 audio-visual questions, each paired with detailed step-by-step reasoning. Our benchmark spans six distinct tasks, including AV-GeoIQ, which evaluates AV reasoning combined with geographical and cultural knowledge. Evaluating 18 AVLLMs on AVReasonBench reveals significant limitations in their multi-modal reasoning capabilities. Using AURELIA, we achieve up to a 100% relative improvement, demonstrating its effectiveness. This performance gain highlights the potential of reasoning-enhanced data generation for advancing AVLLMs in real-world applications."},
	{"id": "avtrustbench", "title": "AVTrustBench", "venue": "arXiv", "url": "https://arxiv.org/abs/2501.02135", "parent": "meerkat", "coauthors": ["Sanjoy Chowdhury"], "full_title": "AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs", "summary": "With the rapid advancement of Multi-modal Large Language Models (MLLMs), several diagnostic benchmarks have recently been developed to assess these models' multi-modal reasoning proficiency. However, these benchmarks are restricted to assessing primarily the visual aspect and do not examine the holistic audio-visual (AV) understanding. Moreover, currently, there are no benchmarks that investigate the capabilities of AVLLMs to calibrate their responses when presented with perturbed inputs. To this end, we introduce Audio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency. Using our benchmark we extensively evaluate 13 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human-like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model-agnostic calibrated audio-visual preference optimization based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks. We will publicly release our code and benchmark to facilitate future research in this direction."}

        // {"id": "keepitsimple", "title": "Keep It Simple", "venue": "ACL2021", "url": "https://arxiv.org/abs/2107.03444", "root_node": 1, "root_name": "Writing/Editing\n2020-now", "root_color": "#B3F5BC", "coauthors": ["Tobias Schnabel", "Paul Bennett", "Marti A. Hearst"], "full_title": "Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text", "summary": "First baby steps in the field of simplification (a sister task to summarization). We approached it in an entirely unsupervised way (a la Summary Loop), by crafting rewards for fluency, relevance, and simplicity, and optimizing through RL. Neat Finding: an algorithmic improvement to the popular SCST algorithm (k-SCST) which yielded much more stable training, and better models. Also: a focus on paragraph-level (rather than sentence-level) simplification, which was not so common at the time!", "additional_links": {"code": "https://github.com/tingofurro/keep_it_simple", "video": "https://aclanthology.org/2021.acl-long.498.mp4", "model": "https://huggingface.co/philippelaban/keep_it_simple"}},
        // {"id": "swipe", "title": "SWiPE", "venue": "ACL2023", "url": "https://arxiv.org/abs/2305.19204", "parent": "keepitsimple", "coauthors": ["Jesse Vig", "Wojciech Kryscinski", "Caiming Xiong", "Chien-Sheng Wu"], "full_title": "SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages", "summary": "Improved the extraction process of simplification document pairs from Simple Wikipedia, showing that Swiki is a great resource to study the simplification process. Prior work had mostly disregarded SWiki in favor of less publicly available datasets such as Newsela.", "additional_links": {"code": "https://github.com/salesforce/simplification"}},
        // {"id": "art_or_artifice", "title": "Art or Artifice", "venue": "CHI2024", "url": "https://arxiv.org/abs/2309.14556", "parent": "swipe", "coauthors": ["Tuhin Chakrabarty", "Chien-Sheng Wu"], "full_title": "Art or Artifice? Large Language Models and the False Promise of Creativity", "summary": "Work led by Tuhin Chakrabarty as an intern at Salesforce Research. Are LLMs capable of high-quality creative fictional writing, to the level of short-stories published in the New Yorker. We designed a methodology to evaluate story quality (through rigorous manual evaluation by expert writers), and found a very large gap between professionally-written and AI-generated stories. Neat Finding: at the time, LLMs were not capable of even judging which stories were better than others, and tended to favor more bland AI-like writing.", "additional_links": {"code": "https://github.com/salesforce/creativity_eval", "data (hf)": "https://huggingface.co/datasets/Salesforce/ttcw_creativity_eval", "video": "https://dl.acm.org/doi/10.1145/3613904.3642731"}},
        // {"id": "ai_writing_idiosyncracies", "title": "Writing Idiosyncracies", "venue": "CHI2025", "url": "https://arxiv.org/abs/2409.14509", "is_left_child": 1, "parent": "art_or_artifice", "coauthors": ["Tuhin Chakrabarty", "Chien-Sheng Wu"], "full_title": "Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving Human-AI Alignment in the Writing Process through Edits", "summary": "Work led by Tuhin at Salesforce Research. Following up on Art or Artifice. Lowering our expectation from a story (1000+ words) to a single paragraph (100-200 words). We find in a controlled experiment that LLMs still struggle to generate high-quality creative writing even in shorter form. Neat finding: when allowing an LLM to: (1) draft, (2) detect issues, (3) edit them through executable edits, then LLMs are capable of <b>improving</b> their writing!", "additional_links": {"code": "https://github.com/salesforce/creativity_eval/"}},
        // {"id": "inksync", "title": "InkSync", "venue": "UIST2024", "url": "https://arxiv.org/abs/2309.15337", "indirect_connections": ["ai_writing_idiosyncracies"], "parent": "swipe", "coauthors": ["Jesse Vig", "Marti A. Hearst", "Caiming Xiong", "Chien-Sheng Wu"], "full_title": "Beyond the Chat: Executable and Verifiable Text-Editing with LLMs", "summary": "How do we imagine writing with an LLM should look like. We design the concept of 'executable edits' that an LLM-based system can propose, and design a framework to verify factual accuracy of these edits (Warn-Verify-Audit).", "additional_links": {"demo video": "https://www.youtube.com/watch?v=q7lf5CIMyvE", "live demo": "https://inksync.salesforceresearch.ai/", "code": "https://github.com/SalesforceAIResearch/inksync", "recorded presentation": "https://www.youtube.com/watch?v=3RMo1nWIqCY"}},

        // {"id": "writing_quality_reward_models", "title": "Writing Quality Reward Models", "venue": "arXiv", "url": "https://arxiv.org/abs/2504.07532", "is_left_child": 1, "parent": "ai_writing_idiosyncracies", "coauthors": ["Tuhin Chakrabarty", "Chien-Sheng Wu"], "full_title": "AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation", "summary": "Work co-led with Tuhin Chakrabarty continuing our great CHI2025. In this work, we build out methods to evaluate *writing quality* in for fiction and non-fiction writing tasks. We release a model (WQRM) we believe is best-in-class at measuring writing quality, as confirmed by our benchmark results.", "additional_links": {"code": "https://github.com/salesforce/creativity_eval/", "model": "huggingface.co/models?search=wqrm", "data": "https://github.com/salesforce/creativity_eval/tree/main/WritingRewards/WQ-benchmark-data"}},

        // {"id": "summaryloop", "title": "Summary Loop", "venue": "ACL2020", "url": "https://arxiv.org/abs/2105.05361", "root_node": 1, "root_name": "Summarization\n2019-now", "root_color": "#FFE699", "coauthors": ["Andrew Hsi", "John Canny", "Marti A. Hearst"], "full_title": "The Summary Loop: Learning to Write Abstractive Summaries Without Examples", "summary": "My first foray into summarization! We designed an entirely unsupervised method for summarization that relied on RL optimization (before it was cool) to train a 'language model' (GPT2) to jointly optimize information coverage & fluency. Neat Idea: the most complex element to define was the Coverage scoring method, check it out in the paper.", "indirect_connections": ["keepitsimple"], "additional_links": {"video": "https://slideslive.com/38929183/summary-loop-unsupervised-abstractive-summarization", "code": "https://github.com/CannyLab/summary_loop", "models": "https://huggingface.co/philippelaban/summary_loop46"}},
        // {"id": "summac", "title": "SummaC", "venue": "TACL2021", "url": "https://arxiv.org/abs/2111.09525", "parent": "summaryloop", "coauthors": ["Tobias Schnabel", "Paul N. Bennett", "Marti A. Hearst"], "full_title": "SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization", "summary": "First adventures with factuality/faithfulness/consistency in text generation & summarization. At the time, most of the work had shown NLI methods did not work reliably to detect factual errors in generated text, and relied instead on more complex QG/QA pipelines. The work shows that more modern NLI models, when used at the right granularity, are competitive and much faster to run. We released the two models & the benchmark we created (SummaC).", "additional_links": {"video": "https://aclanthology.org/2022.tacl-1.10.mp4", "code/data": "https://github.com/tingofurro/summac"}},


        // {"id": "summedits", "title": "SummEdits", "venue": "EMNLP2023", "url": "https://arxiv.org/abs/2305.14540", "parent": "summac", "coauthors": ["Wojciech Kryściński", "Divyansh Agarwal", "Alexander R. Fabbri", "Caiming Xiong", "Shafiq Joty", "Chien-Sheng Wu"], "full_title": "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond", "summary": "First work leveraging LLMs (GPT-3.5-turbo)! Can we use an LLM as part of a benchmark creation process to lower the cost of data acquisition while maintaining quality. Neat Finding: we create a dataset for summary factual inconsistency detection that costs 20x less per sample with very high IAA. Most LLMs at the time struggled to beat near-random performance on SummEdits.", "additional_links": {"video": "https://aclanthology.org/2023.emnlp-main.600.mp4", "data (hf)": "https://huggingface.co/datasets/Salesforce/summedits", "code": "https://github.com/salesforce/factualNLG"}},
        // {"id": "summary_of_a_haystack", "title": "Summary of a Haystack", "is_left_child": 1, "venue": "EMNLP2024", "url": "https://arxiv.org/abs/2407.01370", "parent": "summedits", "coauthors": ["Alexander R. Fabbri", "Caiming Xiong", "Chien-Sheng Wu"], "full_title": "Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems", "summary": "How good are LLMs or RAG systems at dealing with long-context (~100k tokens) and precisely summarizing and citing back to relevant documents. Alex Fabbri & I very carefully synthesized Haystacks of documents to explore the subject. Neat Finding: in 2024, RAG vs. long-context LLMs is not a winner takes all (yet). RAGs are better when citation/attribution is important, whereas long-context single-step is more competitive when information (without citation) is desired.", "additional_links": {"data (hf)": "https://huggingface.co/datasets/Salesforce/summary-of-a-haystack", "code": "https://github.com/salesforce/summary-of-a-haystack"}},
        // {"id": "answer_engine_eval", "title": "Answer Engine Eval.", "venue": "FAccT 2025", "url": "https://arxiv.org/abs/2410.22349", "parent": "summary_of_a_haystack", "coauthors": ["Pranav Narayanan Venkit", "Yilun Zhou", "Yixin Mao", "Chien-Sheng Wu"], "full_title": "Search Engines in an AI Era: The False Promise of Factual and Verifiable Source-Cited Responses", "summary": "Work led by Pranav Venkit as an intern at Salesforce Research. Explored whether 'answer engines' (generative search engines, which are simply RAG-based summarization systems) capbale of producing accurate and attributed answers to technical user queries. Neat Finding: the qualitative study led to a quantitative framework with ~8 metrics, most of which current answer engines struggle on.", "additional_links": {"code": "https://github.com/SalesforceAIResearch/answer-engine-eval"}},


        // {"id": "aggrefact", "title": "AggreFact", "venue": "ACL2023", "url": "https://arxiv.org/abs/2205.12854", "parent": "summac", "coauthors": ["Liyan Tang", "Tanya Goyal", "Alexander R. Fabbri", "Jiacheng Xu", "Semih Yavuz", "Wojciech Kryściński", "Justin F. Rousseau", "Greg Durrett"], "full_title": "Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors", "summary": "Work led by Liyan Tang on creating a larger-scale benchmark for factuality in summarization (AggreFact) to better understand error types that occur across model generations. Neat Finding: the distribution of factual error types shifts with different generations of models. As generative models get better, the kinds of factual errors they make become more subtle... and more difficult to catch. (oh no!)", "additional_links": {"video": "https://aclanthology.org/2023.acl-long.650.mp4", "data": "https://github.com/Liyan06/AggreFact"}},

        // {"id": "minicheck", "title": "MiniCheck", "venue": "EMNLP2024", "url": "https://arxiv.org/abs/2404.10774", "parent": "aggrefact", "indirect_connections": ["summedits", "answer_engine_eval"], "coauthors": ["Liyan Tang", "Greg Durrett"], "full_title": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents", "summary": "Work led by Liyan Tang. Very clever data synthesis process to create challenging (positive/negative) samples for factual inconsistency detection. When training a model on the synthetic data, we obtain a factual inconsistency checker as good as the top LLMs (Claude / GPT) but 300x cheaper to run!", "additional_links": {"code": "https://github.com/Liyan06/MiniCheck", "leaderboard": "https://llm-aggrefact.github.io/", "model (hf)": "https://huggingface.co/bespokelabs/Bespoke-MiniCheck-7B", "data (hf)": "https://huggingface.co/datasets/lytang/LLM-AggreFact"}}
	]

        build_garden(papers);
    </script>

		
</body>

</html>

