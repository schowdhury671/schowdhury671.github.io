<!DOCTYPE html>
<html lang="en-us">

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="theme" content="hugo-academic">
	<meta name="generator" content="Hugo 0.55.6" />
	<meta name="author" content="Sanjoy Chowdhury">
	<meta property="og:image" content="img/About/Adobe.png">

	<link rel="stylesheet" href="css/highlight.min.css">
	<link rel="stylesheet" href="css/bootstrap.min.css">
	<link rel="stylesheet" href="css/font-awesome.min.css">
	<link rel="stylesheet" href="css/academicons.min.css">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Merriweather|Roboto+Mono">
	<link rel="stylesheet" href="css/hugo-academic.css">
	<link rel="stylesheet" href="css/custom.css">

	<link rel="icon" type="image/png" href="../images/IMG-20200202-WA0044_2.jpg">
	<link rel="apple-touch-icon" type="image/png" href="img/apple-touch-icon.png">

	<link rel="canonical" href="https://schowdhury671.github.io/">
	<title>Sanjoy Chowdhury</title>

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-GZEW5QH8Y5"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-GZEW5QH8Y5');
	</script>
</head>

<body id="top">

	<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
		<div class="container">

			<div class="navbar-header">
				<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
					<span class="sr-only">Toggle navigation</span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
				</button>
				<a class="navbar-brand" href="/">Sanjoy Chowdhury</a>
			</div>

			<div class="collapse navbar-collapse" id="#navbar-collapse-1">
				<ul class="nav navbar-nav navbar-right">
					<li class="nav-item"><a href="#top">Home</a></li>
					<li class="nav-item"><a href="#updates">Updates</a></li>
					<li class="nav-item"><a href="#publications">Publications</a></li>
					<!-- <li class="nav-item"><a href="#patents">Patents</a></li> -->
				</ul>
			</div>
		</div>
	</nav>

	<span id="homepage" style="display: none"></span>

	<!-- Section: Home -->
	<section id="bio" class="home-section">
		<div class="container">

			<div class="row" itemprop="author" itemscope itemtype="http://schema.org/Person">
				<div class="col-xs-12 col-md-4">
					<div id="profile">

						<div class="portrait" itemprop="image" style="background-image: url('../images/IMG-20200202-WA0044_2.jpg');"></div>

						<div class="portrait-title">
							<h2 itemprop="name">Sanjoy Chowdhury</h2>
							<h3 id="stanford_email" onmouseover="shuffle_stanford()">hover over me @ umd.edu</h3>
							<h3 itemprop="worksFor">University of Maryland, College Park</h3>
						</div>

						<ul class="social-icon " aria-hidden="true ">
							<li><a href="mailto:sanjoyc@umd.edu"><i class="fa fa-envelope big-icon"></i></a></li>
							<li><a href="https://scholar.google.com/citations?user=CEdJKCIAAAAJ&hl=en"><i class="ai ai-google-scholar big-icon"></i></a></li>
							<li><a href="https://github.com/schowdhury671"><i class="fa fa-github big-icon"></i></a></li>
							<li><a href="https://www.linkedin.com/in/sanjoy2528/"><i class="fa fa-linkedin big-icon"></i></a></li>
						</ul>

					</div>
				</div>

				<div class="col-xs-12 col-md-8 " itemprop="description ">

					<h1 id="biography" style="margin-top: 12%">Biography</h1>

						<p> I am a 3<sup>rd</sup> year CS PhD student at <a href="https://www.cs.umd.edu/" style="font-size: large;"> University of Maryland, College Park </a> 
						advised by <a href="https://www.cs.umd.edu/people/dmanocha" style="font-size: large;">Prof. Dinesh Manocha</a>. I am broadly interested in multi-modal learning 
						and its different applications. My research primarily involves studying the interplay between the vision and audio modalities and developing systems equipped with 
						their comprehensive understanding.  </p>
					
						<p> I am currently working as an ML Research intern at Apple MLR hosted by <a href="https://chunliangli.github.io/" style="font-size: large;">Chun-Liang Li</a>  
						and <a href="https://karreny.github.io/" style="font-size: large;">Karren Yang</a>. I spent the summer of '24 at Meta Reality Labs working as a research scientist intern hosted by 
						<a href = "https://ruohangao.github.io/" style="font-size: large;"> Ruohan Gao </a>. Before this, I was a student researcher 
						at <a href="https://research.google/" style="font-size: large;">Google Research </a> 
						with <a href = "https://scholar.google.co.in/citations?user=4zgNd2UAAAAJ&hl=en" style="font-size: large;">Avisek Lahiri </a> 
						and <a href = "https://research.google/people/vivek-kwatra/" style="font-size: large;">Vivek Kwatra </a> in the Talking heads team on speech driven facial synthesis. 
						Previously, I spent a wonderful summer with <a href = "https://research.adobe.com/" style="font-size: large;">Adobe Research </a> working 
						with <a href = "https://josephkj.in/" style="font-size: large;"> Joseph K J </a> in the Multi-modal AI team as a research PhD intern on multi-modal audio generation. 
						I am also fortunate to have had the chance to work with <a href = "https://www.cs.utexas.edu/users/grauman/" style="font-size: large;"> Prof. Kristen Grauman </a>, 
						<a href = "https://mbzuai.ac.ae/study/faculty/salman-khan/" style="font-size: large;"> Prof. Salman Khan </a>, 
						<a href = "https://www.mohamed-elhoseiny.com/" style="font-size: large;"> Prof. Mohamed Elhoseiny </a> among other wonderful mentors and collaborators. </p>
						
						<p> Before joining for PhD, I was working as a Machine Learning Scientist with the Camera and Video AI team at <a href="https://sharechat.com/about" style="font-size: large;">ShareChat</a>, India. 
						I was also a visiting researcher at the Computer Vision and Pattern Recognition Unit at Indian Statistical Institute Kolkata under 
						<a href="https://www.isical.ac.in/~ujjwal/" style="font-size: large;"> Prof. Ujjwal Bhattacharya</a>. Even before, I was a Senior Research Engineer with the Vision Intelligence Group 
						at <a href="https://research.samsung.com/sri-b" style="font-size: large;">Samsung R&D Institute Bangalore</a>. I primarily worked on developing novel AI-powered solutions for different 
						smart devices of Samsung.</p> 

						<p> I received my MTech in Computer Science & Engineering from <a href="https://www.iiit.ac.in/" style="font-size: large;">IIIT Hyderabad</a> where I was fortunate to be 
						advised by <a href="https://faculty.iiit.ac.in/~jawahar/" style="font-size: large;">Prof. C V Jawahar</a>. During my undergrad, I worked as a research intern 
						under <a href="http://cse.iitkgp.ac.in/~pabitra/" style="font-size: large;">Prof. Pabitra Mitra </a> at IIT Kharagpur and the CVPR Unit at ISI Kolkata.  </p>
						
					</p>
				</div>

			</div>
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <div><h2>Research</h2></div>
              <p>
		I am interested in solving <b>Computer Vision</b>, <b>Computer Audition</b>, and <b>Machine Learning</b>b problems and applying them to broad AI applications. My research focuses on applying multi-modal 
		learning (Vision + X) for generative modeling and holistic cross-modal understanding with minimal supervision. In the past, I have focused on computational photography, tackling 
		challenges such as image reflection removal, intrinsic image decomposition, inverse rendering and video quality assessment.
		Representative papers are highlighted. For full list of publications, please refer to my <a href="https://scholar.google.com/citations?user=CEdJKCIAAAAJ&hl=en">Google Scholar</a>.

		      
              </p>
            </td>
          </tr>
        </tbody></table>
			<table class='about-edu'>
				<tr>
					<td align="middle" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://www.apple.com/careers/us/work-at-apple/seattle.html"><img src="../images/apple_logo.png" width="80% "></a>
					</td>
					<td align="middle" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://about.meta.com/realitylabs/"><img src="../images/metaAI_pic.png" width="80% "></a>
					</td>
					<td align="middle" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://research.google/"><img src="../images/google research.png" width="80% "></a>
					</td>
					<td align="middle" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://www.kaust.edu.sa/en/"><img src="../images/kaust-logo.jpg" width="80% "></a>
					</td>
					<td align="middle" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://research.adobe.com/"><img src="../images/adobe-logo-small.png" width="80% "></a>
					</td>
					<td align="middle" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://www.cs.umd.edu/"><img src="../images/umd-logo.jpg" width="80% "></a>
					</td>
					<td align="middle" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://sharechat.com/about"><img src="../images/ShareChat_logo.png" width="80% "></a>
					</td>
					<td align="middle" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://research.samsung.com/sri-b"><img src="../images/SamsungResearch Logo.jpeg" width="80% "></a>
					</td>
					<td align="middle" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://www.iiit.ac.in/"><img src="../images/iiith.jpeg" width="80% "></a>
					</td>
					<td align="middle" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://www.isical.ac.in/"><img src="../images/isi.jpeg" width="80% "></a>
					</td>
					<td align="middle" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="http://www.iitkgp.ac.in/"><img src="../images/iit-kgp.jpeg" width="80% "></a>
					</td>
					
					
					
					
					
				</tr>

				<tr>
					<td align="center" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://www.apple.com/careers/us/work-at-apple/seattle.html"><div class="link-simple">Research Scientist Intern<br>Apple MLR<br>Mar - Aug, 2025</div></a>
					</td>
					<td align="center" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://about.meta.com/realitylabs/"><div class="link-simple">Research Scientist Intern<br>Meta Reality Labs<br>May - Nov, 2024</div></a>
					</td>
					<td align="center" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://research.google/"><div class="link-simple">Student Researcher<br>Google Research (now Deepmind)<br>Feb - May, 2024</div></a>
					</td>
					<td align="center" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://www.kaust.edu.sa/en/"><div class="link-simple">Visiting Researcher<br>KAUST<br>Jan 2024 - Present</div></a>
					</td>
					<td align="center" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://research.adobe.com/"><div class="link-simple">Research Scientist Intern<br>Adobe Research<br>May - Aug, 2023</div></a>
					</td>
					<td align="center" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://www.cs.umd.edu/"><div class="link-simple">Ph.D, Computer Science<br>UMD College Park<br>Aug 2022 - Present</div></a>
					</td>
					<td align="center" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://sharechat.com/about"><div class="link-simple">ML Research Engineer<br>ShareChat<br>June 2021 - May 2022</div></a>
					</td>
					<td align="center" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://research.samsung.com/sri-b"><div class="link-simple">Senior Research Engineer<br>Samsung Research, Bangalore<br>June 2019 - June 2021</div></a>
					</td>
					<td align="center" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://www.iiit.ac.in/"><div class="link-simple">M.Tech, Computer Science<br>IIIT Hyderabad<br>Aug 2017 - May 2019</div></a>
					</td>
					<td align="center" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="https://www.isical.ac.in/"><div class="link-simple">Research Intern<br>ISI Kolkata<br>Feb-July 2017</div></a>
					</td>
					<td align="center" style="vertical-align: middle; background-color: rgba(255, 255, 255, 1)">
						<a href="http://www.iitkgp.ac.in/"><div class="link-simple">Research Intern<br>IIT Kharagpur<br>Apr-Sep 2016</div></a>
					</td>
					
					
					

				</tr>
			</table>
		</div>
	</section>

	<!-- Section: Updates -->
	<section id="updates" class="home-section">
		<div class="container">

			<div class="row">
		            <div class="col-md-2"></div>
            <div class="col-md-8">
                <div class="section-heading text-center">
                    <h1>Updates</h1></div>
			</div>

			<div class="col-xs-0 col-md-1"></div>
			<div class="col-xs-12 col-md-11 update-text" style="margin: 0em; height: 250px; overflow-y: auto;">
				
			<li> Mar 2025 - Joined <font color="#ff0000"> Apple MLR </font> as a ML Research intern. <img src="../images/new.png" alt="project image" width="20" height="20" /></li>
			<li> Feb 2025 - Invited talk at <a href="https://cs.nyu.edu/~fouhey/NYCVision2025/#:~:text=NYC%20Computer%20Vision%20Day%20is%20an%20invite-only%20event,visibility%20for%20graduate%20students%20and%20early%20career%20researchers.">
				NYC Computer Vision Day 2025</a> organised by New York University. </li>
			<li>Oct 2024 - Invited talk on assessing and addressing the gaps in existing Audio-Visual LLMs at <a href="https://labsites.rochester.edu/air/index.html">AIR lab</a> at 
				University of Rochester  </li>
			<li>July 2024 - Work on Audio-Visual LLM got accepted to <a href="https://eccv.ecva.net/"> <font color="#ff0000"> ECCV 2024 
				</font></a> <img src="../images/new.png" alt="project image" width="20" height="20" /></li>
			<li>June 2024 - Invited talk at the <a href="https://sightsound.org/"> <font color="#ff0000"> Sight and Sound workshop </font></a> at CVPR 2024</li>
        		<li>May 2024 - Joined <font color="#ff0000"> Meta Reality Labs </font> as a Research Scientist intern.</li>
			<li>May 2024 - <a href="https://arxiv.org/pdf/2308.10103"> Paper </a> on Improving Robustness Against Spurious Correlations got accepted to 
				<a href=""> <font color="#ff0000"> ACL 2024 Findings </font></a> </li>
        		<li>May 2024 - Our <a href="https://www.nature.com/articles/s41598-024-60299-w.pdf"> paper </a> on determining perceived audience intent from multi-modal social media posts 
				got accepted to <a href="https://www.nature.com/srep/"> <font color="#ff0000"> Nature Scientific Reports</font></a></li>
        		<li>Mar 2024 - <a href="https://arxiv.org/pdf/2403.11487.pdf"> Paper </a> on LLM guided navigational instruction generation got accepted to 
				<a href="https://2024.naacl.org/"> <font color="#ff0000"> NAACL 2024 </font></a> </li>
        		<li>Feb 2024 - MeLFusion (<font color="#ff0000"> <b> Highlight, Top 2.8% </b> </font>) got accepted to <a href="https://cvpr.thecvf.com/"> 
				<font color="#ff0000"> CVPR 2024 </font></a> </li>
        		<li>Feb 2024 - Joined <font color="#ff0000"> Google Research </font> as a student researcher.</li>
        		<li>Oct 2023 - APoLLo gets accepted to <a href="https://2023.emnlp.org/"><font color="#ff0000">EMNLP 2023</font></a></li>
        		<li>Oct 2023 - Invited talk on AdVerb at <a href="https://av4d.org/"> <font color="#ff0000">AV4D Workshop, ICCV 2023</font></a></li>
        		<li>July 2023 - AdVerb got accepted to <a href="https://iccv2023.thecvf.com/"><font color="#ff0000">ICCV 2023</font></a></li>
        		<li>May 2023 - Joined <a href="https://research.adobe.com/"><font color="#ff0000">Adobe Research</font></a> as a research intern.</a></li>
        		<li>Aug 2022 - Joined as a CS PhD student at <a href="https://www.cs.umd.edu/"><font color="#ff0000">University of Maryland College Park</font> </a>. Awarded 
				<font color="red"> Dean's fellowship. </font> </li>
        		<li>Oct 2021 - Paper on audio-visual summarization accepted in <font color="#ff0000">BMVC 2021</font>.</li>
        		<li>Sep 2021 - <a href=""><font color="#ff0000">Blog</font> </a> on Video Quality Enhancement released at Tech @ ShareChat.</li>
        		<li>July 2021 - Paper on reflection removal got accepted in <font color="#ff0000">ICCV 2021</font>.</li>
        		<li>June 2021 - Joined <font color="#ff0000">ShareChat</font> Data Science team.</li>
        		<li>May 2021 - Paper on audio-visual joint segmentation accepted in <font color="#ff0000">ICIP 2021</font>.</li>
        		<li>Dec 2018 - Accepted <font color="#ff0000">Samsung Research</font> offer. Will be joining in June'19.</li>
        		<li>Sep 2018 - Received <font color="#ff0000">Dean's Merit List Award </font> for academic excellence at IIIT Hyderabad.</li>
			<li>Oct 2017 - Our work on a multi-scale, low-latency face detection framework received <font color="#ff0000">Best Paper Award</font> at NGCT-2017.</li>
					
		</div>
	</section>





	<!-- Publication -->
	<section id="publications" class="home-section ">
		<ul class="fa-ul"><div class="container ">

			<div class="row " style="margin: 0em 0 ">
				<div class="col-md-4 "></div>
				<div class="col-md-8 " style="margin-bottom: 0em; "><h1>Selected Publications</h1></div>
			</div>

			<h2 class="year">2025</h2>


			<div class="row" style="margin: 2.5em 0 ">
				<div class="col-md-4">
					<img class="pub-img " style="margin-right: auto; margin-left: auto;" src="../images/aurelia.png"></img>
				</div>
				<div class="col-md-8">
					<li itemscope itemtype="http://schema.org/CreativeWork ">
						<span itemprop="name">AURELIA: Test-time Reasoning Distillation in Audio-Visual LLMs</span>
						<div class="pub-list-item " itemscope itemtype="http://schema.org/CreativeWork ">
							<div class="pub-authors " itemprop="author ">
					
								<b style="color: #0095eb">Sanjoy Chowdhury*</b>,
								<text style="color: black">Hanan Gani*, Nishit Anand, Sayan Nag, Ruohan Gao, Mohamed Elhoseiny, Salman Khan, Dinesh Manocha </text>
								
							</div>
							<div class="pub-publication ">Arxiv Preprint</div>
							<p>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('aurelia_abstract'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">Abstract</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://arxiv.org/abs/2503.23219">Paper</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://github.com/schowdhury671/aurelia">Code</a>
<!-- 								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('aurelia_bib_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">BibTeX</a> -->
								<pre id="aurelia_abstract", style="display:none">
								In this paper, we introduce AURELIA, a novel actor-critic based audio-visual reasoning framework that distils structured, step-by-step reasoning into 
								AVLLMs at test time, improving their ability to process
								complex multi-modal inputs without additional training or
								fine-tuning. To further advance AVLLM reasoning skills, we
								present AVReasonBench, a challenging benchmark comprising 4500 audio-visual questions, each paired with detailed
								step-by-step reasoning. Our benchmark spans six distinct
								tasks, including AV-GeoIQ, which evaluates AV reasoning
								combined with geographical and cultural knowledge
									
<!-- 								<pre id="aurelia_bib_publication", style="display:none">
								@misc{chowdhury2025aureliatesttimereasoningdistillation,
								      title={Aurelia: Test-time Reasoning Distillation in Audio-Visual LLMs}, 
								      author={Sanjoy Chowdhury and Hanan Gani and Nishit Anand and Sayan Nag and Ruohan Gao and Mohamed Elhoseiny and Salman Khan and Dinesh Manocha},
								      year={2025},
								      eprint={2503.23219},
								      archivePrefix={arXiv},
								      primaryClass={eess.AS},
								      url={https://arxiv.org/abs/2503.23219}, 
								} -->
								
								</pre>
							</p>
						</div>
					</li>
				</div>
			</div>

			
			
			<div class="row" style="margin: 2.5em 0 ">
				<div class="col-md-4">
					<img class="pub-img " style="margin-right: auto; margin-left: auto;" src="../images/avtrustbench_teaser.jpg"></img>
				</div>
				<div class="col-md-8">
					<li itemscope itemtype="http://schema.org/CreativeWork ">
						<span itemprop="name">AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs</span>
						<div class="pub-list-item " itemscope itemtype="http://schema.org/CreativeWork ">
							<div class="pub-authors " itemprop="author ">
								<b style="color: #0095eb">Sanjoy Chowdhury*</b>,
							
								<text style="color: black">Sayan Nag*, Subhrajyoti Dasgupta, Yaoting Wang, Mohamed Elhoseiny, Ruohan Gao, Dinesh Manocha 
</text>
								
							</div>
							<div class="pub-publication ">Arxiv Preprint</div>
							<p>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('avtrustbench_abstract_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">Abstract</a>
								<a class="btn btn-primary btn-outline btn-xs " href="">PDF</a>
								<a class="btn btn-primary btn-outline btn-xs " href="">Code</a>

<pre id="avtrustbench_abstract_publication", style="display:none">
	We introduce Audio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency. Using our benchmark we extensively evaluate 16 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human-like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model-agnostic calibrated audio-visual preference optimization based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks.  

							</p>
						</div>
					</li>
				</div>
			</div>		
			
			<!-- CVPR 2025 UAPOSE -->
			<div class="row" style="margin: 2.5em 0 ">
				<div class="col-md-4">
					<img class="pub-img " style="margin-right: auto; margin-left: auto;" src="img/Publications/UA-pose.png"></img>
				</div>
				<div class="col-md-8">
					<li itemscope itemtype="http://schema.org/CreativeWork ">
						<span itemprop="name">UA-Pose: Uncertainty-aware 6D Object Pose Estimation and Online Object Completion with Partial References</span>
						<div class="pub-list-item " itemscope itemtype="http://schema.org/CreativeWork ">
							<div class="pub-authors " itemprop="author ">
								<!-- <text style="color: black">Nihal Jain</text>, -->
								<!-- <text style="color: black">Praneetha Vaddamanu</text>, -->
								 <!-- Ming-Feng Li, Xin Yang, Fu-En Wang, Hritam Basak, Yuyin Sun, Shreekant Gayaka, Min Sun, Cheng-Hao Kuo -->
								<text style="color: black">Ming-Feng Li, Xin Yang, Fu-En Wang,</text>
								<!-- <text style="color: black">Zhaozheng Yin</text> -->
								<!-- <text style="color: black">Zhaozheng Yin</text> -->
								 <b style="color: #0095eb">Hritam Basak</b>,
								 <!--  -->
								<text style="color: black">Yuyin Sun, Shreekant Gayaka, Min Sun, Cheng-Hao Kuo</text>
								<!-- <text style="color: black">Kuldeep Kulkarni</text> -->
							</div>
							<div class="pub-publication ">International Conference on Computer Vision and Pattern Recognition (CVPR 2025)</div>
							<p>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('ua-pose_abstract_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">Abstract</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://openaccess.thecvf.com/content/CVPR2025/html/Li_UA-Pose_Uncertainty-Aware_6D_Object_Pose_Estimation_and_Online_Object_Completion_CVPR_2025_paper.html">PDF</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://minfenli.github.io/UA-Pose/">Code</a>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('ua-pose_bib_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">BibTeX</a>
<pre id="ua-pose_abstract_publication", style="display:none">
<!-- In creative workflows, designers compile a collection (or “moodboard”) of inspirational assets for ideation. They may use this as reference for finding additional assets. In this work, we aim to stimulate creative ideation by making suggestions that take cues from the designer’s moodboard but also carefully diverge from it. A collection of images may have rich information along different axes (e.g. color, composition, or style) – these axes or channels can be used to model relevance or divergence of any image from a query collection. So, we develop a self-supervised model that can extract channel-specific representations from collections of images. We propose a search algorithm that uses these representations to obtain results that satisfy the collection’s intent along some channels but diverge from the query along others. We show that this allows for effective exploration of the creative space of possibilities. Finally, we demonstrate a mix-and-match visual querying mechanism that allows us to combine channels from different collections of inspirational content, thus facilitating ease in creative expression. -->
6D object pose estimation has shown strong generalizability to novel objects. However, existing methods often require either a complete, well-reconstructed 3D model or numerous reference images that fully cover the object. Estimating 6D poses from partial references, which capture only fragments of an object's appearance and geometry, remains challenging. To address this, we propose UA-Pose, an uncertainty-aware approach for 6D object pose estimation and online object completion specifically designed for partial references. We assume access to either (1) a limited set of RGBD images with known poses or (2) a single 2D image. For the first case, we initialize a partial object 3D model based on the provided images and poses, while for the second, we use image-to-3D techniques to generate an initial object 3D model. Our method integrates uncertainty into the incomplete 3D model, distinguishing between seen and unseen regions. This uncertainty enables confidence assessment in pose estimation and guides an uncertainty-aware sampling strategy for online object completion, enhancing robustness in pose estimation accuracy and improving object completeness. We evaluate our method on the YCB-Video, YCBInEOAT, and HO3D datasets, including RGBD sequences of YCB objects manipulated by robots and human hands. Experimental results demonstrate significant performance improvements over existing methods, particularly when object observations are incomplete or partially captured.
<pre id="ua-pose_bib_publication", style="display:none">
	@inproceedings{li2025ua,
  title={UA-Pose: Uncertainty-aware 6D object pose estimation and online object completion with partial references},
  author={Li, Ming-Feng and Yang, Xin and Wang, Fu-En and Basak, Hritam and Sun, Yuyin and Gayaka, Shreekant and Sun, Min and Kuo, Cheng-Hao},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={1180--1189},
  year={2025}
}
</pre>
							</p>
						</div>
					</li>
				</div>
			</div>


			<!-- CVPR 2025 -->
			<div class="row" style="margin: 2.5em 0 ">
				<div class="col-md-4">
					<img class="pub-img " style="margin-right: auto; margin-left: auto; height: auto;" src="img/Publications/cvpr-25.png"></img>
				</div>
				<div class="col-md-8">
					<li itemscope itemtype="http://schema.org/CreativeWork ">
						<span itemprop="name">SemiDAViL: Semi-supervised Domain Adaptation with Vision-Language Guidance for Semantic Segmentation</span>
						<div class="pub-list-item " itemscope itemtype="http://schema.org/CreativeWork ">
							<div class="pub-authors " itemprop="author ">
								<!-- <text style="color: black">Nihal Jain</text>, -->
								<!-- <text style="color: black">Praneetha Vaddamanu</text>, -->
								<b style="color: #0095eb">Hritam Basak</b>,
								<text style="color: black">Zhaozheng Yin</text>
								<!-- <text style="color: black">Kuldeep Kulkarni</text> -->
							</div>
							<div class="pub-publication ">International Conference on Computer Vision and Pattern Recognition (CVPR 2025)</div>
							<p>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('semidavil_abstract_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">Abstract</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://openaccess.thecvf.com/content/CVPR2025/html/Basak_SemiDAViL_Semi-supervised_Domain_Adaptation_with_Vision-Language_Guidance_for_Semantic_Segmentation_CVPR_2025_paper.html">PDF</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://github.com/hritam-98/SemiDAViL">Code</a>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('semidavil_bib_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">BibTeX</a>
<pre id="semidavil_abstract_publication", style="display:none">
<!-- In creative workflows, designers compile a collection (or “moodboard”) of inspirational assets for ideation. They may use this as reference for finding additional assets. In this work, we aim to stimulate creative ideation by making suggestions that take cues from the designer’s moodboard but also carefully diverge from it. A collection of images may have rich information along different axes (e.g. color, composition, or style) – these axes or channels can be used to model relevance or divergence of any image from a query collection. So, we develop a self-supervised model that can extract channel-specific representations from collections of images. We propose a search algorithm that uses these representations to obtain results that satisfy the collection’s intent along some channels but diverge from the query along others. We show that this allows for effective exploration of the creative space of possibilities. Finally, we demonstrate a mix-and-match visual querying mechanism that allows us to combine channels from different collections of inspirational content, thus facilitating ease in creative expression. -->
Domain Adaptation (DA) and Semi-supervised Learning (SSL) converge in Semi-supervised Domain Adaptation (SSDA), where the objective is to transfer knowledge from a source domain to a target domain using a combination of limited labeled target samples and abundant unlabeled target data. Although intuitive, a simple amalgamation of DA and SSL is suboptimal in semantic segmentation due to two major reasons: (1) previous methods, while able to learn good segmentation boundaries, are prone to confuse classes with similar visual appearance due to limited supervision; and (2) skewed and imbalanced training data distribution preferring source representation learning whereas impeding from exploring limited information about tailed classes. Language guidance can serve as a pivotal semantic bridge, facilitating robust class discrimination and mitigating visual ambiguities by leveraging the rich semantic relationships encoded in pre-trained language models to enhance feature representations across domains. Therefore, we propose the first language-guided SSDA setting for semantic segmentation in this work. Specifically, we harness the semantic generalization capabilities inherent in vision-language models (VLMs) to establish a synergistic framework within the SSDA paradigm. To address the inherent class-imbalance challenges in long-tailed distributions, we introduce class-balanced segmentation loss formulations that effectively regularize the learning process. Through extensive experimentation across diverse domain adaptation scenarios, our approach demonstrates substantial performance improvements over contemporary state-of-the-art (SoTA) methodologies.
<pre id="semidavil_bib_publication", style="display:none">
	@inproceedings{basak2025semidavil,
  title={SemiDAViL: Semi-supervised Domain Adaptation with Vision-Language Guidance for Semantic Segmentation},
  author={Basak, Hritam and Yin, Zhaozheng},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={9816--9828},
  year={2025}
}
</pre>
							</p>
						</div>
					</li>
				</div>
			</div>
			
			



			<h2 class="year">2024</h2>

			<!-- ECCV 2024 -->
			<div class="row" style="margin: 2.5em 0 ">
				<div class="col-md-4">
					<img class="pub-img " style="margin-right: auto; margin-left: auto;" src="img/Publications/eccv24-overall.png"></img>
				</div>
				<div class="col-md-8">
					<li itemscope itemtype="http://schema.org/CreativeWork ">
						<span itemprop="name">Forget More to Learn More: Domain-specific Feature Unlearning for Semi-supervised and Unsupervised Domain Adaptation</span>
						<div class="pub-list-item " itemscope itemtype="http://schema.org/CreativeWork ">
							<div class="pub-authors " itemprop="author ">
								<!-- <text style="color: black">Nihal Jain</text>, -->
								<!-- <text style="color: black">Praneetha Vaddamanu</text>, -->
								<b style="color: #0095eb">Hritam Basak</b>,
								<text style="color: black">Zhaozheng Yin</text>
								<!-- <text style="color: black">Kuldeep Kulkarni</text> -->
							</div>
							<div class="pub-publication ">European Conference on Computer Vision (ECCV 2024)</div>
							<p>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('CanvasAsContext_abstract_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">Abstract</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05513.pdf">PDF</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://github.com/hritam-98/FMLM">Code</a>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('CanvasAsContext_bib_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">BibTeX</a>
<pre id="CanvasAsContext_abstract_publication", style="display:none">
<!-- In creative workflows, designers compile a collection (or “moodboard”) of inspirational assets for ideation. They may use this as reference for finding additional assets. In this work, we aim to stimulate creative ideation by making suggestions that take cues from the designer’s moodboard but also carefully diverge from it. A collection of images may have rich information along different axes (e.g. color, composition, or style) – these axes or channels can be used to model relevance or divergence of any image from a query collection. So, we develop a self-supervised model that can extract channel-specific representations from collections of images. We propose a search algorithm that uses these representations to obtain results that satisfy the collection’s intent along some channels but diverge from the query along others. We show that this allows for effective exploration of the creative space of possibilities. Finally, we demonstrate a mix-and-match visual querying mechanism that allows us to combine channels from different collections of inspirational content, thus facilitating ease in creative expression. -->
Semi-supervised Domain Adaptation (SSDA) encompasses the process of adapting representations acquired from the source domain to a new target domain, utilizing a limited number of labeled samples in conjunction with an abundance of unlabeled data from the target domain. Simple aggregation of domain adaptation (DA) and semi-supervised learning (SSL) falls short of optimal performance due to two primary challenges: (1) skewed training data distribution favoring the source representation learning, and (2) the persistence of superfluous domain-specific features, hindering effective domain-agnostic (i.e., task-specific) feature extraction. In pursuit of greater generalizability and robustness, we present an SSDA framework with a new episodic learning strategy: \lq\lq learn, forget, then learn more\rq\rq. First, we train two encoder-classifier pairs, one for the source and the other for the target domain, aiming to learn domain-specific features. This involves minimizing classification loss for in-domain images and maximizing uncertainty loss for out-of-domain images. Subsequently, we transform the images into a new space, strategically unlearning (forgetting) the domain-specific representations while preserving their structural similarity to the originals. 
This proactive removal of domain-specific attributes is complemented by learning more domain-agnostic features using a Gaussian-guided latent alignment (GLA) strategy that uses a prior distribution to align domain-agnostic source and target representations.  
The proposed SSDA framework can be further extended to unsupervised domain adaptation (UDA). Evaluation across {two} domain adaptive image classification tasks reveals our method's superiority over state-of-the-art (SoTA) methods in both SSDA and UDA scenarios.</pre>
<pre id="CanvasAsContext_bib_publication", style="display:none">
	@inproceedings{basak2024forget,
		title={Forget More to Learn More: Domain-specific Feature Unlearning for Semi-supervised and Unsupervised Domain Adaptation},
		author={Basak, Hritam and Yin, Zhaozheng},
		booktitle={European Conference on Computer Vision},
		year={2024},
		organization={Springer}
	  }
}}</pre>
							</p>
						</div>
					</li>
				</div>
			</div>




			<!-- MICCAI 2024 -->
			<div class="row" style="margin: 2.5em 0 ">
				<div class="col-md-4">
					<img class="pub-img " style="margin-right: auto; margin-left: auto;" src="img/Publications/overall-miccai24-cropped.png"></img>
				</div>
				<div class="col-md-8">
					<li itemscope itemtype="http://schema.org/CreativeWork ">
						<span itemprop="name">Quest for Clone: Test-Time Domain Adaptation for Medical Image Segmentation by Searching the Closest Clone in Latent Space</span>
						<div class="pub-list-item " itemscope itemtype="http://schema.org/CreativeWork ">
							<div class="pub-authors " itemprop="author ">
								<!-- <text style="color: black">Nihal Jain</text>, -->
								<!-- <text style="color: black">Praneetha Vaddamanu</text>, -->
								<b style="color: #0095eb">Hritam Basak</b>,
								<text style="color: black">Zhaozheng Yin</text>
								<!-- <text style="color: black">Kuldeep Kulkarni</text> -->
							</div>
							<div class="pub-publication ">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2024)</div>
							<p>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('miccai24_abstract_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">Abstract</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://papers.miccai.org/miccai-2024/paper/0297_paper.pdf">PDF</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://github.com/hritam-98/Quest4Clone">Code</a>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('CanvasAsContext_bib_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">BibTeX</a>
<pre id="miccai24_abstract_publication", style="display:none">
<!-- In creative workflows, designers compile a collection (or “moodboard”) of inspirational assets for ideation. They may use this as reference for finding additional assets. In this work, we aim to stimulate creative ideation by making suggestions that take cues from the designer’s moodboard but also carefully diverge from it. A collection of images may have rich information along different axes (e.g. color, composition, or style) – these axes or channels can be used to model relevance or divergence of any image from a query collection. So, we develop a self-supervised model that can extract channel-specific representations from collections of images. We propose a search algorithm that uses these representations to obtain results that satisfy the collection’s intent along some channels but diverge from the query along others. We show that this allows for effective exploration of the creative space of possibilities. Finally, we demonstrate a mix-and-match visual querying mechanism that allows us to combine channels from different collections of inspirational content, thus facilitating ease in creative expression. -->
Unsupervised Domain Adaptation (UDA) aims to align labeled source distribution and unlabeled target distribution by mining domain-agnostic feature representation. However, adapting the source-trained model for new target domains after the model is deployed to users poses a significant challenge. To address this, we propose a generative latent search paradigm to reconstruct the closest clone of every target image from the source latent space. This involves utilizing a test-time adaptation (TTA) strategy, wherein a latent optimization step finds the closest clone of each target image from the source representation space using variational sampling of source latent distribution. Thus, our method facilitates domain adaptation without requiring target-domain supervision during training. Moreover, we demonstrate that our approach can be further fine-tuned using a few labeled target data without the need for unlabeled target data, by leveraging global and local label guidance from available target annotations to enhance the downstream segmentation task. We empirically validate the efficacy of our proposed method, surpassing existing UDA, TTA, and SSDA methods in two domain adaptive image segmentation tasks.</pre>
<pre id="miccai24_bib_publication", style="display:none">
	@inproceedings{basak2024quest,
		title={Quest for Clone: Test-Time Domain Adaptation for Medical Image Segmentation by Searching the Closest Clone in Latent Space},
		author={Basak, Hritam and Yin, Zhaozheng},
		booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
		pages={555--566},
		year={2024},
		organization={Springer}
	  }
}}</pre>
							</p>
						</div>
					</li>
				</div>
			</div>



			<h2 class="year">2023</h2>

			<!-- CVPR 23 -->
			<div class="row" style="margin: 2.5em 0 ">
				<div class="col-md-4">
					<img class="pub-img " style="margin-right: auto; margin-left: auto;" src="img/Publications/PLGCL_overall.png"></img>
				</div>
				<div class="col-md-8">
					<li itemscope itemtype="http://schema.org/CreativeWork ">
						<span itemprop="name">Pseudo Label Guided Contrastive Learning for Semi-supervised Medical Image Segmentation</span>
						<div class="pub-list-item " itemscope itemtype="http://schema.org/CreativeWork ">
							<div class="pub-authors " itemprop="author ">
								<!-- <text style="color: black">Nihal Jain</text>, -->
								<!-- <text style="color: black">Praneetha Vaddamanu</text>, -->
								<b style="color: #0095eb">Hritam Basak</b>,
								<text style="color: black">Zhaozheng Yin</text>
								<!-- <text style="color: black">Kuldeep Kulkarni</text> -->
							</div>
							<div class="pub-publication ">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2023)</div>
							<p>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('cvpr23_abstract_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">Abstract</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://openaccess.thecvf.com/content/CVPR2023/papers/Basak_Pseudo-Label_Guided_Contrastive_Learning_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf">PDF</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://github.com/hritam-98/PatchCL-MedSeg">Code</a>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('CanvasAsContext_bib_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">BibTeX</a>
<pre id="cvpr23_abstract_publication", style="display:none">
<!-- In creative workflows, designers compile a collection (or “moodboard”) of inspirational assets for ideation. They may use this as reference for finding additional assets. In this work, we aim to stimulate creative ideation by making suggestions that take cues from the designer’s moodboard but also carefully diverge from it. A collection of images may have rich information along different axes (e.g. color, composition, or style) – these axes or channels can be used to model relevance or divergence of any image from a query collection. So, we develop a self-supervised model that can extract channel-specific representations from collections of images. We propose a search algorithm that uses these representations to obtain results that satisfy the collection’s intent along some channels but diverge from the query along others. We show that this allows for effective exploration of the creative space of possibilities. Finally, we demonstrate a mix-and-match visual querying mechanism that allows us to combine channels from different collections of inspirational content, thus facilitating ease in creative expression. -->
Although recent works in semi-supervised learning (SemiSL) have accomplished significant success in natural image segmentation, the task of learning discriminative representations from limited annotations has been an open problem in medical images. Contrastive Learning (CL) frameworks use the notion of similarity measure which is useful for classification problems, however, they fail to transfer these quality representations for accurate pixel-level segmentation. To this end, we propose a novel semi-supervised patch-based CL framework for medical image segmentation without using any explicit pretext task. We harness the power of both CL and SemiSL, where the pseudo-labels generated from SemiSL aid CL by providing additional guidance, whereas discriminative class information learned in CL leads to accurate multi-class segmentation. Additionally, we formulate a novel loss that synergistically encourages inter-class separability and intra-class compactness among the learned representations. A new inter-patch semantic disparity mapping using average patch entropy is employed for a guided sampling of positives and negatives in the proposed CL framework. Experimental analysis on three publicly available datasets of multiple modalities reveals the superiority of our proposed method as compared to the state-of-the-art methods.
</pre>
<pre id="cvpr23_bib_publication", style="display:none">
	@inproceedings{basak2023pseudo,
		title={Pseudo-Label Guided Contrastive Learning for Semi-Supervised Medical Image Segmentation},
		author={Basak, Hritam and Yin, Zhaozheng},
		booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
		pages={19786--19797},
		year={2023}
	  }
}}</pre>
							</p>
						</div>
					</li>
				</div>
			</div>







			<!-- MICCAI 23 -->
			<div class="row" style="margin: 2.5em 0 ">
				<div class="col-md-4">
					<img class="pub-img " style="margin-right: auto; margin-left: auto;" src="img/Publications/style-content-miccai23.png"></img>
				</div>
				<div class="col-md-8">
					<li itemscope itemtype="http://schema.org/CreativeWork ">
						<span itemprop="name">Semi-supervised Domain Adaptive Medical Image Segmentation through Consistency Regularized Disentangled Contrastive Learning</span>
						<div class="pub-list-item " itemscope itemtype="http://schema.org/CreativeWork ">
							<div class="pub-authors " itemprop="author ">
								<b style="color: #0095eb">Hritam Basak</b>,
								<text style="color: black">Zhaozheng Yin</text>
								<!-- <text style="color: black">Praneetha Vaddamanu</text>,
								<text style="color: black">Dhananjay Raut</text>,
								<text style="color: black">Shraiysh Vaishay</text>,
								<text style="color: black">Vishwa Vinay</text> -->
							</div>
							<div class="pub-publication ">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2023)</div>
							<p>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('AttributeComposition_abstract_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">Abstract</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://link.springer.com/epdf/10.1007/978-3-031-43901-8_25?sharing_token=fKuDPiCxykqg1xU-hoykUPe4RwlQNchNByi7wbcMAY7w2JJtACqQoRPL8LPCRrDOiMd-AD9rORop3ByrXn6ukE75nsnha3KyvBjGQlQg0gauqN36z02fIyGmGiezSJ1c3L9qrlBv2ENrLhqXcJo9rNyy-MwnAm1bjp34Osxh_O4%3D">PDF</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://github.com/hritam-98/GFDA-disentangled">Code</a>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('AttributeComposition_bib_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">BibTeX</a>
<pre id="AttributeComposition_abstract_publication", style="display:none">
	Although unsupervised domain adaptation (UDA) is a promising direction to alleviate domain shift, they fall short of their supervised counterparts. In this work, we investigate relatively less explored semi-supervised domain adaptation (SSDA) for medical image segmentation, where access to a few labeled target samples can improve the adaptation performance substantially. Specifically, we propose a two-stage training process. First, an encoder is pre-trained in a self-learning paradigm using a novel domain-content disentangled contrastive learning (CL) along with a pixel-level feature consistency constraint. The proposed CL enforces the encoder to learn discriminative content-specific but domain-invariant semantics on a global scale from the source and target images, whereas consistency regularization enforces the mining of local pixel-level information by maintaining spatial sensitivity. This pre-trained encoder, along with a decoder, is further fine-tuned for the downstream task, (i.e. pixel-level segmentation) using a semi-supervised setting. Furthermore, we experimentally validate that our proposed method can easily be extended for UDA settings, adding to the superiority of the proposed strategy. Upon evaluation on two domain adaptive image segmentation tasks, our proposed method outperforms the SoTA methods, both in SSDA and UDA settings.
</pre>
<pre id="AttributeComposition_bib_publication", style="display:none">
	@inproceedings{basak2023semi,
		title={Semi-supervised domain adaptive medical image segmentation through consistency regularized disentangled contrastive learning},
		author={Basak, Hritam and Yin, Zhaozheng},
		booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
		pages={260--270},
		year={2023},
		organization={Springer}
	  }
}</pre>
							</p>
						</div>
					</li>
				</div>
			</div>

			
			





			
			<!-- ICASSP 2023 -->
			<div class="row" style="margin: 2.5em 0 ">
				<div class="col-md-4">
					<img class="pub-img " style="margin-right: auto; margin-left: auto; width: 120%; height: 100%" src="img/Publications/icassp-23.png"></img>
				</div>
				<div class="col-md-8 ">
					<li itemscope itemtype="http://schema.org/CreativeWork ">
						<span itemprop="name">IDEAL: Improved DEnse locAL Contrastive Learning for Semi-Supervised Medical Image Segmentation</span>
						<div class="pub-list-item " itemscope itemtype="http://schema.org/CreativeWork ">
							<div class="pub-authors " itemprop="author ">
								<b style="color: #0095eb">Hritam Basak<sup></sup></b>,
								<text style="color: black">Soumitri Chattopadhyay<sup>&dagger;</sup></text>,
								<text style="color: black">Rohit Kundu<sup>&dagger;</sup></text>,
								<text style="color: black">Sayan Nag<sup>&dagger;</sup></text>,
								<text style="color: black">Rammohan Malippeddi</text>
								<text style="color: black"><small>(&dagger; denotes equal contribution)</small></text>
							</div>
							<div class="pub-publication "> IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023)</div>
							<p>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('SG_abstract_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">Abstract</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://arxiv.org/abs/2210.15075">PDF</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://rohit-kundu.github.io/IDEAL-ICASSP23/">Project</a>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('SG_bib_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">BibTeX</a>
<pre id="SG_abstract_publication", style="display:none">
	Due to the scarcity of labeled data, Contrastive Self-Supervised Learning (SSL) frameworks have lately shown great potential in several medical image analysis tasks. However, the existing contrastive mechanisms are sub-optimal for dense pixel-level segmentation tasks due to their inability to mine local features. To this end, we extend the concept of metric learning to the segmentation task, using a dense (dis)similarity learning for pre-training a deep encoder network, and employing a semi-supervised paradigm to fine-tune for the downstream task. Specifically, we propose a simple convolutional projection head for obtaining dense pixel-level features, and a new contrastive loss to utilize these dense projections thereby improving the local representations. A bidirectional consistency regularization mechanism involving two-stream model training is devised for the downstream task. Upon comparison, our IDEAL method outperforms the SoTA methods by fair margins on cardiac MRI segmentation.
</pre>
<pre id="SG_bib_publication", style="display:none">
	@inproceedings{basak2023ideal,
		title={Ideal: Improved Dense Local Contrastive Learning For Semi-Supervised Medical Image Segmentation},
		author={Basak, Hritam and Chattopadhyay, Soumitri and Kundu, Rohit and Nag, Sayan and Mallipeddi, Rammohan},
		booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
		pages={1--5},
		year={2023},
		organization={IEEE}
	  }
}</pre>
							</p>
						</div>
					</li>
				</div>
			</div>








			<!-- UT-Net -->
			<div class="row" style="margin: 2.5em 0 ">
				<div class="col-md-4">
					<img class="pub-img " style="margin-right: auto; margin-left: auto;" src="img/Publications/ut-net.png"></img>
				</div>
				<div class="col-md-8">
					<li itemscope itemtype="http://schema.org/CreativeWork ">
						<span itemprop="name">UT-Net: Combining U-Net and Transformer for Joint Optic Disc and Cup Segmentation and Glaucoma Detection</span>
						<div class="pub-list-item " itemscope itemtype="http://schema.org/CreativeWork ">
							<div class="pub-authors " itemprop="author ">
								<!-- <text style="color: black">Nihal Jain</text>, -->
								<!-- <text style="color: black">Praneetha Vaddamanu</text>, -->
								<text style="color: black">Rukhshanda Hussain</text>,
								<b style="color: #0095eb">Hritam Basak</b>
								
								<!-- <text style="color: black">Kuldeep Kulkarni</text> -->
							</div>
							<div class="pub-publication ">International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC 2023)</div>
							<p>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('utnet_abstract_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">Abstract</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://arxiv.org/abs/2303.04939">PDF</a>
								<!-- <a class="btn btn-primary btn-outline btn-xs " href="https://github. com/hritam-98/PatchCL-MedSeg">Code</a> -->
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('utnet_bib_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">BibTeX</a>
<pre id="utnet_abstract_publication", style="display:none">
<!-- In creative workflows, designers compile a collection (or “moodboard”) of inspirational assets for ideation. They may use this as reference for finding additional assets. In this work, we aim to stimulate creative ideation by making suggestions that take cues from the designer’s moodboard but also carefully diverge from it. A collection of images may have rich information along different axes (e.g. color, composition, or style) – these axes or channels can be used to model relevance or divergence of any image from a query collection. So, we develop a self-supervised model that can extract channel-specific representations from collections of images. We propose a search algorithm that uses these representations to obtain results that satisfy the collection’s intent along some channels but diverge from the query along others. We show that this allows for effective exploration of the creative space of possibilities. Finally, we demonstrate a mix-and-match visual querying mechanism that allows us to combine channels from different collections of inspirational content, thus facilitating ease in creative expression. -->
Glaucoma is a chronic visual disease that may cause permanent irreversible blindness. Measurement of the cup-to-disc ratio (CDR) plays a pivotal role in the detection of glaucoma in its early stage, preventing visual disparities. Therefore, accurate and automatic segmentation of optic disc (OD) and optic cup (OC) from retinal fundus images is a fundamental requirement. Existing CNN-based segmentation frameworks resort to building deep encoders with aggressive downsampling layers, which suffer from a general limitation on modeling explicit long-range dependency. To this end, in this paper, we propose a new segmentation pipeline, called UT-Net, availing the advantages of U-Net and transformer both in its encoding layer, followed by an attention-gated bilinear fusion scheme. In addition to this, we incorporate Multi-Head Contextual attention to enhance the regular self-attention used in traditional vision transformers. Thus low-level features along with global dependencies are captured in a shallow manner. Besides, we extract context information at multiple encoding layers for better exploration of receptive fields, and to aid the model to learn deep hierarchical representations. Finally, an enhanced mixing loss is proposed to tightly supervise the overall learning process. The proposed model has been implemented for joint OD and OC segmentation on three publicly available datasets: DRISHTI-GS, RIM-ONE R3, and REFUGE. Additionally, to validate our proposal, we have performed exhaustive experimentation on Glaucoma detection from all three datasets by measuring the Cup to Disc Ratio (CDR) value. Experimental results demonstrate the superiority of UT-Net as compared to the state-of-the-art methods.
</pre>
<pre id="utnet_bib_publication", style="display:none">
	@article{hussain2023ut,
		title={UT-Net: Combining U-Net and Transformer for Joint Optic Disc and Cup Segmentation and Glaucoma Detection},
		author={Hussain, Rukhshanda and Basak, Hritam},
		journal={arXiv preprint arXiv:2303.04939},
		year={2023}
	  }
}}</pre>
							</p>
						</div>
					</li>
				</div>
			</div>

			<h2 class="year">2022</h2>






			<!-- MICCAI 2022 -->
			<div class="row" style="margin: 2.5em 0 ">
				<div class="col-md-4">
					<img class="pub-img " style="margin-right: auto; margin-left: auto; width: 90%" src="img/Publications/MICCAI_Overall_final.png"></img>
				</div>
				<div class="col-md-8">
					<li itemscope itemtype="http://schema.org/CreativeWork ">
						<span itemprop="name">Addressing Class Imbalance in Semi-supervised Image Segmentation: A Study on Cardiac MRI</span>
						<div class="pub-list-item " itemscope itemtype="http://schema.org/CreativeWork ">
							<div class="pub-authors " itemprop="author ">
								<b style="color: #0095eb">Hritam Basak</b>,
								<text style="color: black">Sagnik Ghosal</text>,
								<text style="color: black">Ram Sarkar</text>
							</div>
							<div class="pub-publication ">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2022)</div>
							<p>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('miccai22_abstract_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">Abstract</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://rdcu.be/cVRY4">PDF</a>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('miccai22_bib_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">BibTeX</a>
<pre id="miccai22_abstract_publication", style="display:none">
	Due to the imbalanced and limited data, semi-supervised medical image segmentation methods often fail to produce superior performance for some specific tailed classes. Inadequate training for those particular classes could introduce more noise to the generated pseudo labels, affecting overall learning. To alleviate this shortcoming and identify the under-performing classes, we propose maintaining a confidence array that records class-wise performance during training. A fuzzy fusion of these confidence scores is proposed to adaptively prioritize individual confidence metrics in every sample rather than traditional ensemble approaches, where a set of predefined fixed weights are assigned for all the test cases. Further, we introduce a robust class-wise sampling method and dynamic stabilization for better training strategy. Our proposed method considers all the under-performing classes with dynamic weighting and tries to remove most of the noises during training. Upon evaluation on two cardiac MRI datasets, ACDC and MMWHS, our proposed method shows effectiveness and generalizability and outperforms several state-of-the-art methods found in the literature.
</pre>
<pre id="miccai22_bib_publication", style="display:none">
	@inproceedings{basak2022addressing,
		title={Addressing class imbalance in semi-supervised image segmentation: A study on cardiac mri},
		author={Basak, Hritam and Ghosal, Sagnik and Sarkar, Ram},
		booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
		pages={224--233},
		year={2022},
		organization={Springer}
	  }
}</pre>
							</p>
						</div>
					</li>
				</div>
			</div>







			<!-- ISBI 2022 -->
			<div class="row" style="margin: 2.5em 0 ">
				<div class="col-md-4">
					<img class="pub-img " style="margin-right: auto; margin-left: auto;" src="img/Publications/ICT-MedSeg-Overall.png"></img>
				</div>
				<div class="col-md-8">
					<li itemscope itemtype="http://schema.org/CreativeWork ">
						<span itemprop="name">An Embarassingly Simple Consistency Regularization Method for Semi-Supervised Medical Image Segmentation</span>
						<div class="pub-list-item " itemscope itemtype="http://schema.org/CreativeWork ">
							<div class="pub-authors " itemprop="author ">
								<!-- <text style="color: black">Natwar Modani<sup>&dagger;</sup></text>, -->
								<b style="color: #0095eb">Hrtam Basak<sup></sup></b>,
								<text style="color: black">Rajarshi Bhattacharya</text>,
								<text style="color: black">Rukhshanda Hussain</text>,
								<text style="color: black">Agniv Chatterjee</text>
								<!-- <text style="color: black">Somak Aditya</text> -->
								<!-- <text style="color: black"><small>(&dagger; denotes equal contribution)</small></text> -->
							</div>
							<div class="pub-publication ">IEEE International Symposium on Biomedical Imaging (ISBI 2022)</div>
							<p>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('isbi22_abstract_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">Abstract</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://arxiv.org/abs/2202.00677">PDF</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://github.com/hritam-98/ICT-MedSeg">Code</a>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('isbi22_bib_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">BibTeX</a>
<pre id="isbi22_abstract_publication", style="display:none">
	The scarcity of pixel-level annotation is a prevalent problem in medical image segmentation tasks. In this paper, we introduce a novel regularization strategy involving interpolation-based mixing for semi-supervised medical image segmentation. The proposed method is a new consistency regularization strategy that encourages segmentation of interpolation of two unlabelled data to be consistent with the interpolation of segmentation maps of those data. This method represents a specific type of data-adaptive regularization paradigm which aids to minimize the overfitting of labelled data under high confidence values. The proposed method is advantageous over adversarial and generative models as it requires no additional computation. Upon evaluation on two publicly available MRI datasets: ACDC and MMWHS, experimental results demonstrate the superiority of the proposed method in comparison to existing semi-supervised models.
</pre>
<pre id="isbi22_bib_publication", style="display:none">
	@inproceedings{basak2022exceedingly,
		title={An exceedingly simple consistency regularization method for semi-supervised medical image segmentation},
		author={Basak, Hritam and Bhattacharya, Rajarshi and Hussain, Rukhshanda and Chatterjee, Agniv},
		booktitle={2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)},
		pages={1--4},
		year={2022},
		organization={IEEE}
	  }
}</pre>
							</p>
						</div>
					</li>
				</div>
			</div>

			<!-- <h2 class="year">2019</h2> -->







			<!-- MFSNet -->
			<div class="row" style="margin: 2.5em 0 ">
				<div class="col-md-4">
					<img class="pub-img " style="margin-right: auto; margin-left: auto; width: 80%" src="img/Publications/mfsnet.png"></img>
				</div>
				<div class="col-md-8">
					<li itemscope itemtype="http://schema.org/CreativeWork ">
						<span itemprop="name">MFSNet: A multi focus segmentation network for skin lesion segmentation</span>
						<div class="pub-list-item " itemscope itemtype="http://schema.org/CreativeWork ">
							<div class="pub-authors " itemprop="author ">
								<!-- <text style="color: black">Sumit Shekhar</text>, -->
								<b style="color: #0095eb">Hritam Basak</b>,
								<text style="color: black">Rohit Kundu</text>,
								<text style="color: black">Ram Sarkar</text>
								<!-- <text style="color: black">Kush Kumar Singh</text>,
								<text style="color: black">Kundan Krishna</text> -->
							</div>
							<div class="pub-publication ">Pattern Recognition, Elsevier</div>
							<p>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('MFSNet'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">Abstract</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://www.sciencedirect.com/science/article/abs/pii/S0031320322001546">PDF</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://github.com/Rohit-Kundu/MFSNet">Code</a>
								<!-- <a class="btn btn-primary btn-outline btn-xs " href="https://www.youtube.com/watch?v=Utt6VIpzbf8">Demo</a> -->
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('MFSNet'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">BibTeX</a>
<pre id="MFSNet", style="display:none">
	Segmentation is essential for medical image analysis to identify and localize diseases, monitor morphological changes, and extract discriminative features for further diagnosis. Skin cancer is one of the most common types of cancer globally, and its early diagnosis is pivotal for the complete elimination of malignant tumors from the body. This research develops an Artificial Intelligence (AI) framework for supervised skin lesion segmentation employing the deep learning approach. The proposed framework, called MFSNet (Multi-Focus Segmentation Network), uses differently scaled feature maps for computing the final segmentation mask using raw input RGB images of skin lesions. In doing so, initially, the images are preprocessed to remove unwanted artifacts and noises. The MFSNet employs the Res2Net backbone, a recently proposed convolutional neural network (CNN), for obtaining deep features used in a Parallel Partial Decoder (PPD) module to get a global map of the segmentation mask. In different stages of the network, convolution features and multi-scale maps are used in two boundary attention (BA) modules and two reverse attention (RA) modules to generate the final segmentation output. MFSNet, when evaluated on three publicly available datasets: ISIC 2017, and HAM10000, outperforms state-of-the-art methods, justifying the reliability of the framework.
</pre>
<pre id="MFSNet", style="display:none">
	@article{basak2022mfsnet,
		title={MFSNet: A multi focus segmentation network for skin lesion segmentation},
		author={Basak, Hritam and Kundu, Rohit and Sarkar, Ram},
		journal={Pattern Recognition},
		volume={128},
		pages={108673},
		year={2022},
		publisher={Elsevier}
	  }
}</pre>
							</p>
						</div>
					</li>
				</div>
			</div>






			<!-- Fuzzy-fusion -->
			<div class="row" style="margin: 2.5em 0 ">
				<div class="col-md-4">
					<img class="pub-img " style="margin-right: auto; margin-left: auto;" src="img/Publications/gompertz.png"></img>
				</div>
				<div class="col-md-8">
					<li itemscope itemtype="http://schema.org/CreativeWork ">
						<span itemprop="name">Fuzzy rank-based fusion of CNN models using Gompertz function for screening COVID-19 CT-scans</span>
						<div class="pub-list-item " itemscope itemtype="http://schema.org/CreativeWork ">
							<div class="pub-authors " itemprop="author ">
								<text style="color: black">Rohit Kundu</text>,
								<b style="color: #0095eb">Hritam Basak</b>,
								<text style="color: black">Pawan Kumar Singh</text>,
								<text style="color: black">Ali Ahmadian</text>,
								<text style="color: black">Massimiliano Ferrara</text>,
								<text style="color: black">Ram Sarkar</text>
								<!-- <text style="color: black">Balaji Vasan Srinivasan</text> -->
							</div>
							<div class="pub-publication ">Scientific Reports, Nature</div>
							<p>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('gompertz_abstract_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">Abstract</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://www.nature.com/articles/s41598-021-93658-y.pdf">PDF</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://github.com/Rohit-Kundu/COVID-Detection-Gompertz-Function-Ensemble">Code</a>
								<!-- <a class="btn btn-primary btn-outline btn-xs " href="https://www.youtube.com/watch?v=3O1TFApSTPQ">Talk</a> -->
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('gompertz_bib_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">BibTeX</a>
<pre id="gompertz_abstract_publication", style="display:none">
	COVID-19 has crippled the world’s healthcare systems, setting back the economy and taking the lives of several people. Although potential vaccines are being tested and supplied around the world, it will take a long time to reach every human being, more so with new variants of the virus emerging, enforcing a lockdown-like situation on parts of the world. Thus, there is a dire need for early and accurate detection of COVID-19 to prevent the spread of the disease, even more. The current gold-standard RT-PCR test is only 71% sensitive and is a laborious test to perform, leading to the incapability of conducting the population-wide screening. To this end, in this paper, we propose an automated COVID-19 detection system that uses CT-scan images of the lungs for classifying the same into COVID and Non-COVID cases. The proposed method applies an ensemble strategy that generates fuzzy ranks of the base classification models using the Gompertz function and fuses the decision scores of the base models adaptively to make the final predictions on the test cases. Three transfer learning-based convolutional neural network models are used, namely VGG-11, Wide ResNet-50-2, and Inception v3, to generate the decision scores to be fused by the proposed ensemble model. The framework has been evaluated on two publicly available chest CT scan datasets achieving state-of-the-art performance, justifying the reliability of the model. 
</pre>
<pre id="gompertz_bib_publication", style="display:none">
	@article{kundu2021fuzzy,
		title={Fuzzy rank-based fusion of CNN models using Gompertz function for screening COVID-19 CT-scans},
		author={Kundu, Rohit and Basak, Hritam and Singh, Pawan Kumar and Ahmadian, Ali and Ferrara, Massimiliano and Sarkar, Ram},
		journal={Scientific reports},
		volume={11},
		number={1},
		pages={14133},
		year={2021},
		publisher={Nature Publishing Group UK London}
	  }
}</pre>
							</p>
						</div>
					</li>
				</div>
			</div>

	
	
	
	
	
	
			<!-- <h2 class="year">2018</h2> -->

			<!-- 3DHar -->
			<div class="row" style="margin: 2.5em 0 ">
				<div class="col-md-4">
					<img class="pub-img " style="margin-right: auto; margin-left: auto; width: 90%" src="img/Publications/3dhar.png"></img>
				</div>
				<div class="col-md-8">
					<li itemscope itemtype="http://schema.org/CreativeWork ">
						<span itemprop="name">A union of deep learning and swarm-based optimization for 3D human action recognition</span>
						<div class="pub-list-item " itemscope itemtype="http://schema.org/CreativeWork ">
							<div class="pub-authors " itemprop="author ">
								<!-- <text style="color: black">Abhilasha Sancheti</text>, -->
								<b style="color: #0095eb">Hritam Basak</b>,
								<text style="color: black">Rohit Kundu</text>,
								<text style="color: black">Pawan Kumar Singh</text>,
								<text style="color: black">Muhammad Fazal Ijaz</text>,
								<text style="color: black">Marcin Woźniak</text>, 
								<text style="color: black">Ram Sarkar</text>
								
							</div>
							<div class="pub-publication ">Scientific Reports, Nature</div>
							<p>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('3dhar_abstract_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">Abstract</a>
								<a class="btn btn-primary btn-outline btn-xs " href="https://www.nature.com/articles/s41598-022-09293-8.pdf">PDF</a>
								<a class="btn btn-primary btn-outline btn-xs" onclick="var div = document.getElementById('3dhar_bib_publication'); if (div.style.display !== 'none') {div.style.display = 'none';} else {div.style.display = 'block';}">BibTeX</a>
<pre id="3dhar_abstract_publication", style="display:none">
	Human Action Recognition (HAR) is a popular area of research in computer vision due to its wide range of applications such as surveillance, health care, and gaming, etc. Action recognition based on 3D skeleton data allows simplistic, cost-efficient models to be formed making it a widely used method. In this work, we propose DSwarm-Net, a framework that employs deep learning and swarm intelligence-based metaheuristic for HAR that uses 3D skeleton data for action classification. We extract four different types of features from the skeletal data namely: Distance, Distance Velocity, Angle, and Angle Velocity, which capture complementary information from the skeleton joints for encoding them into images. Encoding the skeleton data features into images is an alternative to the traditional video-processing approach and it helps in making the classification task less complex. The Distance and Distance Velocity encoded images have been stacked depth-wise and fed into a Convolutional Neural Network model which is a modified version of Inception-ResNet. Similarly, the Angle and Angle Velocity encoded images have been stacked depth-wise and fed into the same network. After training these models, deep features have been extracted from the pre-final layer of the networks, and the obtained feature representation is optimized by a nature-inspired metaheuristic, called Ant Lion Optimizer, to eliminate the non-informative or misleading features and to reduce the dimensionality of the feature set. DSwarm-Net has been evaluated on three publicly available HAR datasets, namely UTD-MHAD, HDM05, and NTU RGB+D 60 achieving competitive results, thus confirming the superiority of the proposed model compared to state-of-the-art models.
</pre>
<pre id="3dhar_bib_publication", style="display:none">
	@article{basak2022union,
		title={A union of deep learning and swarm-based optimization for 3D human action recognition},
		author={Basak, Hritam and Kundu, Rohit and Singh, Pawan Kumar and Ijaz, Muhammad Fazal and Wo{\'z}niak, Marcin and Sarkar, Ram},
		journal={Scientific Reports},
		volume={12},
		number={1},
		pages={5494},
		year={2022},
		publisher={Nature Publishing Group UK London}
	  }
}</pre>
							</p>
						</div>
					</li>
				</div>
			</div>

		</div></ul>


<table style="width:50%;border:0px;border-spacing:0px;horizontal-align:middle;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
  <td style="padding:5px;width:50%;vertical-align:middle">
    <div style="border-bottom: 1px solid #aaaaaa;"><heading>Academic Services</heading></div>
    <ul id="services">
      <li style="padding:4px"><b>Conference Reviewing:</b></li>
      - CVPR '24: Reviewed 3 papers<br>
      - MICCAI '23: Reviewed 4 papers<br>
      - ICASSP '23: Reviewed 3 papers<br>
      - ISBI '23: Reviewed 1 paper
      <br>
      <li style="padding:4px"><b>Journal Reviewing:</b></li>
      - IEEE TNNLS ('23-Present)<br>
      - IEEE TPAMI ('23-Present)<br>
      - IEEE Cybernetics ('23-Present)<br>
      - IEEE TCSVT ('23-Present)<br>
      - ACM Multimedia ('22-Present)<br>
      - Scientific Reports, Nature ('22-Present)<br>
      - IJCV ('23-Present)
      
      <br>
    </ul>
  </td>
</tr></tbody>



</table>


<table class="globe" cellpadding="0" cellspacing="4" style="width:50%; margin:auto; border:5px; border-spacing:0px; border-collapse:separate;">
    <!-- 4 columns -->
    <tr>
        <td></td>
        <td rowspan="4" colspan="0" style="text-align: center">
		<a href="https://info.flagcounter.com/q0IF"><img src="https://s01.flagcounter.com/countxl/q0IF/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a>
<!--             <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=T9TbaiL1AyYllyDpTZfMyJ2wH6KTeGXXOAXwILXgSi8"></script> -->
        </td>
        <td></td>
        <td></td>
    </tr>
</table>





<!-- <table class="globe" cellpadding="0" cellspacing="4"> -->
                    <!-- 4 columns -->

                <!-- </table> -->
            </div>
        </main>




		</section>

	<footer class="site-footer ">
		<div class="container ">
			<p class="powered-by ">

				&copy; Powered by the <a href="https://github.com/gcushen/hugo-academic " target="_blank ">Academic theme</a> for <a href="http://gohugo.io " target="_blank ">Hugo</a>.

				<span class="pull-right " aria-hidden="true ">
					<a href="# " id="back_to_top ">
						<span class="button_icon ">
							<i class="fa fa-chevron-up fa-2x "></i>
						</span>
					</a>
				</span>

			</p>
		</div>
	</footer>

	<script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js "></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js "></script>
	<script src="js/jquery-1.12.3.min.js "></script>
	<script src="js/bootstrap.min.js "></script>
	<script src="js/hugo-academic.js "></script>
	<script src="js/shuffle.js "></script>
	<script src="https://unpkg.com/shuffle-letters "></script>

</body>
</html>
